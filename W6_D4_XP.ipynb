{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e84e4b",
   "metadata": {},
   "source": [
    "# Exercices XP\n",
    "Derni√®re mise √† jour : 14 juillet 2025\n",
    "\n",
    "## üë©‚Äçüè´ üë©üèø‚Äçüè´ Ce que vous apprendrez\n",
    "Strat√©gies de recherche de vecteurs (KNN, ANN) et √©valuation.\n",
    "Utilitaire de base de donn√©es vectorielles (recherche de similarit√©, RAG).\n",
    "Diff√©rences entre les bases de donn√©es vectorielles, les biblioth√®ques et les plugins.\n",
    "Meilleures pratiques pour l‚Äôutilisation et les performances du magasin vectoriel.\n",
    "Comment les mod√®les linguistiques apprennent les connaissances via le contexte.\n",
    "G√©n√©ration d'int√©gration de texte et stockage vectoriel.\n",
    "Interrogation des magasins de vecteurs pour les documents pertinents.\n",
    "Application de mod√®les linguistiques pour r√©pondre √† des questions avec un contexte r√©cup√©r√©.\n",
    "\n",
    "\n",
    "## üõ†Ô∏è Ce que vous allez cr√©er\n",
    "Un pipeline fonctionnel de g√©n√©ration augment√©e de r√©cup√©ration (RAG), d√©montrant la vectorisation de texte, le stockage de vecteurs dans FAISS et ChromaDB, la recherche de similarit√© et la r√©ponse aux questions √† l'aide d'un mod√®le de langage Hugging Face.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3223e4",
   "metadata": {},
   "source": [
    "## üåü Exercice 1 : Chargement et pr√©paration des donn√©es\n",
    "Dans cet exercice, nous allons configurer l'environnement et pr√©parer le jeu de donn√©es que nous utiliserons tout au long du projet. Une pr√©paration ad√©quate des donn√©es garantit le bon d√©roulement des processus en aval, comme la g√©n√©ration d'int√©grations, l'utilisation de bases de donn√©es vectorielles ou la cr√©ation de mod√®les de machine learning. Examinons chaque √©tape ensemble.\n",
    "\n",
    "\n",
    "\n",
    "Pourquoi cette √©tape est importante :\n",
    "Avant de se lancer dans des techniques avanc√©es, il est essentiel de :\n",
    "\n",
    "Assurez-vous que toutes les biblioth√®ques requises sont install√©es.\n",
    "Chargez et inspectez les donn√©es pour comprendre leur structure.\n",
    "Pr√©parez un sous-ensemble g√©rable pour des it√©rations plus rapides pendant le d√©veloppement.\n",
    "Ces √©tapes nous aident √† √©viter les probl√®mes techniques et √† garantir que nos analyses ou mod√®les reposent sur des bases solides.\n",
    "\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Installer les biblioth√®ques requises\n",
    "\n",
    "Le projet n√©cessite des biblioth√®ques sp√©cialis√©es pour la recherche de vecteurs et la gestion de bases de donn√©es :\n",
    "\n",
    "Entrez dans votre dossier, puis, dans votre terminal :\n",
    "\n",
    "\n",
    "\n",
    "pip install -q faiss-cpu==1.7.4 \n",
    "pip install -q chromadb==0.3.21\n",
    "pip install -qU chromadb\n",
    "pip install -q numpy<2\n",
    "\n",
    "\n",
    "Cr√©ez un r√©pertoire de cache pour garder notre espace de travail organis√© et g√©rer toutes les donn√©es interm√©diaires ou les fichiers t√©l√©charg√©s :\n",
    "\n",
    "Dans votre terminal :\n",
    "\n",
    "mkdir cache\n",
    "alors\n",
    "\n",
    "apt install libomp-dev\n",
    "python -m pip install --upgrade faiss-cpu\n",
    "\n",
    "\n",
    "puis, dans votre fichier, importez les biblioth√®ques essentielles\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "faiss-cpu:Une biblioth√®que pour une recherche de similarit√© efficace et un regroupement de vecteurs denses (d√©velopp√©e par Facebook AI Research).\n",
    "chromadb:Une biblioth√®que de bases de donn√©es vectorielles qui nous permet de stocker et d'interroger efficacement les int√©grations.\n",
    "Ces biblioth√®ques sont essentielles pour les √©tapes ult√©rieures lorsque nous g√©rons les int√©grations et effectuons des recherches de similarit√©.\n",
    "\n",
    "2. Charger l'ensemble de donn√©es\n",
    "\n",
    "Nous travaillerons avec un jeu de donn√©es intitul√© labelled_newscatcher_dataset.csv , qui contient des articles d'actualit√© √©tiquet√©s. Ces articles seront ensuite transform√©s en int√©grations pour le stockage vectoriel et la recherche.\n",
    "\n",
    "T√¢che : Charger l‚Äôensemble de donn√©es dans un DataFrame pandas :\n",
    "\n",
    "\n",
    "path =  # Provide the correct file path.\n",
    "pdf =   # Load the CSV file into a pandas DataFrame.\n",
    "\n",
    "\n",
    "Cette √©tape garantit que nos donn√©es sont dans un format adapt√© √† l‚Äôanalyse.\n",
    "\n",
    "3. Ajoutez une colonne d'identifiant (si n√©cessaire)\n",
    "\n",
    "Les identifiants uniques nous aident √† suivre chaque enregistrement, en particulier lorsque nous travaillons avec des bases de donn√©es vectorielles :\n",
    "\n",
    "\n",
    "\n",
    "pdf[\"id\"] =\n",
    "\n",
    "\n",
    "Chaque article d‚Äôactualit√© aura un identifiant unique, ce qui facilitera sa r√©f√©rence lors du stockage et de la r√©cup√©ration.\n",
    "\n",
    "4. Inspectez les donn√©es\n",
    "\n",
    "Utilisez la commande suivante pour obtenir un aper√ßu rapide de l‚Äôensemble de donn√©es :\n",
    "\n",
    "\n",
    "\n",
    "display(pdf)\n",
    "\n",
    "\n",
    "Prenez un moment pour observer :\n",
    "\n",
    "Les colonnes disponibles.\n",
    "Le type de donn√©es qu'ils contiennent (par exemple, texte, √©tiquettes).\n",
    "S'il y a des valeurs manquantes.\n",
    "La compr√©hension de l‚Äôensemble de donn√©es √† ce stade est essentielle pour une prise de d√©cision √©clair√©e lors des √©tapes ult√©rieures.\n",
    "\n",
    "5. Cr√©ez un sous-ensemble pour un traitement plus rapide\n",
    "\n",
    "Travailler avec de grands ensembles de donn√©es peut prendre du temps. Pour acc√©l√©rer les it√©rations pendant le d√©veloppement :\n",
    "\n",
    "T√¢che : s√©lectionnez un sous-ensemble plus petit du DataFrame (par exemple, les 1 000 premi√®res lignes).\n",
    "Cette approche vous permet de tester efficacement votre code avant de le mettre √† l‚Äô√©chelle sur l‚Äôensemble de donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8eb31a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb==0.3.21\n",
      "  Using cached chromadb-0.3.21-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: pandas>=1.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (2.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (2.11.7)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (1.26.4)\n",
      "Collecting clickhouse-connect>=0.5.7\n",
      "  Using cached clickhouse_connect-0.8.18-cp310-cp310-win_amd64.whl (247 kB)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (2.32.4)\n",
      "Collecting hnswlib>=0.7\n",
      "  Using cached hnswlib-0.8.0.tar.gz (36 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: sentence-transformers>=2.2.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (5.0.0)\n",
      "Collecting duckdb>=0.7.1\n",
      "  Using cached duckdb-1.3.2-cp310-cp310-win_amd64.whl (11.4 MB)\n",
      "Collecting fastapi>=0.85.1\n",
      "  Using cached fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Collecting posthog>=2.4.0\n",
      "  Using cached posthog-6.1.0-py3-none-any.whl (109 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: zstandard in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (0.23.0)\n",
      "Collecting lz4\n",
      "  Using cached lz4-4.4.4-cp310-cp310-win_amd64.whl (99 kB)\n",
      "Requirement already satisfied: pytz in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2025.2)\n",
      "Requirement already satisfied: urllib3>=1.26 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2025.6.15)\n",
      "Collecting starlette<0.48.0,>=0.40.0\n",
      "  Using cached starlette-0.47.1-py3-none-any.whl (72 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.85.1->chromadb==0.3.21) (4.14.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.3->chromadb==0.3.21) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.3->chromadb==0.3.21) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.3.21) (1.17.0)\n",
      "Collecting distro>=1.5.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting backoff>=1.10.0\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb==0.3.21) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb==0.3.21) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb==0.3.21) (0.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.3.21) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.3.21) (3.4.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (2.7.1+cu118)\n",
      "Requirement already satisfied: Pillow in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (11.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.33.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.7.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.53.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.16.0)\n",
      "Collecting httptools>=0.6.3\n",
      "  Using cached httptools-0.6.4-cp310-cp310-win_amd64.whl (88 kB)\n",
      "Collecting watchfiles>=0.13\n",
      "  Using cached watchfiles-1.1.0-cp310-cp310-win_amd64.whl (292 kB)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.4.6)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (15.0.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (1.1.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2025.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (4.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.14.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.21.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.6.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (1.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.0.2)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (pyproject.toml): started\n",
      "  Building wheel for hnswlib (pyproject.toml): finished with status 'error'\n",
      "Failed to build hnswlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó Building wheel for hnswlib (pyproject.toml) did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [5 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'hnswlib' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hnswlib\n",
      "ERROR: Could not build wheels for hnswlib, which is required to install pyproject.toml-based projects\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Using cached chromadb-1.0.15-cp39-abi3-win_amd64.whl (19.5 MB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (14.0.0)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Using cached opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
      "Collecting mmh3>=4.0.1\n",
      "  Using cached mmh3-5.1.0-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Using cached kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (2.11.7)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
      "Collecting posthog<6.0.0,>=2.4.0\n",
      "  Using cached posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Collecting build>=1.0.3\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.16.0)\n",
      "Collecting jsonschema>=4.19.0\n",
      "  Using cached jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "Collecting overrides>=7.3.1\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Using cached bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.21.2)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.73.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (4.14.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Collecting onnxruntime>=1.14.1\n",
      "  Using cached onnxruntime-1.22.1-cp310-cp310-win_amd64.whl (12.7 MB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
      "Requirement already satisfied: anyio in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
      "Collecting referencing>=0.28.4\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: requests in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Collecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting googleapis-common-protos~=1.57\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Collecting opentelemetry-proto==1.35.0\n",
      "  Using cached opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.35.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-6.31.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.56b0\n",
      "  Using cached opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
      "Collecting backoff>=1.10.0\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting distro>=1.5.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.33.4)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Collecting watchfiles>=0.13\n",
      "  Using cached watchfiles-1.1.0-cp310-cp310-win_amd64.whl (292 kB)\n",
      "Collecting httptools>=0.6.3\n",
      "  Using cached httptools-0.6.4-cp310-cp310-win_amd64.whl (88 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Installing collected packages: referencing, protobuf, overrides, mmh3, importlib-resources, importlib-metadata, humanfriendly, httptools, distro, build, bcrypt, backoff, uvicorn, posthog, opentelemetry-proto, opentelemetry-api, jsonschema-specifications, googleapis-common-protos, coloredlogs, watchfiles, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, jsonschema, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acc√®s refus√©: 'C:\\\\Users\\\\chume\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\google\\\\~~otobuf\\\\internal\\\\_api_implementation.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb==0.3.21\n",
    "!pip install -U chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3c766a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20.3\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.31.1\n",
      "    Uninstalling protobuf-6.31.1:\n",
      "      Successfully uninstalled protobuf-6.31.1\n",
      "Successfully installed protobuf-3.20.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires keras<2.14,>=2.13.1, but you have keras 2.10.0 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires tensorboard<2.14,>=2.13, but you have tensorboard 2.10.1 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires tensorflow-estimator<2.14,>=2.13.0, but you have tensorflow-estimator 2.10.0 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.14.1 which is incompatible.\n",
      "tensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade protobuf==3.20.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b9a9633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier trouv√©, chargement...\n",
      "\n",
      "Aper√ßu des donn√©es :\n",
      "     topic                                               link          domain  \\\n",
      "0  SCIENCE  https://www.eurekalert.org/pub_releases/2020-0...  eurekalert.org   \n",
      "1  SCIENCE  https://www.pulse.ng/news/world/an-irresistibl...        pulse.ng   \n",
      "2  SCIENCE  https://www.express.co.uk/news/science/1322607...   express.co.uk   \n",
      "3  SCIENCE  https://www.ndtv.com/world-news/glaciers-could...        ndtv.com   \n",
      "4  SCIENCE  https://www.thesun.ie/tech/5742187/perseid-met...       thesun.ie   \n",
      "\n",
      "        published_date                                              title  \\\n",
      "0  2020-08-06 13:59:45  A closer look at water-splitting's solar fuel ...   \n",
      "1  2020-08-12 15:14:19  An irresistible scent makes locusts swarm, stu...   \n",
      "2  2020-08-13 21:01:00  Artificial intelligence warning: AI will know ...   \n",
      "3  2020-08-03 22:18:26   Glaciers Could Have Sculpted Mars Valleys: Study   \n",
      "4  2020-08-12 19:54:36  Perseid meteor shower 2020: What time and how ...   \n",
      "\n",
      "  lang     id  \n",
      "0   en  doc_0  \n",
      "1   en  doc_1  \n",
      "2   en  doc_2  \n",
      "3   en  doc_3  \n",
      "4   en  doc_4  \n",
      "\n",
      "Infos g√©n√©rales :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108774 entries, 0 to 108773\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   topic           108774 non-null  object\n",
      " 1   link            108774 non-null  object\n",
      " 2   domain          108774 non-null  object\n",
      " 3   published_date  108774 non-null  object\n",
      " 4   title           108774 non-null  object\n",
      " 5   lang            108774 non-null  object\n",
      " 6   id              108774 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 5.8+ MB\n",
      "None\n",
      "\n",
      "Sous-ensemble pr√™t : (1000, 7)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Chemin correct vers le fichier CSV\n",
    "path = r\"C:\\Users\\chume\\Desktop\\W06_NLP_LLM\\Day4_Chatbots\\cache\\labelled_newscatcher_dataset.csv\"\n",
    "\n",
    "# V√©rification pr√©sence fichier\n",
    "if os.path.isfile(path):\n",
    "    print(\"Fichier trouv√©, chargement...\")\n",
    "    pdf = pd.read_csv(path, sep=';')\n",
    "\n",
    "    \n",
    "    # Ajout ID si absent\n",
    "    if \"id\" not in pdf.columns:\n",
    "        pdf[\"id\"] = [f\"doc_{i}\" for i in range(len(pdf))]\n",
    "    \n",
    "    # Aper√ßu rapide\n",
    "    print(\"\\nAper√ßu des donn√©es :\")\n",
    "    print(pdf.head())\n",
    "    print(\"\\nInfos g√©n√©rales :\")\n",
    "    print(pdf.info())\n",
    "    \n",
    "    # Sous-ensemble rapide\n",
    "    pdf_small = pdf.head(1000)\n",
    "    print(f\"\\nSous-ensemble pr√™t : {pdf_small.shape}\")\n",
    "else:\n",
    "    print(f\"‚ùå Fichier non trouv√© : {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276fa7cb",
   "metadata": {},
   "source": [
    "## üåü Exercice 2 : Vectorisation avec des transformateurs de phrases\n",
    "Dans cet exercice, nous allons transformer nos donn√©es textuelles (titres d'actualit√©s) en repr√©sentations num√©riques appel√©es int√©grations . Cette √©tape est cruciale pour permettre aux machines de comprendre et d'exploiter les donn√©es textuelles dans des t√¢ches telles que la recherche de similarit√©, le clustering et l'apprentissage automatique. Nous utiliserons Sentence Transformers , une biblioth√®que populaire pour g√©n√©rer des repr√©sentations vectorielles denses de texte.\n",
    "\n",
    "\n",
    "\n",
    "Pourquoi cette √©tape est importante :\n",
    "Les machines ne peuvent pas traiter directement le texte brut ; elles ont besoin d'une entr√©e num√©rique. Les int√©grations sont des vecteurs denses qui capturent le sens et le contexte du texte. En g√©n√©rant des int√©grations pour nos titres d'actualit√©s, nous les rendons utilisables pour des t√¢ches en aval, telles que les recherches de similarit√© ou l'alimentation de mod√®les d'apprentissage automatique.\n",
    "\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Installer et importer la biblioth√®que de transformateurs de phrases\n",
    "\n",
    "La sentence_transformersbiblioth√®que fournit des m√©thodes faciles √† utiliser pour g√©n√©rer des int√©grations au niveau des phrases.\n",
    "\n",
    "\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "\n",
    "InputExample:Une classe utilitaire qui aide √† formater les entr√©es de donn√©es pour la formation ou l'inf√©rence avec des transformateurs de phrases.\n",
    "2. Pr√©parez les donn√©es pour la g√©n√©ration d'int√©gration\n",
    "\n",
    "Nous allons appliquer une fonction d'assistance au sous-ensemble de notre DataFrame cr√©√© pr√©c√©demment. Cette fonction formate chaque ligne en un InputExampleobjet, n√©cessaire au processus d'int√©gration.\n",
    "\n",
    "T√¢che : extraire le sous-ensemble du DataFrame (par exemple, pdf_subset) pour lequel vous souhaitez g√©n√©rer des int√©grations.\n",
    "\n",
    "\n",
    "pdf_subset = ...  # Use the subset created in the previous exercise.\n",
    "\n",
    "\n",
    "3. Cr√©er une fonction d'assistance\n",
    "\n",
    "Cette fonction convertit chaque enregistrement (titre d'actualit√©) au format appropri√© ( InputExample) requis par le mod√®le Sentence Transformer.\n",
    "\n",
    "\n",
    "\n",
    "def example_create_fn(doc1: pd.Series) -> InputExample:\n",
    "    \"\"\"\n",
    "    Helper function that outputs a sentence_transformer guid, label, and text.\n",
    "    \"\"\"\n",
    "    return ...  # Format and return the InputExample.\n",
    "\n",
    "\n",
    "La fonction prendra une ligne (dans ce cas, le titre d'un article d'actualit√©) et la formatera correctement pour le processus de g√©n√©ration d'int√©gration.\n",
    "4. Appliquer la fonction d'assistance au sous-ensemble\n",
    "\n",
    "Nous appliquerons cette fonction sur le sous-ensemble DataFrame pour g√©n√©rer une liste d' InputExampleobjets :\n",
    "\n",
    "\n",
    "\n",
    "faiss_train_examples = pdf_subset.apply(lambda x: example_create_fn(x[\"title\"]), axis=1).tolist()\n",
    "faiss_train_examples[:10]\n",
    "\n",
    "\n",
    "Cela pr√©pare les donn√©es pour la g√©n√©ration d'int√©gration en convertissant chaque titre d'actualit√© en un format structur√©.\n",
    "\n",
    "5. Initialiser le mod√®le d'int√©gration\n",
    "\n",
    "Nous utiliserons le mod√®le pr√©-entra√Æn√© all-MiniLM-L6-v2, qui fournit des int√©grations de haute qualit√© pour une large gamme de t√¢ches de traitement du langage naturel (NLP).\n",
    "\n",
    "T√¢che : Initialiser le mod√®le.\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    ...  # Specify the model name.\n",
    ")\n",
    "\n",
    "\n",
    "Cette √©tape charge le mod√®le en m√©moire, pr√™t pour la g√©n√©ration d‚Äôint√©gration.\n",
    "6. Extraire les titres et les convertir en une liste de cha√Ænes\n",
    "\n",
    "Extrayez la colonne ¬´ titre ¬ª de votre sous-ensemble DataFrame et convertissez-la en liste. Il s'agit des donn√©es textuelles brutes que nous allons int√©grer.\n",
    "\n",
    "T√¢che : Convertir les titres en une liste de cha√Ænes.\n",
    "\n",
    "\n",
    "# Example (fill in appropriately):\n",
    "titles_list = pdf_subset[\"title\"].tolist()\n",
    "\n",
    "\n",
    "7. G√©n√©rer des int√©grations pour les titres\n",
    "\n",
    "√Ä l‚Äôaide du mod√®le initialis√©, g√©n√©rez des int√©grations pour chaque titre :\n",
    "\n",
    "\n",
    "\n",
    "faiss_title_embedding =  # Generate embeddings for the list of titles.\n",
    "\n",
    "\n",
    "Cette √©tape transforme chaque titre en un vecteur dense qui capture sa signification s√©mantique.\n",
    "\n",
    "\n",
    "8. V√©rifiez les dimensions d'int√©gration\n",
    "\n",
    "Pour v√©rifier que les int√©grations ont √©t√© g√©n√©r√©es correctement, v√©rifiez la forme de la sortie :\n",
    "\n",
    "\n",
    "\n",
    "len(faiss_title_embedding), len(faiss_title_embedding[0])\n",
    "\n",
    "\n",
    "Cela confirme le nombre d'int√©grations dont vous disposez (une par titre) et la dimensionnalit√© de chaque vecteur d'int√©gration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c77daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 122.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'int√©grations g√©n√©r√©es : 1000\n",
      "Dimensions de chaque vecteur : 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercice 2 : Vectorisation des titres avec Sentence Transformers\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import InputExample, SentenceTransformer\n",
    "\n",
    "# 1.  Utilisation du sous-ensemble du pr√©c√©dent exercice\n",
    "pdf_subset = pdf_small  # le sous-ensemble de 1000 lignes\n",
    "\n",
    "# 2. Fonction d'aide : transforme chaque ligne en InputExample\n",
    "def example_create_fn(doc1: str) -> InputExample:\n",
    "    \"\"\"\n",
    "    Prend un titre (texte), retourne un InputExample format√©.\n",
    "    \"\"\"\n",
    "    return InputExample(guid=None, texts=[doc1], label=0.0)\n",
    "\n",
    "# 3. Conversion du DataFrame en liste d'InputExample\n",
    "faiss_train_examples = pdf_subset[\"title\"].apply(example_create_fn).tolist()\n",
    "\n",
    "# 4. Initialisation du mod√®le Sentence Transformer (pr√™t √† l'emploi)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 5. Extraction des titres en liste de cha√Ænes simples\n",
    "titles_list = pdf_subset[\"title\"].tolist()\n",
    "\n",
    "# 6. G√©n√©ration des embeddings\n",
    "faiss_title_embedding = model.encode(titles_list, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# 7. V√©rification de la taille des embeddings\n",
    "print(f\"Nombre d'int√©grations g√©n√©r√©es : {len(faiss_title_embedding)}\")\n",
    "print(f\"Dimensions de chaque vecteur : {len(faiss_title_embedding[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a08eb",
   "metadata": {},
   "source": [
    "## üåü Exercice 3 : Indexation et recherche FAISS\n",
    "Dans cet exercice, nous utiliserons FAISS (Facebook AI Similarity Search) pour cr√©er un index des repr√©sentations vectorielles continues g√©n√©r√©es lors de l'exercice pr√©c√©dent. Cela nous permettra d'effectuer des recherches de similarit√© rapides et efficaces sur de vastes collections de vecteurs. L'objectif est de r√©cup√©rer les articles d'actualit√© les plus pertinents en fonction de la requ√™te d'un utilisateur.\n",
    "\n",
    "\n",
    "\n",
    "Pourquoi cette √©tape est importante :\n",
    "FAISS est une biblioth√®que con√ßue pour effectuer des recherches de similarit√© √† grande √©chelle. L'utilisation d'embeddings (vecteurs de grande dimension) complique consid√©rablement la recherche efficace. FAISS propose des algorithmes optimis√©s d'indexation et de recherche, permettant de retrouver des √©l√©ments similaires en quelques millisecondes, m√™me √† partir de grands ensembles de donn√©es.\n",
    "\n",
    "\n",
    "\n",
    "Instructions:\n",
    "Installer et importer la biblioth√®que FAISS\n",
    "Si vous ne l'avez pas d√©j√† fait, assurez-vous que FAISS est install√© et importez les modules n√©cessaires :\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "\n",
    "numpy:Pour g√©rer les tableaux et les op√©rations matricielles.\n",
    "faiss: Pour construire et interroger l'index vectoriel.\n",
    "\n",
    "\n",
    "2. Pr√©parez les donn√©es pour l'indexation\n",
    "\n",
    "Utilisez les vecteurs d‚Äôint√©gration g√©n√©r√©s √† partir de l‚Äôexercice pr√©c√©dent et pr√©parez-les pour l‚Äôindexation :\n",
    "\n",
    "\n",
    "\n",
    "pdf_to_index =  # This should be your subset DataFrame containing the articles.\n",
    "id_index =  # An array of IDs corresponding to each article.\n",
    "\n",
    "\n",
    "pdf_to_index: Le sous-ensemble du DataFrame que nous voulons indexer.\n",
    "id_index:Un tableau d‚Äôidentifiants uniques pour chaque vecteur d‚Äôint√©gration.\n",
    "\n",
    "\n",
    "3. Normaliser les vecteurs d'int√©gration\n",
    "\n",
    "Pour effectuer une recherche de similarit√© cosinus (qui mesure l'angle entre les vecteurs plut√¥t que leur distance), nous devons d'abord normaliser les vecteurs d'int√©gration :\n",
    "\n",
    "\n",
    "\n",
    "content_encoded_normalized =  # Embedding vectors.\n",
    "faiss.normalize_L2(content_encoded_normalized)\n",
    "\n",
    "\n",
    "La normalisation garantit que les vecteurs ont une longueur unitaire, ce qui est n√©cessaire pour que la similarit√© cosinus fonctionne correctement.\n",
    "\n",
    "\n",
    "4. Cr√©er l'index FAISS\n",
    "\n",
    "FAISS propose diff√©rents types d'index selon la mesure de similarit√© et les exigences de recherche. Nous utiliserons un IndexFlatIP (produit interne) encapsul√© dans un IndexIDMap :\n",
    "\n",
    "\n",
    "\n",
    "index_content = faiss.IndexIDMap(faiss.IndexFlatIP(len(faiss_title_embedding[0])))\n",
    "index_content.add_with_ids(content_encoded_normalized, id_index)\n",
    "\n",
    "\n",
    "IndexFlatIP: Un type d'index qui utilise le produit interne (qui est √©quivalent √† la similarit√© cosinus pour les vecteurs normalis√©s).\n",
    "IndexIDMap: Les r√©sultats de recherche de cartes renvoient aux identifiants d'origine, garantissant ainsi que nous pouvons r√©cup√©rer les articles correspondants.\n",
    "Cette √©tape cr√©e l‚Äôindex et ajoute les vecteurs normalis√©s avec leurs identifiants.\n",
    "\n",
    "\n",
    "\n",
    "5. Impl√©menter une fonction de recherche\n",
    "\n",
    "Ensuite, nous allons d√©finir une fonction search_contentqui prend une requ√™te utilisateur et r√©cup√®re les articles les plus similaires de l'index :\n",
    "\n",
    "\n",
    "\n",
    "def search_content(query, pdf_to_index, k=3):\n",
    "    query_vector =  # Encode the query string into an embedding vector.\n",
    "    faiss.normalize_L2(query_vector)  # Normalize the query vector.\n",
    "\n",
    "    # Perform the search\n",
    "    top_k =  # The top-k similar vectors.\n",
    "    ids =  # The IDs of the matching vectors.\n",
    "    similarities =  # Similarity scores for the matches.\n",
    "\n",
    "    results =  # Retrieve the matching articles from pdf_to_index.\n",
    "    results[\"similarities\"] = similarities  # Add similarity scores.\n",
    "    return results\n",
    "\n",
    "\n",
    "Cette fonction encode la requ√™te de l'utilisateur dans un vecteur, recherche l'index FAISS, r√©cup√®re les k vecteurs les plus similaires et renvoie les articles correspondants ainsi que leurs scores de similarit√©.\n",
    "\n",
    "\n",
    "6. Testez la fonction de recherche\n",
    "\n",
    "Utilisez la fonction de recherche pour trouver des articles li√©s √† un exemple de requ√™te :\n",
    "\n",
    "\n",
    "\n",
    "display(search_content(\"animal\", pdf_to_index, k=5))\n",
    "\n",
    "\n",
    "Cela vous permet de v√©rifier que le processus de recherche fonctionne et renvoie des articles pertinents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6991aeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index FAISS construit avec 1000 vecteurs.\n",
      "\n",
      "R√©sultats les plus similaires :\n",
      "                                                 title  similarity\n",
      "176  Random: You Can Pick Up and Pet Cats in Assass...    0.391902\n",
      "975  Researchers explore social behavior of animals...    0.376784\n",
      "99               Ghostwire: Tokyo confirms dog petting    0.344058\n",
      "928                 Just Let This Lizard Be a Dinosaur    0.317387\n",
      "762  'Secret' life of sharks: Study reveals their s...    0.295497\n"
     ]
    }
   ],
   "source": [
    "# Exercice 3 : Indexation et recherche avec FAISS\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# 1. Pr√©paration des donn√©es pour FAISS\n",
    "pdf_to_index = pdf_subset.copy()  # on indexe le m√™me sous-ensemble que pr√©c√©demment\n",
    "id_index = np.array([i for i in range(len(pdf_to_index))]).astype(np.int64)\n",
    "\n",
    "# 2. Normalisation des embeddings pour la similarit√© cosinus\n",
    "content_encoded_normalized = faiss_title_embedding.copy().astype('float32')\n",
    "faiss.normalize_L2(content_encoded_normalized)\n",
    "\n",
    "# 3. Cr√©ation de l'index FAISS (produit interne avec identifiants mapp√©s)\n",
    "dimension = content_encoded_normalized.shape[1]\n",
    "index_content = faiss.IndexIDMap(faiss.IndexFlatIP(dimension))\n",
    "\n",
    "# Ajout des vecteurs √† l'index\n",
    "index_content.add_with_ids(content_encoded_normalized, id_index)\n",
    "\n",
    "print(f\"Index FAISS construit avec {index_content.ntotal} vecteurs.\")\n",
    "\n",
    "# 4. Fonction de recherche\n",
    "def search_content(query, pdf_to_index, k=3):\n",
    "    \"\"\"\n",
    "    Recherche les k documents les plus similaires √† la requ√™te.\n",
    "    \"\"\"\n",
    "    # Encodage et normalisation de la requ√™te\n",
    "    query_vector = model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    faiss.normalize_L2(query_vector)\n",
    "\n",
    "    # Recherche FAISS\n",
    "    distances, indices = index_content.search(query_vector, k)\n",
    "\n",
    "    # Extraction des r√©sultats\n",
    "    results = pdf_to_index.iloc[indices[0]].copy()\n",
    "    results[\"similarity\"] = distances[0]\n",
    "    return results\n",
    "\n",
    "# 5. Exemple de recherche\n",
    "resultats = search_content(\"animal\", pdf_to_index, k=5)\n",
    "print(\"\\nR√©sultats les plus similaires :\")\n",
    "print(resultats[[\"title\", \"similarity\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa217c8d",
   "metadata": {},
   "source": [
    "## üåü Exercice 4 : Collecte et interrogation de ChromaDB\n",
    "Dans cet exercice, nous pr√©senterons ChromaDB , une base de donn√©es vectorielle open source con√ßue pour stocker, indexer et interroger les vecteurs d'incorporation. ChromaDB simplifie l'utilisation des incorporations et, contrairement √† FAISS, g√®re automatiquement la tokenisation, l'incorporation et l'indexation sans n√©cessiter de g√©n√©ration manuelle. Cela la rend id√©ale pour l'int√©gration avec les applications LLM (Large Language Model), notamment pour la cr√©ation de syst√®mes de questions-r√©ponses ou de moteurs de recherche.\n",
    "\n",
    "\n",
    "\n",
    "Pourquoi cette √©tape est importante :\n",
    "Une fois les int√©grations g√©n√©r√©es pour nos donn√©es, l'√©tape logique suivante consiste √† les stocker et √† les interroger efficacement. ChromaDB offre une interface de gestion avanc√©e des int√©grations et prend en charge les m√©tadonn√©es, ce qui en fait un outil id√©al pour la cr√©ation d'applications telles que la recherche documentaire ou les syst√®mes de questions-r√©ponses. Gr√¢ce √† ChromaDB, nous d√©montrons comment int√©grer les int√©grations dans un workflow concret prenant en charge l'interrogation et la r√©cup√©ration de documents pertinents.\n",
    "\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Installer et importer la biblioth√®que ChromaDB\n",
    "\n",
    "Assurez-vous que ChromaDB est install√© et importez les composants n√©cessaires :\n",
    "\n",
    "\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "chromadb:La biblioth√®que principale pour la gestion des collections de vecteurs et des requ√™tes.\n",
    "Settings: Options de configuration pour ChromaDB.\n",
    "\n",
    "\n",
    "2. Initialiser un client ChromaDB et cr√©er une collection\n",
    "\n",
    "ChromaDB organise les vecteurs en collections , similaires aux tables d'une base de donn√©es. Chaque collection contient un ensemble de documents (vecteurs) et les m√©tadonn√©es associ√©es.\n",
    "\n",
    "\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection_name = \"my_news\"\n",
    "\n",
    "# If a collection with the same name exists, delete it to avoid conflicts\n",
    "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "print(f\"Creating collection: '{collection_name}'\")\n",
    "collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "\n",
    "Ce code initialise le client ChromaDB, v√©rifie si une collection nomm√©e ¬´ my_news ¬ª existe d√©j√†, la supprime si c'est le cas et cr√©e une nouvelle collection.\n",
    "Les collections stockent √† la fois des documents (le texte ou les incorporations) et des m√©tadonn√©es (par exemple, les √©tiquettes de sujet).\n",
    "\n",
    "\n",
    "3. Ajouter des donn√©es √† la collection\n",
    "\n",
    "ChromaDB simplifie l'ingestion des donn√©es en g√©n√©rant automatiquement des int√©grations si vous ne fournissez pas de mod√®le d'int√©gration personnalis√©. Il utilise le mod√®le par d√©faut SentenceTransformerEmbeddingFunction, qui g√®re la tokenisation, l'int√©gration et l'indexation.\n",
    "\n",
    "T√¢che : Ajouter les 100 premiers titres d'actualit√© du sous-ensemble DataFrame √† la collection. Inclure le sujet correspondant √† chaque titre comme m√©tadonn√©es et attribuer un identifiant unique √† chaque document.\n",
    "\n",
    "\n",
    "# Display the DataFrame subset (for reference)\n",
    "display(pdf_subset)\n",
    "\n",
    "collection.add(\n",
    "    documents=pdf_subset[\"title\"][:100].tolist(),\n",
    "    metadatas=[{\"topic\": topic} for topic in pdf_subset[\"topic\"][:100].tolist()],\n",
    "    ids=...  # Provide a list of unique IDs.\n",
    ")\n",
    "\n",
    "\n",
    "Le documentsparam√®tre contient la liste des titres d'actualit√©s.\n",
    "Le metadatasparam√®tre contient les rubriques associ√©es en tant que m√©tadonn√©es.\n",
    "Le idsparam√®tre doit √™tre une liste d' identifiants uniques (par exemple, des cha√Ænes ou des entiers) pour chaque document.\n",
    "Remarque : l‚Äôajout de donn√©es √† la collection peut prendre du temps en fonction du volume de donn√©es, car ChromaDB traite et indexe le texte en arri√®re-plan.\n",
    "\n",
    "\n",
    "\n",
    "4. Interroger la collection\n",
    "\n",
    "Enfin, effectuez une requ√™te de recherche pour r√©cup√©rer les documents les plus pertinents en fonction d‚Äôun terme de recherche.\n",
    "\n",
    "T√¢che : Interroger la collection √† l'aide d'un terme (par exemple, ¬´ espace ¬ª ) et r√©cup√©rer les 10 documents les plus pertinents .\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "results = ...  # Perform the search query.\n",
    "\n",
    "print(json.dumps(results, indent=4))\n",
    "\n",
    "\n",
    "Le terme de recherche (par exemple, ¬´ espace ¬ª) est automatiquement converti en une incorporation par ChromaDB, et la collection renvoie les 10 voisins les plus proches , c'est-√†-dire les documents les plus s√©mantiquement similaires √† la requ√™te.\n",
    "Les r√©sultats incluent les documents correspondants, leurs m√©tadonn√©es et leurs scores de similarit√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f83f0fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb==0.3.21\n",
      "  Using cached chromadb-0.3.21-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (2.32.4)\n",
      "Collecting hnswlib>=0.7\n",
      "  Using cached hnswlib-0.8.0.tar.gz (36 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fastapi>=0.85.1\n",
      "  Using cached fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Collecting posthog>=2.4.0\n",
      "  Using cached posthog-6.1.0-py3-none-any.whl (109 kB)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (1.26.4)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (2.11.7)\n",
      "Requirement already satisfied: pandas>=1.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (2.3.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (5.0.0)\n",
      "Collecting duckdb>=0.7.1\n",
      "  Using cached duckdb-1.3.2-cp310-cp310-win_amd64.whl (11.4 MB)\n",
      "Collecting clickhouse-connect>=0.5.7\n",
      "  Using cached clickhouse_connect-0.8.18-cp310-cp310-win_amd64.whl (247 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2025.6.15)\n",
      "Requirement already satisfied: urllib3>=1.26 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2.5.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2025.2)\n",
      "Collecting lz4\n",
      "  Using cached lz4-4.4.4-cp310-cp310-win_amd64.whl (99 kB)\n",
      "Requirement already satisfied: zstandard in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (0.23.0)\n",
      "Collecting starlette<0.48.0,>=0.40.0\n",
      "  Using cached starlette-0.47.1-py3-none-any.whl (72 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.85.1->chromadb==0.3.21) (4.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.3->chromadb==0.3.21) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.3->chromadb==0.3.21) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.3.21) (1.17.0)\n",
      "Collecting distro>=1.5.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting backoff>=1.10.0\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb==0.3.21) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb==0.3.21) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb==0.3.21) (0.7.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.3.21) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.3.21) (3.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.33.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.53.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.7.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (2.7.1+cu118)\n",
      "Requirement already satisfied: scipy in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (11.3.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.16.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (1.1.1)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.4.6)\n",
      "Collecting httptools>=0.6.3\n",
      "  Using cached httptools-0.6.4-cp310-cp310-win_amd64.whl (88 kB)\n",
      "Collecting watchfiles>=0.13\n",
      "  Using cached watchfiles-1.1.0-cp310-cp310-win_amd64.whl (292 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (15.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (6.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (24.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (4.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.1.6)\n",
      "Requirement already satisfied: networkx in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.4.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.14.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.21.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2024.11.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.5.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (1.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.0.2)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (pyproject.toml): started\n",
      "  Building wheel for hnswlib (pyproject.toml): finished with status 'error'\n",
      "Failed to build hnswlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó Building wheel for hnswlib (pyproject.toml) did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [5 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'hnswlib' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hnswlib\n",
      "ERROR: Could not build wheels for hnswlib, which is required to install pyproject.toml-based projects\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb==0.3.21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a447ca6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chromadb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# üìå Exercice 4 : Collecte et interrogation avec ChromaDB\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Settings\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'"
     ]
    }
   ],
   "source": [
    "# üìå Exercice 4 : Collecte et interrogation avec ChromaDB\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import json\n",
    "\n",
    "# 1. ‚úÖ Initialisation du client ChromaDB\n",
    "chroma_client = chromadb.Client(Settings())\n",
    "\n",
    "collection_name = \"my_news\"\n",
    "\n",
    "# ‚úÖ Suppression si collection existante\n",
    "collections_existantes = [col.name for col in chroma_client.list_collections()]\n",
    "if collection_name in collections_existantes:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "print(f\"‚úÖ Cr√©ation de la collection : '{collection_name}'\")\n",
    "collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "# 2. ‚úÖ Ajout des donn√©es (100 premiers titres) avec m√©tadonn√©es\n",
    "documents = pdf_subset[\"title\"][:100].tolist()\n",
    "metadatas = [{\"topic\": topic} for topic in pdf_subset[\"topic\"][:100].tolist()]\n",
    "ids = [f\"id_{i}\" for i in range(100)]\n",
    "\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")\n",
    "print(f\"‚úÖ Ajout de {len(documents)} documents dans ChromaDB.\")\n",
    "\n",
    "# 3. ‚úÖ Recherche sur la collection\n",
    "query = \"espace\"  # terme de recherche\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=10\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ R√©sultats de la recherche :\")\n",
    "print(json.dumps(results, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54788d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 137.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index FAISS cr√©√© avec 1000 vecteurs\n",
      "                                                 title     score\n",
      "241  2021 Maserati Ghibli Trofeo, Quattroporte Trof...  0.286445\n",
      "74   Blasphemous gets a new storyline, New Game Plu...  0.275283\n",
      "262  Halo: The Master Chief Collection Crossplay Su...  0.256196\n",
      "927  Tennocon 2020 recap: The future of Warframe re...  0.228718\n",
      "468  Ghost of Tsushima Has the Most Impressive Meta...  0.225673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ‚úÖ 1. Chargement des donn√©es\n",
    "path = r\"C:\\Users\\chume\\Desktop\\W06_NLP_LLM\\Day4_Chatbots\\cache\\labelled_newscatcher_dataset.csv\"\n",
    "#pdf = pd.read_csv(path, sep=',')\n",
    "pdf = pd.read_csv(path, sep=';')\n",
    "\n",
    "\n",
    "# Sous-ensemble pour tests rapides\n",
    "pdf_small = pdf.head(1000).copy()\n",
    "pdf_small[\"id\"] = [f\"doc_{i}\" for i in range(len(pdf_small))]\n",
    "\n",
    "# ‚úÖ 2. Embedding avec Sentence Transformers\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "titles = pdf_small[\"title\"].tolist()\n",
    "embeddings = model.encode(titles, convert_to_numpy=True, show_progress_bar=True).astype('float32')\n",
    "\n",
    "# ‚úÖ 3. Normalisation pour similarit√© cosinus\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# ‚úÖ 4. Indexation FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index = faiss.IndexIDMap(index)\n",
    "ids = np.arange(len(pdf_small)).astype(np.int64)\n",
    "index.add_with_ids(embeddings, ids)\n",
    "\n",
    "print(f\"Index FAISS cr√©√© avec {index.ntotal} vecteurs\")\n",
    "\n",
    "# ‚úÖ 5. Fonction recherche\n",
    "def search_faiss(query, k=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = pdf_small.iloc[indices[0]].copy()\n",
    "    results[\"score\"] = distances[0]\n",
    "    return results[[\"title\", \"score\"]]\n",
    "\n",
    "# ‚úÖ 6. Ex√©cution d‚Äôune recherche exemple\n",
    "print(search_faiss(\"climat\", k=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f70d7",
   "metadata": {},
   "source": [
    "## Exercice 5 : R√©pondre aux questions avec un mod√®le de visage qui fait un c√¢lin\n",
    "Dans cet exercice, nous allons rassembler tous ces √©l√©ments en construisant un syst√®me de questions-r√©ponses (Q/R) utilisant un mod√®le de langage Hugging Face . En combinant la r√©cup√©ration de documents (via ChromaDB) et la g√©n√©ration de texte (via Hugging Face), nous cr√©ons un pipeline simple mais puissant o√π un mod√®le g√©n√®re des r√©ponses en fonction du contexte pertinent.\n",
    "\n",
    "\n",
    "\n",
    "Pourquoi cette √©tape est importante :\n",
    "R√©cup√©rer des documents pertinents n'est que la moiti√© du travail. L'√©tape suivante consiste √† g√©n√©rer des r√©ponses pertinentes √† partir du contenu r√©cup√©r√©. Il s'agit d'une technique essentielle des syst√®mes modernes de g√©n√©ration augment√©e de recherche (RAG) , o√π un mod√®le linguistique exploite √† la fois des connaissances pr√©-entra√Æn√©es et des informations externes pour r√©pondre aux questions avec plus de pr√©cision. En int√©grant ChromaDB et les transformateurs Hugging Face , nous simulons un pipeline de questions/r√©ponses r√©el.\n",
    "\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Installer et importer la biblioth√®que Transformers\n",
    "\n",
    "La biblioth√®que Hugging Face transformersdonne acc√®s √† une vari√©t√© de mod√®les de langage pr√©-entra√Æn√©s.\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "AutoTokenizer: Charge automatiquement le tokenizer appropri√© pour le mod√®le s√©lectionn√©.\n",
    "AutoModelForCausalLM: Charge un mod√®le de langage causal (tel que GPT-2) pour la g√©n√©ration de texte.\n",
    "pipeline:Une interface de haut niveau pour les t√¢ches courantes comme la g√©n√©ration de texte.\n",
    "\n",
    "\n",
    "2. Initialiser le mod√®le et le tokenizer\n",
    "\n",
    "S√©lectionnez un mod√®le pr√©-entra√Æn√© pour la g√©n√©ration de texte (par exemple, GPT-2 ou un mod√®le de langage causal similaire) et initialisez √† la fois le mod√®le et son tokeniseur :\n",
    "\n",
    "\n",
    "\n",
    "model_id =  # Specify the Hugging Face model ID (e.g., 'gpt2').\n",
    "tokenizer =  # Load the tokenizer for the model.\n",
    "lm_model =  # Load the causal language model.\n",
    "\n",
    "\n",
    "Le mod√®le g√©n√®re du texte en fonction des entr√©es fournies.\n",
    "Le tokeniseur convertit entre le texte brut et le format tokenis√© requis par le mod√®le.\n",
    "3. Cr√©er un pipeline de g√©n√©ration de texte\n",
    "\n",
    "Configurez un pipeline pour la g√©n√©ration de texte, qui encapsule le mod√®le et le tokeniseur dans une interface pratique :\n",
    "\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,  # Maximum number of tokens to generate.\n",
    "    device_map=\"auto\",   # Automatically uses available GPU/CPU resources.\n",
    ")\n",
    "\n",
    "\n",
    "Cela simplifie l‚Äôex√©cution de l‚Äôinf√©rence et la g√©n√©ration de texte avec le mod√®le.\n",
    "4. Construisez un mod√®le d'invite\n",
    "\n",
    "L'invite inclut √† la fois le contexte r√©cup√©r√© (depuis ChromaDB) et la question de l'utilisateur . Ainsi, le mod√®le g√©n√®re une r√©ponse bas√©e sur les documents pertinents.\n",
    "\n",
    "\n",
    "\n",
    "question =  # Define the user's question (e.g., \"What's the latest news on space development?\").\n",
    "context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])  # Concatenate the retrieved documents.\n",
    "prompt_template = f\"Relevant context: {context}\\n\\n The user's question: {question}\"\n",
    "\n",
    "\n",
    "Contexte : Une concat√©nation de documents r√©cup√©r√©s qui fournissent des informations g√©n√©rales.\n",
    "Question : La requ√™te de l'utilisateur.\n",
    "Invite : combine les deux pour guider la r√©ponse du mod√®le de langage.\n",
    "\n",
    "\n",
    "5. G√©n√©rer une r√©ponse √† l'aide du pipeline\n",
    "\n",
    "Envoyez l'invite au pipeline de g√©n√©ration de texte et g√©n√©rez une r√©ponse :\n",
    "\n",
    "\n",
    "\n",
    "lm_response =  # Use the pipeline to generate text based on the prompt.\n",
    "print(lm_response[0][\"generated_text\"])\n",
    "\n",
    "\n",
    "La sortie est une cha√Æne de texte g√©n√©r√©e qui tente de r√©pondre √† la question de l'utilisateur en utilisant le contexte fourni.\n",
    "\n",
    "\n",
    "6. Exp√©rimentez avec diff√©rentes invites et fen√™tres contextuelles\n",
    "\n",
    "Essayez de varier la question et la taille du contexte (par exemple, en utilisant plus ou moins de documents r√©cup√©r√©s) pour observer comment les r√©ponses du mod√®le changent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "565271eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 103.23it/s]\n",
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Maserati unveils Trofeo super sedans 2021 Maserati Ghibli Trofeo, Quattroporte Trofeo revealed COVID-19 stoush keeps detainee in limbo\n",
      "\n",
      "Question: Que disent les actualit√©s sur le climat ?\n",
      "Answer: As the name implies, the Trofeo is an all-electric, non-stop electric super sedans. The powertrain is built on the same chassis as the Ghibli, and the engine is powered by an 8.2-liter V8, with an output of 731 hp. The engine is on a 728-hp, two-speed automatic transmission.\n",
      "\n",
      "The Trofeo comes with a six-speed manual transmission. It is equipped with four-speed automatic transmission (with an automatic transmission option on the headrest for the dual mode).\n",
      "\n",
      "The Trofeo gets the following features:\n",
      "\n",
      "‚Ä¢ A 6.0-liter V8 with a torque of 6,500 rpm, and a top\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Charger donn√©es\n",
    "#path = r\"C:\\chemin\\vers\\labelled_newscatcher_dataset.csv\"\n",
    "path = r\"C:\\Users\\chume\\Desktop\\W06_NLP_LLM\\Day4_Chatbots\\cache\\labelled_newscatcher_dataset.csv\"\n",
    "pdf = pd.read_csv(path, sep=\";\")  # adapte sep si besoin\n",
    "pdf_small = pdf.head(1000).copy()\n",
    "pdf_small[\"id\"] = [f\"doc_{i}\" for i in range(len(pdf_small))]\n",
    "\n",
    "# 2. Embedding avec SentenceTransformer\n",
    "model_embed = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "titles = pdf_small[\"title\"].tolist()\n",
    "embeddings = model_embed.encode(titles, convert_to_numpy=True, show_progress_bar=True).astype('float32')\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# 3. Indexation FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index = faiss.IndexIDMap(index)\n",
    "ids = np.arange(len(pdf_small)).astype(np.int64)\n",
    "index.add_with_ids(embeddings, ids)\n",
    "\n",
    "# 4. Initialiser mod√®le de g√©n√©ration Hugging Face\n",
    "model_id = \"gpt2\"  # tu peux changer par un mod√®le FR ex: \"asi/gpt-fr-cased-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=lm_model, tokenizer=tokenizer, max_new_tokens=150, device_map=\"auto\")\n",
    "\n",
    "# 5. Fonction recherche FAISS\n",
    "def search(query, k=3):\n",
    "    query_embed = model_embed.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    faiss.normalize_L2(query_embed)\n",
    "    D, I = index.search(query_embed, k)\n",
    "    results = pdf_small.iloc[I[0]]\n",
    "    return results\n",
    "\n",
    "# 6. Pipeline complet question / r√©ponse\n",
    "def answer_question(question, k=3):\n",
    "    docs = search(question, k)\n",
    "    context = \" \".join(docs[\"title\"].tolist())\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    response = pipe(prompt)[0][\"generated_text\"]\n",
    "    return response\n",
    "\n",
    "# 7. Exemple d'utilisation\n",
    "question = \"Que disent les actualit√©s sur le climat ?\"\n",
    "print(answer_question(question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0fd5c2",
   "metadata": {},
   "source": [
    "## Appliations potentielles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddecee8",
   "metadata": {},
   "source": [
    "### 1. **Agent Conversationnel Multi-Document avec RAG Contextuel**\n",
    "\n",
    "* **Objectif** : Construire un chatbot capable de r√©pondre √† des questions √† partir de plusieurs fichiers PDF, Word, ou sites web personnels.\n",
    "* **Techno** : LangChain ou LlamaIndex, FAISS ou ChromaDB, mod√®le HF (Mistral, Llama).\n",
    "* **Livrable** : Interface simple (streamlit ou gradio) o√π l‚Äôutilisateur charge ses documents et interagit avec un agent personnalis√©.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **G√©n√©rateur de Contenu Vid√©o Automatique (Script + Visuels)**\n",
    "\n",
    "* **Objectif** : Cr√©er un outil qui g√©n√®re un script vid√©o + images libres de droit (via DALL¬∑E ou Stable Diffusion) pour des vid√©os YouTube th√©matiques (ex : histoire, science).\n",
    "* **Techno** : GPT pour scripts, API image (SD), ffmpeg pour montage.\n",
    "* **Livrable** : G√©n√©ration automatique d‚Äôun mini reportage vid√©o de 1-2 min en local.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Assistant Code IA Multi-langages avec Explications P√©dagogiques**\n",
    "\n",
    "* **Objectif** : Un assistant qui g√©n√®re du code sur plusieurs langages (Python, JavaScript, SQL‚Ä¶) avec explications d√©taill√©es, exemples et variantes.\n",
    "* **Techno** : GPT-4 Turbo ou Open Source, gradio interface, extraction des explications pas √† pas.\n",
    "* **Livrable** : Interface locale type ‚ÄúIA prof particulier‚Äù pour dev junior.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **G√©n√©rateur d‚ÄôEmails Personnalis√©s avec Tonalit√© Modifiable**\n",
    "\n",
    "* **Objectif** : G√©n√©rer des emails professionnels ou commerciaux avec choix de tonalit√© (formel, amical, percutant, humoristique).\n",
    "* **Techno** : LLM avec prompt engineering avanc√©, sliders de tonalit√© (streamlit UI).\n",
    "* **Livrable** : Application fonctionnelle d‚Äôemailing rapide, t√©l√©chargeable en .txt ou .pdf.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **IA de R√©√©criture et Simplification de Documents Techniques**\n",
    "\n",
    "* **Objectif** : Transformer des documents techniques complexes (par ex : notices, docs de dev) en versions simplifi√©es ou vulgaris√©es.\n",
    "* **Techno** : LLM + mod√®les de classification de complexit√©, pipeline de transformation texte.\n",
    "* **Livrable** : Outil qui prend un doc compliqu√© et sort un r√©sum√© clair pour non-sp√©cialiste.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
