{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e84e4b",
   "metadata": {},
   "source": [
    "# Exercices XP\n",
    "Dernière mise à jour : 14 juillet 2025\n",
    "\n",
    "## 👩‍🏫 👩🏿‍🏫 Ce que vous apprendrez\n",
    "Stratégies de recherche de vecteurs (KNN, ANN) et évaluation.\n",
    "Utilitaire de base de données vectorielles (recherche de similarité, RAG).\n",
    "Différences entre les bases de données vectorielles, les bibliothèques et les plugins.\n",
    "Meilleures pratiques pour l’utilisation et les performances du magasin vectoriel.\n",
    "Comment les modèles linguistiques apprennent les connaissances via le contexte.\n",
    "Génération d'intégration de texte et stockage vectoriel.\n",
    "Interrogation des magasins de vecteurs pour les documents pertinents.\n",
    "Application de modèles linguistiques pour répondre à des questions avec un contexte récupéré.\n",
    "\n",
    "\n",
    "## 🛠️ Ce que vous allez créer\n",
    "Un pipeline fonctionnel de génération augmentée de récupération (RAG), démontrant la vectorisation de texte, le stockage de vecteurs dans FAISS et ChromaDB, la recherche de similarité et la réponse aux questions à l'aide d'un modèle de langage Hugging Face.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3223e4",
   "metadata": {},
   "source": [
    "## 🌟 Exercice 1 : Chargement et préparation des données\n",
    "Dans cet exercice, nous allons configurer l'environnement et préparer le jeu de données que nous utiliserons tout au long du projet. Une préparation adéquate des données garantit le bon déroulement des processus en aval, comme la génération d'intégrations, l'utilisation de bases de données vectorielles ou la création de modèles de machine learning. Examinons chaque étape ensemble.\n",
    "\n",
    "\n",
    "\n",
    "Pourquoi cette étape est importante :\n",
    "Avant de se lancer dans des techniques avancées, il est essentiel de :\n",
    "\n",
    "Assurez-vous que toutes les bibliothèques requises sont installées.\n",
    "Chargez et inspectez les données pour comprendre leur structure.\n",
    "Préparez un sous-ensemble gérable pour des itérations plus rapides pendant le développement.\n",
    "Ces étapes nous aident à éviter les problèmes techniques et à garantir que nos analyses ou modèles reposent sur des bases solides.\n",
    "\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Installer les bibliothèques requises\n",
    "\n",
    "Le projet nécessite des bibliothèques spécialisées pour la recherche de vecteurs et la gestion de bases de données :\n",
    "\n",
    "Entrez dans votre dossier, puis, dans votre terminal :\n",
    "\n",
    "\n",
    "\n",
    "pip install -q faiss-cpu==1.7.4 \n",
    "pip install -q chromadb==0.3.21\n",
    "pip install -qU chromadb\n",
    "pip install -q numpy<2\n",
    "\n",
    "\n",
    "Créez un répertoire de cache pour garder notre espace de travail organisé et gérer toutes les données intermédiaires ou les fichiers téléchargés :\n",
    "\n",
    "Dans votre terminal :\n",
    "\n",
    "mkdir cache\n",
    "alors\n",
    "\n",
    "apt install libomp-dev\n",
    "python -m pip install --upgrade faiss-cpu\n",
    "\n",
    "\n",
    "puis, dans votre fichier, importez les bibliothèques essentielles\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "faiss-cpu:Une bibliothèque pour une recherche de similarité efficace et un regroupement de vecteurs denses (développée par Facebook AI Research).\n",
    "chromadb:Une bibliothèque de bases de données vectorielles qui nous permet de stocker et d'interroger efficacement les intégrations.\n",
    "Ces bibliothèques sont essentielles pour les étapes ultérieures lorsque nous gérons les intégrations et effectuons des recherches de similarité.\n",
    "\n",
    "2. Charger l'ensemble de données\n",
    "\n",
    "Nous travaillerons avec un jeu de données intitulé labelled_newscatcher_dataset.csv , qui contient des articles d'actualité étiquetés. Ces articles seront ensuite transformés en intégrations pour le stockage vectoriel et la recherche.\n",
    "\n",
    "Tâche : Charger l’ensemble de données dans un DataFrame pandas :\n",
    "\n",
    "\n",
    "path =  # Provide the correct file path.\n",
    "pdf =   # Load the CSV file into a pandas DataFrame.\n",
    "\n",
    "\n",
    "Cette étape garantit que nos données sont dans un format adapté à l’analyse.\n",
    "\n",
    "3. Ajoutez une colonne d'identifiant (si nécessaire)\n",
    "\n",
    "Les identifiants uniques nous aident à suivre chaque enregistrement, en particulier lorsque nous travaillons avec des bases de données vectorielles :\n",
    "\n",
    "\n",
    "\n",
    "pdf[\"id\"] =\n",
    "\n",
    "\n",
    "Chaque article d’actualité aura un identifiant unique, ce qui facilitera sa référence lors du stockage et de la récupération.\n",
    "\n",
    "4. Inspectez les données\n",
    "\n",
    "Utilisez la commande suivante pour obtenir un aperçu rapide de l’ensemble de données :\n",
    "\n",
    "\n",
    "\n",
    "display(pdf)\n",
    "\n",
    "\n",
    "Prenez un moment pour observer :\n",
    "\n",
    "Les colonnes disponibles.\n",
    "Le type de données qu'ils contiennent (par exemple, texte, étiquettes).\n",
    "S'il y a des valeurs manquantes.\n",
    "La compréhension de l’ensemble de données à ce stade est essentielle pour une prise de décision éclairée lors des étapes ultérieures.\n",
    "\n",
    "5. Créez un sous-ensemble pour un traitement plus rapide\n",
    "\n",
    "Travailler avec de grands ensembles de données peut prendre du temps. Pour accélérer les itérations pendant le développement :\n",
    "\n",
    "Tâche : sélectionnez un sous-ensemble plus petit du DataFrame (par exemple, les 1 000 premières lignes).\n",
    "Cette approche vous permet de tester efficacement votre code avant de le mettre à l’échelle sur l’ensemble de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8eb31a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb==0.3.21\n",
      "  Using cached chromadb-0.3.21-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: pandas>=1.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (2.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (2.11.7)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (1.26.4)\n",
      "Collecting clickhouse-connect>=0.5.7\n",
      "  Using cached clickhouse_connect-0.8.18-cp310-cp310-win_amd64.whl (247 kB)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (2.32.4)\n",
      "Collecting hnswlib>=0.7\n",
      "  Using cached hnswlib-0.8.0.tar.gz (36 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: sentence-transformers>=2.2.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (5.0.0)\n",
      "Collecting duckdb>=0.7.1\n",
      "  Using cached duckdb-1.3.2-cp310-cp310-win_amd64.whl (11.4 MB)\n",
      "Collecting fastapi>=0.85.1\n",
      "  Using cached fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Collecting posthog>=2.4.0\n",
      "  Using cached posthog-6.1.0-py3-none-any.whl (109 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: zstandard in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (0.23.0)\n",
      "Collecting lz4\n",
      "  Using cached lz4-4.4.4-cp310-cp310-win_amd64.whl (99 kB)\n",
      "Requirement already satisfied: pytz in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2025.2)\n",
      "Requirement already satisfied: urllib3>=1.26 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2025.6.15)\n",
      "Collecting starlette<0.48.0,>=0.40.0\n",
      "  Using cached starlette-0.47.1-py3-none-any.whl (72 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.85.1->chromadb==0.3.21) (4.14.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.3->chromadb==0.3.21) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.3->chromadb==0.3.21) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.3.21) (1.17.0)\n",
      "Collecting distro>=1.5.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting backoff>=1.10.0\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb==0.3.21) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb==0.3.21) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb==0.3.21) (0.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.3.21) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.3.21) (3.4.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (2.7.1+cu118)\n",
      "Requirement already satisfied: Pillow in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (11.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.33.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.7.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.53.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.16.0)\n",
      "Collecting httptools>=0.6.3\n",
      "  Using cached httptools-0.6.4-cp310-cp310-win_amd64.whl (88 kB)\n",
      "Collecting watchfiles>=0.13\n",
      "  Using cached watchfiles-1.1.0-cp310-cp310-win_amd64.whl (292 kB)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.4.6)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (15.0.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (1.1.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2025.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (4.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.14.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.21.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.6.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (1.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.0.2)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (pyproject.toml): started\n",
      "  Building wheel for hnswlib (pyproject.toml): finished with status 'error'\n",
      "Failed to build hnswlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for hnswlib (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [5 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'hnswlib' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hnswlib\n",
      "ERROR: Could not build wheels for hnswlib, which is required to install pyproject.toml-based projects\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Using cached chromadb-1.0.15-cp39-abi3-win_amd64.whl (19.5 MB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (14.0.0)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Using cached opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
      "Collecting mmh3>=4.0.1\n",
      "  Using cached mmh3-5.1.0-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Using cached kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (2.11.7)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
      "Collecting posthog<6.0.0,>=2.4.0\n",
      "  Using cached posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Collecting build>=1.0.3\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.16.0)\n",
      "Collecting jsonschema>=4.19.0\n",
      "  Using cached jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "Collecting overrides>=7.3.1\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Using cached bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.21.2)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.73.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (4.14.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Collecting onnxruntime>=1.14.1\n",
      "  Using cached onnxruntime-1.22.1-cp310-cp310-win_amd64.whl (12.7 MB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
      "Requirement already satisfied: anyio in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
      "Collecting referencing>=0.28.4\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: requests in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Collecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting googleapis-common-protos~=1.57\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Collecting opentelemetry-proto==1.35.0\n",
      "  Using cached opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.35.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-6.31.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.56b0\n",
      "  Using cached opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
      "Collecting backoff>=1.10.0\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting distro>=1.5.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.33.4)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Collecting watchfiles>=0.13\n",
      "  Using cached watchfiles-1.1.0-cp310-cp310-win_amd64.whl (292 kB)\n",
      "Collecting httptools>=0.6.3\n",
      "  Using cached httptools-0.6.4-cp310-cp310-win_amd64.whl (88 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Installing collected packages: referencing, protobuf, overrides, mmh3, importlib-resources, importlib-metadata, humanfriendly, httptools, distro, build, bcrypt, backoff, uvicorn, posthog, opentelemetry-proto, opentelemetry-api, jsonschema-specifications, googleapis-common-protos, coloredlogs, watchfiles, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, jsonschema, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Accès refusé: 'C:\\\\Users\\\\chume\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\google\\\\~~otobuf\\\\internal\\\\_api_implementation.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb==0.3.21\n",
    "!pip install -U chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3c766a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20.3\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.31.1\n",
      "    Uninstalling protobuf-6.31.1:\n",
      "      Successfully uninstalled protobuf-6.31.1\n",
      "Successfully installed protobuf-3.20.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires keras<2.14,>=2.13.1, but you have keras 2.10.0 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires tensorboard<2.14,>=2.13, but you have tensorboard 2.10.1 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires tensorflow-estimator<2.14,>=2.13.0, but you have tensorflow-estimator 2.10.0 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.14.1 which is incompatible.\n",
      "tensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade protobuf==3.20.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b9a9633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier trouvé, chargement...\n",
      "\n",
      "Aperçu des données :\n",
      "     topic                                               link          domain  \\\n",
      "0  SCIENCE  https://www.eurekalert.org/pub_releases/2020-0...  eurekalert.org   \n",
      "1  SCIENCE  https://www.pulse.ng/news/world/an-irresistibl...        pulse.ng   \n",
      "2  SCIENCE  https://www.express.co.uk/news/science/1322607...   express.co.uk   \n",
      "3  SCIENCE  https://www.ndtv.com/world-news/glaciers-could...        ndtv.com   \n",
      "4  SCIENCE  https://www.thesun.ie/tech/5742187/perseid-met...       thesun.ie   \n",
      "\n",
      "        published_date                                              title  \\\n",
      "0  2020-08-06 13:59:45  A closer look at water-splitting's solar fuel ...   \n",
      "1  2020-08-12 15:14:19  An irresistible scent makes locusts swarm, stu...   \n",
      "2  2020-08-13 21:01:00  Artificial intelligence warning: AI will know ...   \n",
      "3  2020-08-03 22:18:26   Glaciers Could Have Sculpted Mars Valleys: Study   \n",
      "4  2020-08-12 19:54:36  Perseid meteor shower 2020: What time and how ...   \n",
      "\n",
      "  lang     id  \n",
      "0   en  doc_0  \n",
      "1   en  doc_1  \n",
      "2   en  doc_2  \n",
      "3   en  doc_3  \n",
      "4   en  doc_4  \n",
      "\n",
      "Infos générales :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108774 entries, 0 to 108773\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   topic           108774 non-null  object\n",
      " 1   link            108774 non-null  object\n",
      " 2   domain          108774 non-null  object\n",
      " 3   published_date  108774 non-null  object\n",
      " 4   title           108774 non-null  object\n",
      " 5   lang            108774 non-null  object\n",
      " 6   id              108774 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 5.8+ MB\n",
      "None\n",
      "\n",
      "Sous-ensemble prêt : (1000, 7)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Chemin correct vers le fichier CSV\n",
    "path = r\"C:\\Users\\chume\\Desktop\\W06_NLP_LLM\\Day4_Chatbots\\cache\\labelled_newscatcher_dataset.csv\"\n",
    "\n",
    "# Vérification présence fichier\n",
    "if os.path.isfile(path):\n",
    "    print(\"Fichier trouvé, chargement...\")\n",
    "    pdf = pd.read_csv(path, sep=';')\n",
    "\n",
    "    \n",
    "    # Ajout ID si absent\n",
    "    if \"id\" not in pdf.columns:\n",
    "        pdf[\"id\"] = [f\"doc_{i}\" for i in range(len(pdf))]\n",
    "    \n",
    "    # Aperçu rapide\n",
    "    print(\"\\nAperçu des données :\")\n",
    "    print(pdf.head())\n",
    "    print(\"\\nInfos générales :\")\n",
    "    print(pdf.info())\n",
    "    \n",
    "    # Sous-ensemble rapide\n",
    "    pdf_small = pdf.head(1000)\n",
    "    print(f\"\\nSous-ensemble prêt : {pdf_small.shape}\")\n",
    "else:\n",
    "    print(f\"❌ Fichier non trouvé : {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276fa7cb",
   "metadata": {},
   "source": [
    "## 🌟 Exercice 2 : Vectorisation avec des transformateurs de phrases\n",
    "Dans cet exercice, nous allons transformer nos données textuelles (titres d'actualités) en représentations numériques appelées intégrations . Cette étape est cruciale pour permettre aux machines de comprendre et d'exploiter les données textuelles dans des tâches telles que la recherche de similarité, le clustering et l'apprentissage automatique. Nous utiliserons Sentence Transformers , une bibliothèque populaire pour générer des représentations vectorielles denses de texte.\n",
    "\n",
    "\n",
    "\n",
    "Pourquoi cette étape est importante :\n",
    "Les machines ne peuvent pas traiter directement le texte brut ; elles ont besoin d'une entrée numérique. Les intégrations sont des vecteurs denses qui capturent le sens et le contexte du texte. En générant des intégrations pour nos titres d'actualités, nous les rendons utilisables pour des tâches en aval, telles que les recherches de similarité ou l'alimentation de modèles d'apprentissage automatique.\n",
    "\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Installer et importer la bibliothèque de transformateurs de phrases\n",
    "\n",
    "La sentence_transformersbibliothèque fournit des méthodes faciles à utiliser pour générer des intégrations au niveau des phrases.\n",
    "\n",
    "\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "\n",
    "InputExample:Une classe utilitaire qui aide à formater les entrées de données pour la formation ou l'inférence avec des transformateurs de phrases.\n",
    "2. Préparez les données pour la génération d'intégration\n",
    "\n",
    "Nous allons appliquer une fonction d'assistance au sous-ensemble de notre DataFrame créé précédemment. Cette fonction formate chaque ligne en un InputExampleobjet, nécessaire au processus d'intégration.\n",
    "\n",
    "Tâche : extraire le sous-ensemble du DataFrame (par exemple, pdf_subset) pour lequel vous souhaitez générer des intégrations.\n",
    "\n",
    "\n",
    "pdf_subset = ...  # Use the subset created in the previous exercise.\n",
    "\n",
    "\n",
    "3. Créer une fonction d'assistance\n",
    "\n",
    "Cette fonction convertit chaque enregistrement (titre d'actualité) au format approprié ( InputExample) requis par le modèle Sentence Transformer.\n",
    "\n",
    "\n",
    "\n",
    "def example_create_fn(doc1: pd.Series) -> InputExample:\n",
    "    \"\"\"\n",
    "    Helper function that outputs a sentence_transformer guid, label, and text.\n",
    "    \"\"\"\n",
    "    return ...  # Format and return the InputExample.\n",
    "\n",
    "\n",
    "La fonction prendra une ligne (dans ce cas, le titre d'un article d'actualité) et la formatera correctement pour le processus de génération d'intégration.\n",
    "4. Appliquer la fonction d'assistance au sous-ensemble\n",
    "\n",
    "Nous appliquerons cette fonction sur le sous-ensemble DataFrame pour générer une liste d' InputExampleobjets :\n",
    "\n",
    "\n",
    "\n",
    "faiss_train_examples = pdf_subset.apply(lambda x: example_create_fn(x[\"title\"]), axis=1).tolist()\n",
    "faiss_train_examples[:10]\n",
    "\n",
    "\n",
    "Cela prépare les données pour la génération d'intégration en convertissant chaque titre d'actualité en un format structuré.\n",
    "\n",
    "5. Initialiser le modèle d'intégration\n",
    "\n",
    "Nous utiliserons le modèle pré-entraîné all-MiniLM-L6-v2, qui fournit des intégrations de haute qualité pour une large gamme de tâches de traitement du langage naturel (NLP).\n",
    "\n",
    "Tâche : Initialiser le modèle.\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    ...  # Specify the model name.\n",
    ")\n",
    "\n",
    "\n",
    "Cette étape charge le modèle en mémoire, prêt pour la génération d’intégration.\n",
    "6. Extraire les titres et les convertir en une liste de chaînes\n",
    "\n",
    "Extrayez la colonne « titre » de votre sous-ensemble DataFrame et convertissez-la en liste. Il s'agit des données textuelles brutes que nous allons intégrer.\n",
    "\n",
    "Tâche : Convertir les titres en une liste de chaînes.\n",
    "\n",
    "\n",
    "# Example (fill in appropriately):\n",
    "titles_list = pdf_subset[\"title\"].tolist()\n",
    "\n",
    "\n",
    "7. Générer des intégrations pour les titres\n",
    "\n",
    "À l’aide du modèle initialisé, générez des intégrations pour chaque titre :\n",
    "\n",
    "\n",
    "\n",
    "faiss_title_embedding =  # Generate embeddings for the list of titles.\n",
    "\n",
    "\n",
    "Cette étape transforme chaque titre en un vecteur dense qui capture sa signification sémantique.\n",
    "\n",
    "\n",
    "8. Vérifiez les dimensions d'intégration\n",
    "\n",
    "Pour vérifier que les intégrations ont été générées correctement, vérifiez la forme de la sortie :\n",
    "\n",
    "\n",
    "\n",
    "len(faiss_title_embedding), len(faiss_title_embedding[0])\n",
    "\n",
    "\n",
    "Cela confirme le nombre d'intégrations dont vous disposez (une par titre) et la dimensionnalité de chaque vecteur d'intégration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c77daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 32/32 [00:00<00:00, 122.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'intégrations générées : 1000\n",
      "Dimensions de chaque vecteur : 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercice 2 : Vectorisation des titres avec Sentence Transformers\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import InputExample, SentenceTransformer\n",
    "\n",
    "# 1.  Utilisation du sous-ensemble du précédent exercice\n",
    "pdf_subset = pdf_small  # le sous-ensemble de 1000 lignes\n",
    "\n",
    "# 2. Fonction d'aide : transforme chaque ligne en InputExample\n",
    "def example_create_fn(doc1: str) -> InputExample:\n",
    "    \"\"\"\n",
    "    Prend un titre (texte), retourne un InputExample formaté.\n",
    "    \"\"\"\n",
    "    return InputExample(guid=None, texts=[doc1], label=0.0)\n",
    "\n",
    "# 3. Conversion du DataFrame en liste d'InputExample\n",
    "faiss_train_examples = pdf_subset[\"title\"].apply(example_create_fn).tolist()\n",
    "\n",
    "# 4. Initialisation du modèle Sentence Transformer (prêt à l'emploi)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 5. Extraction des titres en liste de chaînes simples\n",
    "titles_list = pdf_subset[\"title\"].tolist()\n",
    "\n",
    "# 6. Génération des embeddings\n",
    "faiss_title_embedding = model.encode(titles_list, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# 7. Vérification de la taille des embeddings\n",
    "print(f\"Nombre d'intégrations générées : {len(faiss_title_embedding)}\")\n",
    "print(f\"Dimensions de chaque vecteur : {len(faiss_title_embedding[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a08eb",
   "metadata": {},
   "source": [
    "## 🌟 Exercice 3 : Indexation et recherche FAISS\n",
    "Dans cet exercice, nous utiliserons FAISS (Facebook AI Similarity Search) pour créer un index des représentations vectorielles continues générées lors de l'exercice précédent. Cela nous permettra d'effectuer des recherches de similarité rapides et efficaces sur de vastes collections de vecteurs. L'objectif est de récupérer les articles d'actualité les plus pertinents en fonction de la requête d'un utilisateur.\n",
    "\n",
    "\n",
    "\n",
    "Pourquoi cette étape est importante :\n",
    "FAISS est une bibliothèque conçue pour effectuer des recherches de similarité à grande échelle. L'utilisation d'embeddings (vecteurs de grande dimension) complique considérablement la recherche efficace. FAISS propose des algorithmes optimisés d'indexation et de recherche, permettant de retrouver des éléments similaires en quelques millisecondes, même à partir de grands ensembles de données.\n",
    "\n",
    "\n",
    "\n",
    "Instructions:\n",
    "Installer et importer la bibliothèque FAISS\n",
    "Si vous ne l'avez pas déjà fait, assurez-vous que FAISS est installé et importez les modules nécessaires :\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "\n",
    "numpy:Pour gérer les tableaux et les opérations matricielles.\n",
    "faiss: Pour construire et interroger l'index vectoriel.\n",
    "\n",
    "\n",
    "2. Préparez les données pour l'indexation\n",
    "\n",
    "Utilisez les vecteurs d’intégration générés à partir de l’exercice précédent et préparez-les pour l’indexation :\n",
    "\n",
    "\n",
    "\n",
    "pdf_to_index =  # This should be your subset DataFrame containing the articles.\n",
    "id_index =  # An array of IDs corresponding to each article.\n",
    "\n",
    "\n",
    "pdf_to_index: Le sous-ensemble du DataFrame que nous voulons indexer.\n",
    "id_index:Un tableau d’identifiants uniques pour chaque vecteur d’intégration.\n",
    "\n",
    "\n",
    "3. Normaliser les vecteurs d'intégration\n",
    "\n",
    "Pour effectuer une recherche de similarité cosinus (qui mesure l'angle entre les vecteurs plutôt que leur distance), nous devons d'abord normaliser les vecteurs d'intégration :\n",
    "\n",
    "\n",
    "\n",
    "content_encoded_normalized =  # Embedding vectors.\n",
    "faiss.normalize_L2(content_encoded_normalized)\n",
    "\n",
    "\n",
    "La normalisation garantit que les vecteurs ont une longueur unitaire, ce qui est nécessaire pour que la similarité cosinus fonctionne correctement.\n",
    "\n",
    "\n",
    "4. Créer l'index FAISS\n",
    "\n",
    "FAISS propose différents types d'index selon la mesure de similarité et les exigences de recherche. Nous utiliserons un IndexFlatIP (produit interne) encapsulé dans un IndexIDMap :\n",
    "\n",
    "\n",
    "\n",
    "index_content = faiss.IndexIDMap(faiss.IndexFlatIP(len(faiss_title_embedding[0])))\n",
    "index_content.add_with_ids(content_encoded_normalized, id_index)\n",
    "\n",
    "\n",
    "IndexFlatIP: Un type d'index qui utilise le produit interne (qui est équivalent à la similarité cosinus pour les vecteurs normalisés).\n",
    "IndexIDMap: Les résultats de recherche de cartes renvoient aux identifiants d'origine, garantissant ainsi que nous pouvons récupérer les articles correspondants.\n",
    "Cette étape crée l’index et ajoute les vecteurs normalisés avec leurs identifiants.\n",
    "\n",
    "\n",
    "\n",
    "5. Implémenter une fonction de recherche\n",
    "\n",
    "Ensuite, nous allons définir une fonction search_contentqui prend une requête utilisateur et récupère les articles les plus similaires de l'index :\n",
    "\n",
    "\n",
    "\n",
    "def search_content(query, pdf_to_index, k=3):\n",
    "    query_vector =  # Encode the query string into an embedding vector.\n",
    "    faiss.normalize_L2(query_vector)  # Normalize the query vector.\n",
    "\n",
    "    # Perform the search\n",
    "    top_k =  # The top-k similar vectors.\n",
    "    ids =  # The IDs of the matching vectors.\n",
    "    similarities =  # Similarity scores for the matches.\n",
    "\n",
    "    results =  # Retrieve the matching articles from pdf_to_index.\n",
    "    results[\"similarities\"] = similarities  # Add similarity scores.\n",
    "    return results\n",
    "\n",
    "\n",
    "Cette fonction encode la requête de l'utilisateur dans un vecteur, recherche l'index FAISS, récupère les k vecteurs les plus similaires et renvoie les articles correspondants ainsi que leurs scores de similarité.\n",
    "\n",
    "\n",
    "6. Testez la fonction de recherche\n",
    "\n",
    "Utilisez la fonction de recherche pour trouver des articles liés à un exemple de requête :\n",
    "\n",
    "\n",
    "\n",
    "display(search_content(\"animal\", pdf_to_index, k=5))\n",
    "\n",
    "\n",
    "Cela vous permet de vérifier que le processus de recherche fonctionne et renvoie des articles pertinents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6991aeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index FAISS construit avec 1000 vecteurs.\n",
      "\n",
      "Résultats les plus similaires :\n",
      "                                                 title  similarity\n",
      "176  Random: You Can Pick Up and Pet Cats in Assass...    0.391902\n",
      "975  Researchers explore social behavior of animals...    0.376784\n",
      "99               Ghostwire: Tokyo confirms dog petting    0.344058\n",
      "928                 Just Let This Lizard Be a Dinosaur    0.317387\n",
      "762  'Secret' life of sharks: Study reveals their s...    0.295497\n"
     ]
    }
   ],
   "source": [
    "# Exercice 3 : Indexation et recherche avec FAISS\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# 1. Préparation des données pour FAISS\n",
    "pdf_to_index = pdf_subset.copy()  # on indexe le même sous-ensemble que précédemment\n",
    "id_index = np.array([i for i in range(len(pdf_to_index))]).astype(np.int64)\n",
    "\n",
    "# 2. Normalisation des embeddings pour la similarité cosinus\n",
    "content_encoded_normalized = faiss_title_embedding.copy().astype('float32')\n",
    "faiss.normalize_L2(content_encoded_normalized)\n",
    "\n",
    "# 3. Création de l'index FAISS (produit interne avec identifiants mappés)\n",
    "dimension = content_encoded_normalized.shape[1]\n",
    "index_content = faiss.IndexIDMap(faiss.IndexFlatIP(dimension))\n",
    "\n",
    "# Ajout des vecteurs à l'index\n",
    "index_content.add_with_ids(content_encoded_normalized, id_index)\n",
    "\n",
    "print(f\"Index FAISS construit avec {index_content.ntotal} vecteurs.\")\n",
    "\n",
    "# 4. Fonction de recherche\n",
    "def search_content(query, pdf_to_index, k=3):\n",
    "    \"\"\"\n",
    "    Recherche les k documents les plus similaires à la requête.\n",
    "    \"\"\"\n",
    "    # Encodage et normalisation de la requête\n",
    "    query_vector = model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    faiss.normalize_L2(query_vector)\n",
    "\n",
    "    # Recherche FAISS\n",
    "    distances, indices = index_content.search(query_vector, k)\n",
    "\n",
    "    # Extraction des résultats\n",
    "    results = pdf_to_index.iloc[indices[0]].copy()\n",
    "    results[\"similarity\"] = distances[0]\n",
    "    return results\n",
    "\n",
    "# 5. Exemple de recherche\n",
    "resultats = search_content(\"animal\", pdf_to_index, k=5)\n",
    "print(\"\\nRésultats les plus similaires :\")\n",
    "print(resultats[[\"title\", \"similarity\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa217c8d",
   "metadata": {},
   "source": [
    "## 🌟 Exercice 4 : Collecte et interrogation de ChromaDB\n",
    "Dans cet exercice, nous présenterons ChromaDB , une base de données vectorielle open source conçue pour stocker, indexer et interroger les vecteurs d'incorporation. ChromaDB simplifie l'utilisation des incorporations et, contrairement à FAISS, gère automatiquement la tokenisation, l'incorporation et l'indexation sans nécessiter de génération manuelle. Cela la rend idéale pour l'intégration avec les applications LLM (Large Language Model), notamment pour la création de systèmes de questions-réponses ou de moteurs de recherche.\n",
    "\n",
    "\n",
    "\n",
    "Pourquoi cette étape est importante :\n",
    "Une fois les intégrations générées pour nos données, l'étape logique suivante consiste à les stocker et à les interroger efficacement. ChromaDB offre une interface de gestion avancée des intégrations et prend en charge les métadonnées, ce qui en fait un outil idéal pour la création d'applications telles que la recherche documentaire ou les systèmes de questions-réponses. Grâce à ChromaDB, nous démontrons comment intégrer les intégrations dans un workflow concret prenant en charge l'interrogation et la récupération de documents pertinents.\n",
    "\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Installer et importer la bibliothèque ChromaDB\n",
    "\n",
    "Assurez-vous que ChromaDB est installé et importez les composants nécessaires :\n",
    "\n",
    "\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "chromadb:La bibliothèque principale pour la gestion des collections de vecteurs et des requêtes.\n",
    "Settings: Options de configuration pour ChromaDB.\n",
    "\n",
    "\n",
    "2. Initialiser un client ChromaDB et créer une collection\n",
    "\n",
    "ChromaDB organise les vecteurs en collections , similaires aux tables d'une base de données. Chaque collection contient un ensemble de documents (vecteurs) et les métadonnées associées.\n",
    "\n",
    "\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection_name = \"my_news\"\n",
    "\n",
    "# If a collection with the same name exists, delete it to avoid conflicts\n",
    "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "print(f\"Creating collection: '{collection_name}'\")\n",
    "collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "\n",
    "Ce code initialise le client ChromaDB, vérifie si une collection nommée « my_news » existe déjà, la supprime si c'est le cas et crée une nouvelle collection.\n",
    "Les collections stockent à la fois des documents (le texte ou les incorporations) et des métadonnées (par exemple, les étiquettes de sujet).\n",
    "\n",
    "\n",
    "3. Ajouter des données à la collection\n",
    "\n",
    "ChromaDB simplifie l'ingestion des données en générant automatiquement des intégrations si vous ne fournissez pas de modèle d'intégration personnalisé. Il utilise le modèle par défaut SentenceTransformerEmbeddingFunction, qui gère la tokenisation, l'intégration et l'indexation.\n",
    "\n",
    "Tâche : Ajouter les 100 premiers titres d'actualité du sous-ensemble DataFrame à la collection. Inclure le sujet correspondant à chaque titre comme métadonnées et attribuer un identifiant unique à chaque document.\n",
    "\n",
    "\n",
    "# Display the DataFrame subset (for reference)\n",
    "display(pdf_subset)\n",
    "\n",
    "collection.add(\n",
    "    documents=pdf_subset[\"title\"][:100].tolist(),\n",
    "    metadatas=[{\"topic\": topic} for topic in pdf_subset[\"topic\"][:100].tolist()],\n",
    "    ids=...  # Provide a list of unique IDs.\n",
    ")\n",
    "\n",
    "\n",
    "Le documentsparamètre contient la liste des titres d'actualités.\n",
    "Le metadatasparamètre contient les rubriques associées en tant que métadonnées.\n",
    "Le idsparamètre doit être une liste d' identifiants uniques (par exemple, des chaînes ou des entiers) pour chaque document.\n",
    "Remarque : l’ajout de données à la collection peut prendre du temps en fonction du volume de données, car ChromaDB traite et indexe le texte en arrière-plan.\n",
    "\n",
    "\n",
    "\n",
    "4. Interroger la collection\n",
    "\n",
    "Enfin, effectuez une requête de recherche pour récupérer les documents les plus pertinents en fonction d’un terme de recherche.\n",
    "\n",
    "Tâche : Interroger la collection à l'aide d'un terme (par exemple, « espace » ) et récupérer les 10 documents les plus pertinents .\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "results = ...  # Perform the search query.\n",
    "\n",
    "print(json.dumps(results, indent=4))\n",
    "\n",
    "\n",
    "Le terme de recherche (par exemple, « espace ») est automatiquement converti en une incorporation par ChromaDB, et la collection renvoie les 10 voisins les plus proches , c'est-à-dire les documents les plus sémantiquement similaires à la requête.\n",
    "Les résultats incluent les documents correspondants, leurs métadonnées et leurs scores de similarité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f83f0fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb==0.3.21\n",
      "  Using cached chromadb-0.3.21-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (2.32.4)\n",
      "Collecting hnswlib>=0.7\n",
      "  Using cached hnswlib-0.8.0.tar.gz (36 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fastapi>=0.85.1\n",
      "  Using cached fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Collecting posthog>=2.4.0\n",
      "  Using cached posthog-6.1.0-py3-none-any.whl (109 kB)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (1.26.4)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (2.11.7)\n",
      "Requirement already satisfied: pandas>=1.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (2.3.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.21) (5.0.0)\n",
      "Collecting duckdb>=0.7.1\n",
      "  Using cached duckdb-1.3.2-cp310-cp310-win_amd64.whl (11.4 MB)\n",
      "Collecting clickhouse-connect>=0.5.7\n",
      "  Using cached clickhouse_connect-0.8.18-cp310-cp310-win_amd64.whl (247 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2025.6.15)\n",
      "Requirement already satisfied: urllib3>=1.26 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2.5.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2025.2)\n",
      "Collecting lz4\n",
      "  Using cached lz4-4.4.4-cp310-cp310-win_amd64.whl (99 kB)\n",
      "Requirement already satisfied: zstandard in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (0.23.0)\n",
      "Collecting starlette<0.48.0,>=0.40.0\n",
      "  Using cached starlette-0.47.1-py3-none-any.whl (72 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastapi>=0.85.1->chromadb==0.3.21) (4.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.3->chromadb==0.3.21) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.3->chromadb==0.3.21) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.3.21) (1.17.0)\n",
      "Collecting distro>=1.5.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting backoff>=1.10.0\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb==0.3.21) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb==0.3.21) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb==0.3.21) (0.7.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.3.21) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.3.21) (3.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.33.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.53.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.7.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (2.7.1+cu118)\n",
      "Requirement already satisfied: scipy in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (11.3.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.16.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (1.1.1)\n",
      "Requirement already satisfied: colorama>=0.4 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.4.6)\n",
      "Collecting httptools>=0.6.3\n",
      "  Using cached httptools-0.6.4-cp310-cp310-win_amd64.whl (88 kB)\n",
      "Collecting watchfiles>=0.13\n",
      "  Using cached watchfiles-1.1.0-cp310-cp310-win_amd64.whl (292 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (15.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (6.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (24.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (4.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.1.6)\n",
      "Requirement already satisfied: networkx in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.4.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.14.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.21.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2024.11.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.5.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\chume\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (1.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.0.2)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (pyproject.toml): started\n",
      "  Building wheel for hnswlib (pyproject.toml): finished with status 'error'\n",
      "Failed to build hnswlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for hnswlib (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [5 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'hnswlib' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hnswlib\n",
      "ERROR: Could not build wheels for hnswlib, which is required to install pyproject.toml-based projects\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chume\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb==0.3.21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a447ca6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chromadb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 📌 Exercice 4 : Collecte et interrogation avec ChromaDB\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Settings\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'"
     ]
    }
   ],
   "source": [
    "# 📌 Exercice 4 : Collecte et interrogation avec ChromaDB\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import json\n",
    "\n",
    "# 1. ✅ Initialisation du client ChromaDB\n",
    "chroma_client = chromadb.Client(Settings())\n",
    "\n",
    "collection_name = \"my_news\"\n",
    "\n",
    "# ✅ Suppression si collection existante\n",
    "collections_existantes = [col.name for col in chroma_client.list_collections()]\n",
    "if collection_name in collections_existantes:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "print(f\"✅ Création de la collection : '{collection_name}'\")\n",
    "collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "# 2. ✅ Ajout des données (100 premiers titres) avec métadonnées\n",
    "documents = pdf_subset[\"title\"][:100].tolist()\n",
    "metadatas = [{\"topic\": topic} for topic in pdf_subset[\"topic\"][:100].tolist()]\n",
    "ids = [f\"id_{i}\" for i in range(100)]\n",
    "\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")\n",
    "print(f\"✅ Ajout de {len(documents)} documents dans ChromaDB.\")\n",
    "\n",
    "# 3. ✅ Recherche sur la collection\n",
    "query = \"espace\"  # terme de recherche\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=10\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Résultats de la recherche :\")\n",
    "print(json.dumps(results, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54788d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 32/32 [00:00<00:00, 137.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index FAISS créé avec 1000 vecteurs\n",
      "                                                 title     score\n",
      "241  2021 Maserati Ghibli Trofeo, Quattroporte Trof...  0.286445\n",
      "74   Blasphemous gets a new storyline, New Game Plu...  0.275283\n",
      "262  Halo: The Master Chief Collection Crossplay Su...  0.256196\n",
      "927  Tennocon 2020 recap: The future of Warframe re...  0.228718\n",
      "468  Ghost of Tsushima Has the Most Impressive Meta...  0.225673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ✅ 1. Chargement des données\n",
    "path = r\"C:\\Users\\chume\\Desktop\\W06_NLP_LLM\\Day4_Chatbots\\cache\\labelled_newscatcher_dataset.csv\"\n",
    "#pdf = pd.read_csv(path, sep=',')\n",
    "pdf = pd.read_csv(path, sep=';')\n",
    "\n",
    "\n",
    "# Sous-ensemble pour tests rapides\n",
    "pdf_small = pdf.head(1000).copy()\n",
    "pdf_small[\"id\"] = [f\"doc_{i}\" for i in range(len(pdf_small))]\n",
    "\n",
    "# ✅ 2. Embedding avec Sentence Transformers\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "titles = pdf_small[\"title\"].tolist()\n",
    "embeddings = model.encode(titles, convert_to_numpy=True, show_progress_bar=True).astype('float32')\n",
    "\n",
    "# ✅ 3. Normalisation pour similarité cosinus\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# ✅ 4. Indexation FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index = faiss.IndexIDMap(index)\n",
    "ids = np.arange(len(pdf_small)).astype(np.int64)\n",
    "index.add_with_ids(embeddings, ids)\n",
    "\n",
    "print(f\"Index FAISS créé avec {index.ntotal} vecteurs\")\n",
    "\n",
    "# ✅ 5. Fonction recherche\n",
    "def search_faiss(query, k=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = pdf_small.iloc[indices[0]].copy()\n",
    "    results[\"score\"] = distances[0]\n",
    "    return results[[\"title\", \"score\"]]\n",
    "\n",
    "# ✅ 6. Exécution d’une recherche exemple\n",
    "print(search_faiss(\"climat\", k=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f70d7",
   "metadata": {},
   "source": [
    "## Exercice 5 : Répondre aux questions avec un modèle de visage qui fait un câlin\n",
    "Dans cet exercice, nous allons rassembler tous ces éléments en construisant un système de questions-réponses (Q/R) utilisant un modèle de langage Hugging Face . En combinant la récupération de documents (via ChromaDB) et la génération de texte (via Hugging Face), nous créons un pipeline simple mais puissant où un modèle génère des réponses en fonction du contexte pertinent.\n",
    "\n",
    "\n",
    "\n",
    "Pourquoi cette étape est importante :\n",
    "Récupérer des documents pertinents n'est que la moitié du travail. L'étape suivante consiste à générer des réponses pertinentes à partir du contenu récupéré. Il s'agit d'une technique essentielle des systèmes modernes de génération augmentée de recherche (RAG) , où un modèle linguistique exploite à la fois des connaissances pré-entraînées et des informations externes pour répondre aux questions avec plus de précision. En intégrant ChromaDB et les transformateurs Hugging Face , nous simulons un pipeline de questions/réponses réel.\n",
    "\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Installer et importer la bibliothèque Transformers\n",
    "\n",
    "La bibliothèque Hugging Face transformersdonne accès à une variété de modèles de langage pré-entraînés.\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "AutoTokenizer: Charge automatiquement le tokenizer approprié pour le modèle sélectionné.\n",
    "AutoModelForCausalLM: Charge un modèle de langage causal (tel que GPT-2) pour la génération de texte.\n",
    "pipeline:Une interface de haut niveau pour les tâches courantes comme la génération de texte.\n",
    "\n",
    "\n",
    "2. Initialiser le modèle et le tokenizer\n",
    "\n",
    "Sélectionnez un modèle pré-entraîné pour la génération de texte (par exemple, GPT-2 ou un modèle de langage causal similaire) et initialisez à la fois le modèle et son tokeniseur :\n",
    "\n",
    "\n",
    "\n",
    "model_id =  # Specify the Hugging Face model ID (e.g., 'gpt2').\n",
    "tokenizer =  # Load the tokenizer for the model.\n",
    "lm_model =  # Load the causal language model.\n",
    "\n",
    "\n",
    "Le modèle génère du texte en fonction des entrées fournies.\n",
    "Le tokeniseur convertit entre le texte brut et le format tokenisé requis par le modèle.\n",
    "3. Créer un pipeline de génération de texte\n",
    "\n",
    "Configurez un pipeline pour la génération de texte, qui encapsule le modèle et le tokeniseur dans une interface pratique :\n",
    "\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,  # Maximum number of tokens to generate.\n",
    "    device_map=\"auto\",   # Automatically uses available GPU/CPU resources.\n",
    ")\n",
    "\n",
    "\n",
    "Cela simplifie l’exécution de l’inférence et la génération de texte avec le modèle.\n",
    "4. Construisez un modèle d'invite\n",
    "\n",
    "L'invite inclut à la fois le contexte récupéré (depuis ChromaDB) et la question de l'utilisateur . Ainsi, le modèle génère une réponse basée sur les documents pertinents.\n",
    "\n",
    "\n",
    "\n",
    "question =  # Define the user's question (e.g., \"What's the latest news on space development?\").\n",
    "context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])  # Concatenate the retrieved documents.\n",
    "prompt_template = f\"Relevant context: {context}\\n\\n The user's question: {question}\"\n",
    "\n",
    "\n",
    "Contexte : Une concaténation de documents récupérés qui fournissent des informations générales.\n",
    "Question : La requête de l'utilisateur.\n",
    "Invite : combine les deux pour guider la réponse du modèle de langage.\n",
    "\n",
    "\n",
    "5. Générer une réponse à l'aide du pipeline\n",
    "\n",
    "Envoyez l'invite au pipeline de génération de texte et générez une réponse :\n",
    "\n",
    "\n",
    "\n",
    "lm_response =  # Use the pipeline to generate text based on the prompt.\n",
    "print(lm_response[0][\"generated_text\"])\n",
    "\n",
    "\n",
    "La sortie est une chaîne de texte générée qui tente de répondre à la question de l'utilisateur en utilisant le contexte fourni.\n",
    "\n",
    "\n",
    "6. Expérimentez avec différentes invites et fenêtres contextuelles\n",
    "\n",
    "Essayez de varier la question et la taille du contexte (par exemple, en utilisant plus ou moins de documents récupérés) pour observer comment les réponses du modèle changent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "565271eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 32/32 [00:00<00:00, 103.23it/s]\n",
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Maserati unveils Trofeo super sedans 2021 Maserati Ghibli Trofeo, Quattroporte Trofeo revealed COVID-19 stoush keeps detainee in limbo\n",
      "\n",
      "Question: Que disent les actualités sur le climat ?\n",
      "Answer: As the name implies, the Trofeo is an all-electric, non-stop electric super sedans. The powertrain is built on the same chassis as the Ghibli, and the engine is powered by an 8.2-liter V8, with an output of 731 hp. The engine is on a 728-hp, two-speed automatic transmission.\n",
      "\n",
      "The Trofeo comes with a six-speed manual transmission. It is equipped with four-speed automatic transmission (with an automatic transmission option on the headrest for the dual mode).\n",
      "\n",
      "The Trofeo gets the following features:\n",
      "\n",
      "• A 6.0-liter V8 with a torque of 6,500 rpm, and a top\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Charger données\n",
    "#path = r\"C:\\chemin\\vers\\labelled_newscatcher_dataset.csv\"\n",
    "path = r\"C:\\Users\\chume\\Desktop\\W06_NLP_LLM\\Day4_Chatbots\\cache\\labelled_newscatcher_dataset.csv\"\n",
    "pdf = pd.read_csv(path, sep=\";\")  # adapte sep si besoin\n",
    "pdf_small = pdf.head(1000).copy()\n",
    "pdf_small[\"id\"] = [f\"doc_{i}\" for i in range(len(pdf_small))]\n",
    "\n",
    "# 2. Embedding avec SentenceTransformer\n",
    "model_embed = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "titles = pdf_small[\"title\"].tolist()\n",
    "embeddings = model_embed.encode(titles, convert_to_numpy=True, show_progress_bar=True).astype('float32')\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# 3. Indexation FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index = faiss.IndexIDMap(index)\n",
    "ids = np.arange(len(pdf_small)).astype(np.int64)\n",
    "index.add_with_ids(embeddings, ids)\n",
    "\n",
    "# 4. Initialiser modèle de génération Hugging Face\n",
    "model_id = \"gpt2\"  # tu peux changer par un modèle FR ex: \"asi/gpt-fr-cased-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=lm_model, tokenizer=tokenizer, max_new_tokens=150, device_map=\"auto\")\n",
    "\n",
    "# 5. Fonction recherche FAISS\n",
    "def search(query, k=3):\n",
    "    query_embed = model_embed.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    faiss.normalize_L2(query_embed)\n",
    "    D, I = index.search(query_embed, k)\n",
    "    results = pdf_small.iloc[I[0]]\n",
    "    return results\n",
    "\n",
    "# 6. Pipeline complet question / réponse\n",
    "def answer_question(question, k=3):\n",
    "    docs = search(question, k)\n",
    "    context = \" \".join(docs[\"title\"].tolist())\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    response = pipe(prompt)[0][\"generated_text\"]\n",
    "    return response\n",
    "\n",
    "# 7. Exemple d'utilisation\n",
    "question = \"Que disent les actualités sur le climat ?\"\n",
    "print(answer_question(question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0fd5c2",
   "metadata": {},
   "source": [
    "## Appliations potentielles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddecee8",
   "metadata": {},
   "source": [
    "### 1. **Agent Conversationnel Multi-Document avec RAG Contextuel**\n",
    "\n",
    "* **Objectif** : Construire un chatbot capable de répondre à des questions à partir de plusieurs fichiers PDF, Word, ou sites web personnels.\n",
    "* **Techno** : LangChain ou LlamaIndex, FAISS ou ChromaDB, modèle HF (Mistral, Llama).\n",
    "* **Livrable** : Interface simple (streamlit ou gradio) où l’utilisateur charge ses documents et interagit avec un agent personnalisé.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Générateur de Contenu Vidéo Automatique (Script + Visuels)**\n",
    "\n",
    "* **Objectif** : Créer un outil qui génère un script vidéo + images libres de droit (via DALL·E ou Stable Diffusion) pour des vidéos YouTube thématiques (ex : histoire, science).\n",
    "* **Techno** : GPT pour scripts, API image (SD), ffmpeg pour montage.\n",
    "* **Livrable** : Génération automatique d’un mini reportage vidéo de 1-2 min en local.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Assistant Code IA Multi-langages avec Explications Pédagogiques**\n",
    "\n",
    "* **Objectif** : Un assistant qui génère du code sur plusieurs langages (Python, JavaScript, SQL…) avec explications détaillées, exemples et variantes.\n",
    "* **Techno** : GPT-4 Turbo ou Open Source, gradio interface, extraction des explications pas à pas.\n",
    "* **Livrable** : Interface locale type “IA prof particulier” pour dev junior.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Générateur d’Emails Personnalisés avec Tonalité Modifiable**\n",
    "\n",
    "* **Objectif** : Générer des emails professionnels ou commerciaux avec choix de tonalité (formel, amical, percutant, humoristique).\n",
    "* **Techno** : LLM avec prompt engineering avancé, sliders de tonalité (streamlit UI).\n",
    "* **Livrable** : Application fonctionnelle d’emailing rapide, téléchargeable en .txt ou .pdf.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **IA de Réécriture et Simplification de Documents Techniques**\n",
    "\n",
    "* **Objectif** : Transformer des documents techniques complexes (par ex : notices, docs de dev) en versions simplifiées ou vulgarisées.\n",
    "* **Techno** : LLM + modèles de classification de complexité, pipeline de transformation texte.\n",
    "* **Livrable** : Outil qui prend un doc compliqué et sort un résumé clair pour non-spécialiste.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
