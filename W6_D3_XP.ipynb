{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0829adaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5112450505ae471c90e4341f657298d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3786f609cd9c498bbd1a0e7dab8f4ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8434912a26114d978c6267f841cde9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb36c225539412cbe1a6db568b3209a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hello', ',', 'how', 'are', 'you', 'today', '?']\n",
      "Input IDs: tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 2651, 1029,  102]])\n",
      "Tokens: ['[CLS]', 'hello', ',', 'how', 'are', 'you', 'today', '?', '[SEP]']\n",
      "\n",
      "Special tokens added by BERT:\n",
      "[CLS]\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "# Sample code for BERT tokenization using transformers library\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"Hello, how are you today?\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Add special tokens, padding, and truncation for model input\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=100)  # max_length is arbitrary here\n",
    "print(\"Input IDs:\", inputs['input_ids'])\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "\n",
    "# Review token IDs and tokens\n",
    "print(\"\\nSpecial tokens added by BERT:\")\n",
    "for token in tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]):\n",
    "    if token in ['[CLS]', '[SEP]']:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c93a4f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ad591f3a1244b0ae2490662ac789c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9037351f2534405a952c02063910c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adabdd3aa9e44930a1a40d849d381ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54ef95cb2974f37bdfa8cbd713fd575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I love this product!\n",
      "Predicted Sentiment: POSITIVE\n",
      "Confidence Score: 0.9998855590820312\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Sample sentence for sentiment analysis\n",
    "sentence = \"I love this product!\"\n",
    "\n",
    "# Perform sentiment analysis on the sample sentence\n",
    "result = sentiment_pipeline(sentence)\n",
    "\n",
    "# Output the predicted sentiment and confidence score\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Predicted Sentiment:\", result[0]['label'])\n",
    "print(\"Confidence Score:\", result[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388b4d48",
   "metadata": {},
   "source": [
    "CICE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57db89c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "class BERTSentimentAnalyzer:\n",
    "    def __init__(self, model_name='distilbert-base-uncased-finetuned-sst-2-english'):\n",
    "        # Load pre-trained tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Tokenize and prepare the input text as tensor\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        return inputs\n",
    "\n",
    "    def predict_sentiment(self, text):\n",
    "        # Preprocess the text\n",
    "        inputs = self.preprocess(text)\n",
    "\n",
    "        # Perform sentiment prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # Compute probabilities and determine sentiment\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "        confidence_score = probabilities[0][predicted_class].item()\n",
    "\n",
    "        # Map predicted class index to sentiment label\n",
    "        sentiment_labels = ['NEGATIVE', 'POSITIVE']\n",
    "        predicted_label = sentiment_labels[predicted_class]\n",
    "\n",
    "        return predicted_label, confidence_score\n",
    "\n",
    "# Testing the custom sentiment analyzer\n",
    "analyzer = BERTSentimentAnalyzer()\n",
    "\n",
    "test_texts = [\n",
    "    \"I love this product!\",\n",
    "    \"I hate this product.\",\n",
    "    \"This is okay, nothing special.\",\n",
    "    \"I'm really satisfied with the service.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    sentiment, score = analyzer.predict_sentiment(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}, Confidence Score: {score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac574f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0646e",
   "metadata": {},
   "source": [
    "CICE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bcffbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f3f8196cce42eb90f76cb82ab0b18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f78b9a631447ba80d1272114002960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fad3ea959a44eab7e4084749be11ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6211917759834824bf918cfc5af4fcc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde3cd0e1b0c40ef9f0893a3371922ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c323d0cfddb0430f8b0c8274ef115602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer and model loaded successfully.\n",
      "Text: Apple Inc. is looking at buying U.K. startup for $1 billion. Steve Jobs was the founder of Apple Inc.\n",
      "Recognized Entities:\n",
      "  Apple Inc: ORG\n",
      "  U . K .: LOC\n",
      "  Steve Job s: PER\n",
      "  Apple Inc: ORG\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "class BERTNamedEntityRecognizer:\n",
    "    def __init__(self, model_name='dslim/bert-base-NER'):\n",
    "        print(\"Loading tokenizer and model...\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "            print(\"Tokenizer and model loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during model initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "    def recognize_entities(self, text):\n",
    "        # Tokenize the input text and prepare tensors\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        # Predict named entities\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # Get the predictions as label IDs\n",
    "        predictions = outputs.logits.argmax(dim=-1).squeeze().tolist()\n",
    "\n",
    "        # Map predictions to tokens and labels\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        labels = self.model.config.id2label\n",
    "\n",
    "        # Extract entities based on BIO scheme\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "\n",
    "        for token, prediction in zip(tokens, predictions):\n",
    "            label = labels[prediction]\n",
    "\n",
    "            # Skip special tokens like [CLS] and [SEP]\n",
    "            if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                continue\n",
    "\n",
    "            if label.startswith('B-'):\n",
    "                # Beginning of a new entity\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                current_entity = {\n",
    "                    'text': token.replace('##', ''),  # Remove subword markers\n",
    "                    'entity': label.split('-')[1],\n",
    "                    'start': None,  # You can map token positions to text positions for more accuracy\n",
    "                    'end': None\n",
    "                }\n",
    "            elif label.startswith('I-') and current_entity:\n",
    "                # Continuation of the current entity\n",
    "                current_entity['text'] += ' ' + token.replace('##', '')\n",
    "            else:\n",
    "                # End of an entity or no entity\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "\n",
    "        # Add the last entity if there is one\n",
    "        if current_entity:\n",
    "            entities.append(current_entity)\n",
    "\n",
    "        return entities\n",
    "\n",
    "# Testing the NER system\n",
    "try:\n",
    "    recognizer = BERTNamedEntityRecognizer()\n",
    "    test_text = \"Apple Inc. is looking at buying U.K. startup for $1 billion. Steve Jobs was the founder of Apple Inc.\"\n",
    "    entities = recognizer.recognize_entities(test_text)\n",
    "    print(f\"Text: {test_text}\")\n",
    "    print(\"Recognized Entities:\")\n",
    "    for entity in entities:\n",
    "        print(f\"  {entity['text']}: {entity['entity']}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae192345",
   "metadata": {},
   "source": [
    "EXERCICE 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a7e54",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Initialization:\n",
    "\n",
    "The BERTNamedEntityRecognizer class is initialized with a pre-trained model name, which loads the corresponding BERT model and tokenizer for NER tasks.\n",
    "Token Classification:\n",
    "\n",
    "The recognize_entities method processes the input text by tokenizing it and passing it through the model to predict entity labels using the B-I-O tagging scheme.\n",
    "Entity Extraction:\n",
    "\n",
    "It then groups these tokens into entities, handling subword markers and ignoring special tokens such as [CLS] and [SEP].\n",
    "Testing:\n",
    "\n",
    "The script tests the NER system with a sample sentence containing entities like organizations (Apple Inc.), locations (U.K.), and persons (Steve Jobs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ff99a",
   "metadata": {},
   "source": [
    "Feature\tBERT\tGPT\n",
    "Architecture\tEncoder-based\tDecoder-based\n",
    "Primary Purpose\tUnderstanding context bidirectionally\tGenerating coherent text\n",
    "Common Use Cases\tQuestion answering, sentiment analysis\tText generation, completion, conversation\n",
    "Strengths\tDeep bidirectional understanding of context\tGenerates coherent and contextually relevant text\n",
    "Weaknesses\tLess effective at text generation\tMay lack depth in understanding context\n",
    "Reflection on Differences and Similarities:\n",
    "\n",
    "Both BERT and GPT are based on the Transformer architecture but are designed for different purposes. BERT excels in tasks requiring a deep understanding of text due to its bidirectional nature, while GPT is adept at generating text, thanks to its unidirectional approach which focuses on predicting the next token in a sequence. The choice between the two models should be guided by the specific requirements of the task at hand—whether it leans more towards understanding or generating text.\n",
    "\n",
    "This table and reflection should help in understanding the unique advantages and applications of BERT and GPT in the field of NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b784e",
   "metadata": {},
   "source": [
    "EXERCICE 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158b27c",
   "metadata": {},
   "source": [
    "BERT's Role in Retrieval\n",
    "\n",
    "In RAG systems, BERT is primarily used for the retrieval component. BERT’s bidirectional context understanding capabilities are leveraged to interpret and retrieve relevant information from a large corpus of documents. This involves:\n",
    "\n",
    "Generating Embeddings\n",
    "\n",
    "BERT generates embeddings for documents and queries by converting them into fixed-size vector representations. These vectors capture the semantic essence of the text, allowing for meaningful comparisons between documents and queries based on their content.\n",
    "\n",
    "Vector Database Usage\n",
    "\n",
    "The embeddings produced by BERT are stored in a vector database. This database allows for efficient similarity searches — when a query is made, its embedding is matched against those in the database to identify and retrieve the most relevant documents.\n",
    "\n",
    "Example of BERT and GPT Collaboration\n",
    "\n",
    "In a typical RAG system, the process might work as follows:\n",
    "\n",
    "A user submits a query.\n",
    "BERT processes this query and generates an embedding.\n",
    "The system searches the vector database for document embeddings that closely match the query embedding, retrieving the most relevant documents.\n",
    "These documents are then fed into a generative model like GPT, which uses the information to generate a comprehensive, contextually accurate response.\n",
    "By integrating BERT’s retrieval capabilities with GPT’s generative strength, RAG systems can produce responses that are not only contextually rich but also grounded in external knowledge, enhancing both the relevance and accuracy of generated content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
