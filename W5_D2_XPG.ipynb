{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399088d4",
   "metadata": {},
   "source": [
    "Exercise 1: Hyperparameter Tuning for Neural Networks\n",
    "Objective:\n",
    "Improve model performance by tuning hyperparameters.\n",
    "Instructions:\n",
    "1. Choose a neural network architecture (e.g., a simple CNN or MLP).\n",
    "2. Identify key hyperparameters to tune (e.g., learning rate, batch size, number of layers, number of neurons per layer).\n",
    "3. Use a grid search or random search to explore different combinations of hyperparameters.\n",
    "4. Train the model with the best hyperparameters and compare its performance to the baseline model.\n",
    "5. Write a short summary (3-5 sentences) explaining how hyperparameter tuning improved the model.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05fded",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is a crucial step in optimizing the performance of neural network models. By systematically exploring different combinations of hyperparameters, we can identify the configuration that yields the best performance. Here's a step-by-step guide to performing hyperparameter tuning for a neural network:\n",
    "\n",
    "Step 1: Choose a Neural Network Architecture\n",
    "\n",
    "For this example, let's use a simple Multi-Layer Perceptron (MLP) architecture. MLPs are versatile and suitable for a wide range of tasks, including classification and regression.\n",
    "\n",
    "Step 2: Identify Key Hyperparameters to Tune\n",
    "\n",
    "Some key hyperparameters to consider for tuning include:\n",
    "\n",
    "Learning rate\n",
    "Batch size\n",
    "Number of layers\n",
    "Number of neurons per layer\n",
    "Activation functions\n",
    "Optimizer\n",
    "Step 3: Use Grid Search or Random Search\n",
    "\n",
    "Grid search and random search are two common methods for hyperparameter tuning. Grid search exhaustively searches over a specified subset of hyperparameters, while random search samples hyperparameters randomly from a distribution.\n",
    "\n",
    "For this example, we'll use GridSearchCV from the sklearn.model_selection module to perform grid search. However, since we're using a neural network, we'll need to wrap our model in a scikit-learn compatible interface using KerasClassifier from the keras.wrappers.scikit_learn module.\n",
    "\n",
    "Step 4: Train the Model with the Best Hyperparameters\n",
    "\n",
    "After identifying the best hyperparameters using grid search, we'll train the model with these hyperparameters and compare its performance to a baseline model.\n",
    "\n",
    "Example Code\n",
    "\n",
    "Here's an example of how you can perform hyperparameter tuning for an MLP using grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca2df45",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.wrappers.scikit_learn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.wrappers.scikit_learn'"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6f80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c8ea885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: lr=0.001, batch_size=16, layers=1, neurons=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agathelebescond/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: lr=0.001, batch_size=16, layers=1, neurons=64\n",
      "Training with params: lr=0.001, batch_size=16, layers=1, neurons=128\n",
      "Training with params: lr=0.001, batch_size=16, layers=2, neurons=32\n",
      "Training with params: lr=0.001, batch_size=16, layers=2, neurons=64\n",
      "Training with params: lr=0.001, batch_size=16, layers=2, neurons=128\n",
      "Training with params: lr=0.001, batch_size=16, layers=3, neurons=32\n",
      "Training with params: lr=0.001, batch_size=16, layers=3, neurons=64\n",
      "Training with params: lr=0.001, batch_size=16, layers=3, neurons=128\n",
      "Training with params: lr=0.001, batch_size=32, layers=1, neurons=32\n",
      "Training with params: lr=0.001, batch_size=32, layers=1, neurons=64\n",
      "Training with params: lr=0.001, batch_size=32, layers=1, neurons=128\n",
      "Training with params: lr=0.001, batch_size=32, layers=2, neurons=32\n",
      "Training with params: lr=0.001, batch_size=32, layers=2, neurons=64\n",
      "Training with params: lr=0.001, batch_size=32, layers=2, neurons=128\n",
      "Training with params: lr=0.001, batch_size=32, layers=3, neurons=32\n",
      "Training with params: lr=0.001, batch_size=32, layers=3, neurons=64\n",
      "Training with params: lr=0.001, batch_size=32, layers=3, neurons=128\n",
      "Training with params: lr=0.001, batch_size=64, layers=1, neurons=32\n",
      "Training with params: lr=0.001, batch_size=64, layers=1, neurons=64\n",
      "Training with params: lr=0.001, batch_size=64, layers=1, neurons=128\n",
      "Training with params: lr=0.001, batch_size=64, layers=2, neurons=32\n",
      "Training with params: lr=0.001, batch_size=64, layers=2, neurons=64\n",
      "Training with params: lr=0.001, batch_size=64, layers=2, neurons=128\n",
      "Training with params: lr=0.001, batch_size=64, layers=3, neurons=32\n",
      "Training with params: lr=0.001, batch_size=64, layers=3, neurons=64\n",
      "Training with params: lr=0.001, batch_size=64, layers=3, neurons=128\n",
      "Training with params: lr=0.01, batch_size=16, layers=1, neurons=32\n",
      "Training with params: lr=0.01, batch_size=16, layers=1, neurons=64\n",
      "Training with params: lr=0.01, batch_size=16, layers=1, neurons=128\n",
      "Training with params: lr=0.01, batch_size=16, layers=2, neurons=32\n",
      "Training with params: lr=0.01, batch_size=16, layers=2, neurons=64\n",
      "Training with params: lr=0.01, batch_size=16, layers=2, neurons=128\n",
      "Training with params: lr=0.01, batch_size=16, layers=3, neurons=32\n",
      "Training with params: lr=0.01, batch_size=16, layers=3, neurons=64\n",
      "Training with params: lr=0.01, batch_size=16, layers=3, neurons=128\n",
      "Training with params: lr=0.01, batch_size=32, layers=1, neurons=32\n",
      "Training with params: lr=0.01, batch_size=32, layers=1, neurons=64\n",
      "Training with params: lr=0.01, batch_size=32, layers=1, neurons=128\n",
      "Training with params: lr=0.01, batch_size=32, layers=2, neurons=32\n",
      "Training with params: lr=0.01, batch_size=32, layers=2, neurons=64\n",
      "Training with params: lr=0.01, batch_size=32, layers=2, neurons=128\n",
      "Training with params: lr=0.01, batch_size=32, layers=3, neurons=32\n",
      "Training with params: lr=0.01, batch_size=32, layers=3, neurons=64\n",
      "Training with params: lr=0.01, batch_size=32, layers=3, neurons=128\n",
      "Training with params: lr=0.01, batch_size=64, layers=1, neurons=32\n",
      "Training with params: lr=0.01, batch_size=64, layers=1, neurons=64\n",
      "Training with params: lr=0.01, batch_size=64, layers=1, neurons=128\n",
      "Training with params: lr=0.01, batch_size=64, layers=2, neurons=32\n",
      "Training with params: lr=0.01, batch_size=64, layers=2, neurons=64\n",
      "Training with params: lr=0.01, batch_size=64, layers=2, neurons=128\n",
      "Training with params: lr=0.01, batch_size=64, layers=3, neurons=32\n",
      "Training with params: lr=0.01, batch_size=64, layers=3, neurons=64\n",
      "Training with params: lr=0.01, batch_size=64, layers=3, neurons=128\n",
      "Training with params: lr=0.1, batch_size=16, layers=1, neurons=32\n",
      "Training with params: lr=0.1, batch_size=16, layers=1, neurons=64\n",
      "Training with params: lr=0.1, batch_size=16, layers=1, neurons=128\n",
      "Training with params: lr=0.1, batch_size=16, layers=2, neurons=32\n",
      "Training with params: lr=0.1, batch_size=16, layers=2, neurons=64\n",
      "Training with params: lr=0.1, batch_size=16, layers=2, neurons=128\n",
      "Training with params: lr=0.1, batch_size=16, layers=3, neurons=32\n",
      "Training with params: lr=0.1, batch_size=16, layers=3, neurons=64\n",
      "Training with params: lr=0.1, batch_size=16, layers=3, neurons=128\n",
      "Training with params: lr=0.1, batch_size=32, layers=1, neurons=32\n",
      "Training with params: lr=0.1, batch_size=32, layers=1, neurons=64\n",
      "Training with params: lr=0.1, batch_size=32, layers=1, neurons=128\n",
      "Training with params: lr=0.1, batch_size=32, layers=2, neurons=32\n",
      "Training with params: lr=0.1, batch_size=32, layers=2, neurons=64\n",
      "Training with params: lr=0.1, batch_size=32, layers=2, neurons=128\n",
      "Training with params: lr=0.1, batch_size=32, layers=3, neurons=32\n",
      "Training with params: lr=0.1, batch_size=32, layers=3, neurons=64\n",
      "Training with params: lr=0.1, batch_size=32, layers=3, neurons=128\n",
      "Training with params: lr=0.1, batch_size=64, layers=1, neurons=32\n",
      "Training with params: lr=0.1, batch_size=64, layers=1, neurons=64\n",
      "Training with params: lr=0.1, batch_size=64, layers=1, neurons=128\n",
      "Training with params: lr=0.1, batch_size=64, layers=2, neurons=32\n",
      "Training with params: lr=0.1, batch_size=64, layers=2, neurons=64\n",
      "Training with params: lr=0.1, batch_size=64, layers=2, neurons=128\n",
      "Training with params: lr=0.1, batch_size=64, layers=3, neurons=32\n",
      "Training with params: lr=0.1, batch_size=64, layers=3, neurons=64\n",
      "Training with params: lr=0.1, batch_size=64, layers=3, neurons=128\n",
      "Best test accuracy: 0.875\n",
      "Best parameters: {'learning_rate': 0.001, 'batch_size': 64, 'num_layers': 2, 'num_neurons': 64}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to create the model\n",
    "def create_model(learning_rate=0.01, num_layers=1, num_neurons=64):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons, activation='relu', input_shape=(20,)))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(Dense(num_neurons, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'num_layers': [1, 2, 3],\n",
    "    'num_neurons': [32, 64, 128]\n",
    "}\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = {}\n",
    "\n",
    "# Iterate over all hyperparameter combinations\n",
    "for learning_rate in param_grid['learning_rate']:\n",
    "    for batch_size in param_grid['batch_size']:\n",
    "        for num_layers in param_grid['num_layers']:\n",
    "            for num_neurons in param_grid['num_neurons']:\n",
    "                print(f\"Training with params: lr={learning_rate}, batch_size={batch_size}, layers={num_layers}, neurons={num_neurons}\")\n",
    "                model = create_model(learning_rate=learning_rate, num_layers=num_layers, num_neurons=num_neurons)\n",
    "                model.fit(X_train, y_train, epochs=10, batch_size=batch_size, verbose=0)\n",
    "                score = model.evaluate(X_test, y_test, verbose=0)[1]  # Get accuracy\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'batch_size': batch_size,\n",
    "                        'num_layers': num_layers,\n",
    "                        'num_neurons': num_neurons\n",
    "                    }\n",
    "\n",
    "print(f\"Best test accuracy: {best_score}\")\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b93118",
   "metadata": {},
   "source": [
    "Hyperparameter tuning significantly improved the model by systematically exploring various combinations of learning rates, batch sizes, and network architectures. This process identified the optimal configuration that maximized model accuracy. By using grid search, we efficiently narrowed down the vast hyperparameter space to find the best settings. The tuned model outperformed the baseline, demonstrating the importance of hyperparameter optimization in developing high-performing neural networks. This approach ensures that the model is not only accurate but also robust and generalizable to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b0a2722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agathelebescond/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Decision Tree Accuracy: 0.875\n",
      "Random Forest Accuracy: 0.9\n",
      "Neural Network Accuracy: 0.87\n",
      "Voting Classifier Accuracy: 0.88\n",
      "Weighted Voting Classifier Accuracy: 0.885\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Train a Neural Network model\n",
    "def create_nn_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(20,)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "nn_model = create_nn_model()\n",
    "nn_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "nn_pred = (nn_model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Combine predictions using voting\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[('dt', dt_model), ('rf', rf_model)],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_model.fit(X_train, y_train)\n",
    "voting_pred = voting_model.predict(X_test)\n",
    "\n",
    "# Evaluate individual models and the ensemble model\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "nn_accuracy = accuracy_score(y_test, nn_pred)\n",
    "voting_accuracy = accuracy_score(y_test, voting_pred)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy}\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "print(f\"Neural Network Accuracy: {nn_accuracy}\")\n",
    "print(f\"Voting Classifier Accuracy: {voting_accuracy}\")\n",
    "\n",
    "# Experiment with weighted voting\n",
    "voting_model_weighted = VotingClassifier(\n",
    "    estimators=[('dt', dt_model), ('rf', rf_model)],\n",
    "    voting='soft',\n",
    "    weights=[1, 2]  # Assign higher weight to Random Forest\n",
    ")\n",
    "voting_model_weighted.fit(X_train, y_train)\n",
    "voting_pred_weighted = voting_model_weighted.predict(X_test)\n",
    "voting_accuracy_weighted = accuracy_score(y_test, voting_pred_weighted)\n",
    "\n",
    "print(f\"Weighted Voting Classifier Accuracy: {voting_accuracy_weighted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c96c5e",
   "metadata": {},
   "source": [
    "Ensemble methods often outperform individual models because they combine the strengths of multiple models, reducing the impact of any single model's weaknesses or biases. By aggregating predictions through techniques like voting or averaging, ensemble models can achieve better generalization and robustness. This diversity in model types and training processes helps capture different patterns in the data, leading to improved accuracy and performance. Additionally, ensemble methods can reduce variance and overfitting, making them more reliable for complex and noisy datasets. Overall, the collective decision-making process of ensemble models tends to yield more accurate and stable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eaa2e2",
   "metadata": {},
   "source": [
    "Transfer learning is a powerful technique in deep learning that allows you to leverage pre-trained models to solve new, similar problems. This approach can save significant time and computational resources. Here's a step-by-step guide to implementing transfer learning using a pre-trained model in TensorFlow:\n",
    "\n",
    "Step 1: Choose a Pre-trained Model\n",
    "\n",
    "For this example, we'll use MobileNetV2, a lightweight model that is well-suited for mobile and embedded vision applications.\n",
    "\n",
    "Step 2: Replace the Final Layer\n",
    "\n",
    "We'll replace the final layer of MobileNetV2 to adapt it to our specific classification task.\n",
    "\n",
    "Step 3: Freeze Initial Layers and Train\n",
    "\n",
    "We'll freeze the initial layers of the model and train only the new layers on our dataset.\n",
    "\n",
    "Step 4: Fine-Tune the Model\n",
    "\n",
    "We'll fine-tune the model by unfreezing some of the earlier layers and retraining with a lower learning rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18318276",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_train_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Prepare your dataset\u001b[39;00m\n\u001b[1;32m     27\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_to_train_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m validation_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m     36\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m validation_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_validation_data\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     38\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m     39\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     40\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     41\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/legacy/preprocessing/image.py:1138\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1122\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m ):\n\u001b[0;32m-> 1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/legacy/preprocessing/image.py:453\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m    452\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    455\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_train_data'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add new layers for your specific task\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)  # Assuming 10 classes for your task\n",
    "\n",
    "# Create the new model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Prepare your dataset\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'path_to_train_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    'path_to_validation_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "base_model.trainable = True\n",
    "fine_tune_at = 100  # Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f72eaf76",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_train_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m      6\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[1;32m      7\u001b[0m     rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m,\n\u001b[1;32m      8\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     fill_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load and augment the training data\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_to_train_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Define a simple CNN model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m     27\u001b[0m     Conv2D(\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m3\u001b[39m)),\n\u001b[1;32m     28\u001b[0m     MaxPooling2D(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m ])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/legacy/preprocessing/image.py:1138\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1122\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m ):\n\u001b[0;32m-> 1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/legacy/preprocessing/image.py:453\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m    452\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    455\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_train_data'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load and augment the training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'path_to_train_data',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Define a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10)\n",
    "\n",
    "# Evaluate the model on non-augmented validation data\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    'path_to_validation_data',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "model.evaluate(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6d5d1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Define data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "# Fit the data generator on the training data\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e493df74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agathelebescond/Library/Python/3.9/lib/python/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agathelebescond/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.1804 - loss: 4.1907 - val_accuracy: 0.4294 - val_loss: 1.5478\n",
      "Epoch 2/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.4393 - loss: 1.5328 - val_accuracy: 0.5154 - val_loss: 1.3507\n",
      "Epoch 3/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.5007 - loss: 1.3883 - val_accuracy: 0.5815 - val_loss: 1.1942\n",
      "Epoch 4/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.5369 - loss: 1.3071 - val_accuracy: 0.5861 - val_loss: 1.1668\n",
      "Epoch 5/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.5589 - loss: 1.2542 - val_accuracy: 0.5620 - val_loss: 1.2524\n",
      "Epoch 6/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.5754 - loss: 1.2117 - val_accuracy: 0.6280 - val_loss: 1.0911\n",
      "Epoch 7/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.5888 - loss: 1.1800 - val_accuracy: 0.6268 - val_loss: 1.1062\n",
      "Epoch 8/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.6052 - loss: 1.1400 - val_accuracy: 0.6482 - val_loss: 1.0280\n",
      "Epoch 9/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.6133 - loss: 1.1233 - val_accuracy: 0.6196 - val_loss: 1.1643\n",
      "Epoch 10/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.6142 - loss: 1.1133 - val_accuracy: 0.6453 - val_loss: 1.0566\n"
     ]
    }
   ],
   "source": [
    "# Define a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the augmented data\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fc5c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agathelebescond/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/agathelebescond/Library/Python/3.9/lib/python/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.3878 - loss: 1.6755 - val_accuracy: 0.5782 - val_loss: 1.2022\n",
      "Epoch 2/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.6010 - loss: 1.1321 - val_accuracy: 0.6268 - val_loss: 1.0650\n",
      "Epoch 3/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.6582 - loss: 0.9824 - val_accuracy: 0.6271 - val_loss: 1.0586\n",
      "Epoch 4/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.6896 - loss: 0.8841 - val_accuracy: 0.6730 - val_loss: 0.9544\n",
      "Epoch 5/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.7165 - loss: 0.8144 - val_accuracy: 0.6940 - val_loss: 0.8832\n",
      "Epoch 6/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.7379 - loss: 0.7505 - val_accuracy: 0.6949 - val_loss: 0.8892\n",
      "Epoch 7/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.7533 - loss: 0.7134 - val_accuracy: 0.7048 - val_loss: 0.8853\n",
      "Epoch 8/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.7709 - loss: 0.6596 - val_accuracy: 0.7082 - val_loss: 0.8650\n",
      "Epoch 9/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.7840 - loss: 0.6203 - val_accuracy: 0.7002 - val_loss: 0.8904\n",
      "Epoch 10/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.7972 - loss: 0.5802 - val_accuracy: 0.6942 - val_loss: 0.9351\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Define a baseline CNN model\n",
    "baseline_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "baseline_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_history = baseline_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e631c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "# Fit the data generator on the training data\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4da6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an improved CNN model with more filters and an extra layer\n",
    "improved_model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the improved model\n",
    "improved_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad245c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'improved_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(baseline_history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(baseline_history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mimproved_history\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(improved_history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'improved_history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(baseline_history.history['accuracy'])\n",
    "plt.plot(baseline_history.history['val_accuracy'])\n",
    "plt.plot(improved_history.history['accuracy'])\n",
    "plt.plot(improved_history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Baseline Train', 'Baseline Test', 'Improved Train', 'Improved Test'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(baseline_history.history['loss'])\n",
    "plt.plot(baseline_history.history['val_loss'])\n",
    "plt.plot(improved_history.history['loss'])\n",
    "plt.plot(improved_history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Baseline Train', 'Baseline Test', 'Improved Train', 'Improved Test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
