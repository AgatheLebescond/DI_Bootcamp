{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32c4578",
   "metadata": {},
   "source": [
    "\n",
    "EXERCICE 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be3bd8",
   "metadata": {},
   "source": [
    "Exercise 1: Identifying Limitations of Traditional Language Models\n",
    "Objective: Recognize the challenges faced by traditional language models in handling long-range dependencies and complex contexts.\n",
    "Instructions:\n",
    "Consider the following sentence: “The scientist, who had been working on the project for years, finally made a breakthrough discovery.”\n",
    "Analyze how a traditional language model, which processes text sequentially, might struggle to capture the relationship between “scientist” and “discovery” due to the intervening words.\n",
    "Explain how this limitation can affect the model’s ability to accurately understand the sentence’s meaning and perform tasks like question answering or summarization.\n",
    "Discuss how the attention mechanism addresses this challenge by allowing the model to focus on relevant words regardless of their position in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ff67e",
   "metadata": {},
   "source": [
    "Traditional language models, which process text sequentially, often struggle with capturing long-range dependencies in sentences. This is primarily because they rely on the immediate context to generate or interpret each word, which can dilute the relationship between words that are far apart in a sentence.\n",
    "\n",
    "Let's break down the example sentence: “The scientist, who had been working on the project for years, finally made a breakthrough discovery.”\n",
    "\n",
    "Sequential Processing Challenge:\n",
    "\n",
    "In traditional models, the information about the \"scientist\" is processed at the beginning of the sentence. As the model processes each subsequent word, the initial context about the scientist can become less influential.\n",
    "By the time the model reaches the word \"discovery,\" the direct connection to the \"scientist\" may be weakened due to the intervening words and clauses (\"who had been working on the project for years, finally made a breakthrough\").\n",
    "This can lead to a diminished understanding of who is making the discovery, potentially impacting tasks like question answering or summarization where identifying the subject and their actions is crucial.\n",
    "Impact on Understanding and Tasks:\n",
    "\n",
    "Question Answering: If asked, \"Who made the breakthrough discovery?\" the model might struggle to accurately identify the \"scientist\" as the subject due to the weakened connection.\n",
    "Summarization: In summarizing the sentence, the model might fail to emphasize the relationship between the scientist and the discovery, leading to a less coherent summary.\n",
    "Attention Mechanism:\n",
    "\n",
    "The attention mechanism addresses this challenge by allowing the model to dynamically focus on different parts of the input sequence, regardless of their position.\n",
    "It assigns weights to each word in the sentence, determining the importance of each word relative to the current word being processed. This means that even if \"scientist\" and \"discovery\" are far apart, the model can still strongly associate them.\n",
    "For example, when processing the word \"discovery,\" the attention mechanism can assign a high weight to the word \"scientist,\" reinforcing the connection between the two.\n",
    "This ability to focus on relevant words helps the model maintain a coherent understanding of the sentence, improving performance in tasks that require comprehension of long-range dependencies.\n",
    "In summary, while traditional language models may struggle with long-range dependencies due to their sequential processing nature, the attention mechanism enhances the model's ability to capture and maintain relationships between distant words, leading to better understanding and task performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d015756b",
   "metadata": {},
   "source": [
    "Exercise 2: Exploring the Impact of Attention in Transformers\n",
    "Objective: Understand how the attention mechanism enhances the capabilities of transformer models in various NLP tasks.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Choose an NLP task, such as machine translation, text summarization, or question answering.\n",
    "Research how transformer models, like BERT or GPT, utilize the attention mechanism to achieve state-of-the-art results in the chosen task.\n",
    "Provide specific examples of how attention helps the model capture long-range dependencies, resolve ambiguities, and handle complex contexts.\n",
    "Compare the performance of transformer models with and without attention mechanisms on the chosen task, highlighting the improvements achieved through attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa2c99d",
   "metadata": {},
   "source": [
    "Transformer models, such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer), have revolutionized the field of natural language processing (NLP) by leveraging the attention mechanism to achieve state-of-the-art results across various tasks. Let's explore how attention enhances the capabilities of these models in the context of machine translation, one of the fundamental NLP tasks.\n",
    "\n",
    "Machine Translation\n",
    "\n",
    "How Attention is Utilized\n",
    "\n",
    "Capturing Long-Range Dependencies:\n",
    "\n",
    "In machine translation, capturing the relationship between words across the source and target languages is crucial. Attention mechanisms allow the model to focus on relevant parts of the input sentence, regardless of the distance between related words.\n",
    "For example, when translating the sentence \"The cat, which was sitting on the mat, is black\" from English to French, the model needs to associate \"cat\" with \"black\" despite the intervening words. The attention mechanism helps maintain this association by assigning higher weights to these words during the translation process.\n",
    "Resolving Ambiguities:\n",
    "\n",
    "Attention helps resolve ambiguities by allowing the model to consider the context of each word in the sentence. For instance, the word \"bank\" can refer to a financial institution or the side of a river. The attention mechanism enables the model to focus on surrounding words that provide context, such as \"money\" or \"river,\" to determine the correct translation.\n",
    "Handling Complex Contexts:\n",
    "\n",
    "In complex sentences with multiple clauses or nested structures, attention mechanisms help the model keep track of the relationships between different parts of the sentence. This is particularly important in languages with different grammatical structures, where the order of words may vary significantly between the source and target languages.\n",
    "Performance Comparison\n",
    "\n",
    "With Attention:\n",
    "\n",
    "Transformer models with attention mechanisms have achieved significant improvements in machine translation tasks. For example, the original Transformer model introduced in the paper \"Attention Is All You Need\" by Vaswani et al. demonstrated superior performance on various machine translation benchmarks, such as the WMT 2014 English-to-German and English-to-French translation tasks.\n",
    "Attention allows the model to generate more accurate and fluent translations by capturing the nuances and dependencies in the source language and effectively mapping them to the target language.\n",
    "Without Attention:\n",
    "\n",
    "Traditional models, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, process text sequentially and struggle with long-range dependencies. These models often rely on hidden states to capture context, which can be insufficient for complex sentences with multiple clauses or nested structures.\n",
    "Without attention, these models may produce translations that lack coherence and accuracy, particularly in sentences with ambiguous or complex contexts. The sequential processing nature of RNNs and LSTMs can lead to a loss of important contextual information, resulting in poorer performance on machine translation tasks.\n",
    "Examples and Improvements\n",
    "\n",
    "Example Sentence:\n",
    "\n",
    "Consider the sentence \"The agreement on the bank account was signed yesterday.\" In this sentence, the word \"bank\" is ambiguous. The attention mechanism helps the model focus on the word \"account,\" which provides context and indicates that \"bank\" refers to a financial institution.\n",
    "Improvements Achieved:\n",
    "\n",
    "Transformer models with attention have shown significant improvements in translation quality, as measured by metrics such as BLEU (Bilingual Evaluation Understudy) scores. For instance, the Transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, outperforming previous state-of-the-art models.\n",
    "Attention mechanisms also enable the model to handle rare and unseen words more effectively, as they can leverage contextual information to infer the meaning of these words.\n",
    "In summary, the attention mechanism plays a crucial role in enhancing the capabilities of transformer models in machine translation. By capturing long-range dependencies, resolving ambiguities, and handling complex contexts, attention enables these models to achieve state-of-the-art results and generate more accurate and fluent translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e9a3b0",
   "metadata": {},
   "source": [
    "EXERCICE 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301780f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ea205",
   "metadata": {},
   "source": [
    "Step 2: Load and Preprocess the Knowledge Source\n",
    "\n",
    "Choose a knowledge source, such as a Wikipedia article or a collection of text documents. For this example, let's assume you have a list of text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
    "    \"It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\",\n",
    "    \"Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\",\n",
    "    \"The tower is 330 meters tall, about the same height as an 81-storey building, and the tallest structure in Paris.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b5134",
   "metadata": {},
   "source": [
    "EXERCICE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3366e65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d296c18c177b4e0ead5ad44476e9960a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Generate embeddings for the documents\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m document_embeddings = generate_embeddings(\u001b[43mdocuments\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = generate_embeddings(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f606bb",
   "metadata": {},
   "source": [
    "EXERCICE 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1247d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2696,  0.2138,  0.0747,  ..., -0.0606,  0.1561,  0.2414],\n",
      "        [-0.0578,  0.2224,  0.1793,  ...,  0.0750,  0.0988,  0.2229],\n",
      "        [-0.2990,  0.2202, -0.1587,  ..., -0.1633,  0.0174,  0.1159],\n",
      "        [-0.2566,  0.0132,  0.3361,  ...,  0.0572,  0.3264,  0.1562]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(texts):\n",
    "    # Tokenize the input texts\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Use the mean of the last hidden states as the embedding\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Example list of documents\n",
    "documents = [\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
    "    \"It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\",\n",
    "    \"Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\",\n",
    "    \"The tower is 330 meters tall, about the same height as an 81-storey building, and the tallest structure in Paris.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = generate_embeddings(documents)\n",
    "\n",
    "# Print the embeddings\n",
    "print(document_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0de206",
   "metadata": {},
   "source": [
    "EXERCICE 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d6516",
   "metadata": {},
   "source": [
    "Role of BERT in the Retrieval Component\n",
    "\n",
    "Generating Embeddings:\n",
    "\n",
    "Document Embeddings: BERT is used to generate embeddings for documents in the knowledge source. Each document is converted into a fixed-size vector that represents its semantic content. This is done by feeding the document text into BERT and extracting the embeddings from the model's output.\n",
    "Query Embeddings: Similarly, when a user submits a query, BERT generates an embedding for the query. This embedding captures the semantic meaning of the query and is used to retrieve relevant documents.\n",
    "Similarity Search:\n",
    "\n",
    "The retrieval component uses the embeddings generated by BERT to perform a similarity search. The query embedding is compared against the document embeddings using a similarity metric, such as cosine similarity. Documents with embeddings that are most similar to the query embedding are retrieved as the most relevant documents.\n",
    "Advantages of Using BERT Embeddings\n",
    "\n",
    "Contextual Understanding:\n",
    "\n",
    "BERT generates contextual embeddings, meaning that the embedding for a word or sentence takes into account the context in which it appears. This is in contrast to traditional methods like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings (e.g., Word2Vec, GloVe), which do not capture context to the same extent.\n",
    "Contextual embeddings allow BERT to understand the nuances and complexities of language, leading to more accurate and relevant retrieval results.\n",
    "Semantic Similarity:\n",
    "\n",
    "BERT embeddings capture semantic similarity, meaning that documents and queries with similar meanings will have similar embeddings, even if they use different words. This is particularly useful in question answering, where the same concept can be expressed in various ways.\n",
    "Traditional methods like TF-IDF rely on exact word matches, which can lead to less accurate retrieval results when dealing with synonyms or paraphrases.\n",
    "Handling Long-Range Dependencies:\n",
    "\n",
    "BERT's attention mechanism allows it to capture long-range dependencies in text, meaning that it can understand the relationships between words and sentences that are far apart. This is important in question answering, where relevant information may be spread across different parts of a document.\n",
    "Traditional methods like TF-IDF or word embeddings struggle with long-range dependencies, as they do not have a mechanism to capture the relationships between distant words.\n",
    "Improved Performance:\n",
    "\n",
    "The use of BERT embeddings in the retrieval component has been shown to improve the performance of RAG systems in various applications. For example, in question answering, BERT-based retrieval can lead to more accurate and relevant answers, as it is better able to capture the semantic meaning of the query and the documents.\n",
    "Analysis of BERT's Contribution\n",
    "\n",
    "The effectiveness of BERT in RAG systems can be attributed to its ability to generate high-quality contextual embeddings. These embeddings capture the semantic meaning of text, allowing the retrieval component to identify relevant documents based on their content rather than just keyword matches. This leads to more accurate and relevant retrieval results, which in turn improves the performance of the generation component.\n",
    "\n",
    "In question answering, the use of BERT embeddings ensures that the retrieved documents are semantically similar to the query, providing the generation component with the most relevant information to generate an accurate answer. This is particularly important in complex question answering tasks, where the query may involve multiple concepts or require a deep understanding of the context.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "BERT's ability to generate contextual embeddings makes it a powerful tool for the retrieval component of RAG systems. Its advantages over traditional methods like TF-IDF or word embeddings, such as contextual understanding, semantic similarity, and handling long-range dependencies, contribute to the effectiveness of RAG systems in various applications, including question answering. By leveraging BERT embeddings, RAG systems can achieve state-of-the-art performance and provide more accurate and relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3daaa4",
   "metadata": {},
   "source": [
    "To implement a basic Retrieval-Augmented Generation (RAG) system, we'll need to set up a few components: a knowledge source, a method to generate embeddings using BERT, a vector database to store and retrieve these embeddings, and a generative model to produce answers based on retrieved information.\n",
    "\n",
    "Below is a simplified example of how you might implement such a system using Python. This example will use the Hugging Face Transformers library for BERT and a simple in-memory storage for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89fede23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context: It is named after the engineer Gustave Eiffel, whose company designed and built the tower., the answer to 'Who designed the Eiffel Tower?' is: Gustave Eiffel's company designed and built the tower.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample knowledge source\n",
    "documents = [\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
    "    \"It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\",\n",
    "    \"Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair.\",\n",
    "    \"The tower is 330 meters tall, about the same height as an 81-storey building.\"\n",
    "]\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# Generate embeddings for documents\n",
    "document_embeddings = generate_embeddings(documents)\n",
    "\n",
    "# Function to retrieve relevant documents\n",
    "def retrieve_relevant_documents(query, document_embeddings, documents, top_k=1):\n",
    "    query_embedding = generate_embeddings([query])\n",
    "    similarities = cosine_similarity(query_embedding, document_embeddings)\n",
    "    relevant_indices = np.argsort(similarities[0])[-top_k:][::-1]\n",
    "    return [documents[i] for i in relevant_indices]\n",
    "\n",
    "# Example query\n",
    "query = \"Who designed the Eiffel Tower?\"\n",
    "relevant_docs = retrieve_relevant_documents(query, document_embeddings, documents)\n",
    "\n",
    "# Simple generator function\n",
    "def generate_answer(query, relevant_docs):\n",
    "    context = \" \".join(relevant_docs)\n",
    "    # In a real application, you would use a generative model here\n",
    "    answer = f\"Based on the context: {context}, the answer to '{query}' is: Gustave Eiffel's company designed and built the tower.\"\n",
    "    return answer\n",
    "\n",
    "# Generate an answer\n",
    "answer = generate_answer(query, relevant_docs)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ef438",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9db4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context: It is named after the engineer Gustave Eiffel, whose company designed and built the tower., the answer to 'Who designed the Eiffel Tower?' is: Gustave Eiffel's company designed and built the tower.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample knowledge source\n",
    "documents = [\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
    "    \"It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\",\n",
    "    \"Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair.\",\n",
    "    \"The tower is 330 meters tall, about the same height as an 81-storey building.\"\n",
    "]\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# Generate embeddings for documents\n",
    "document_embeddings = generate_embeddings(documents)\n",
    "\n",
    "# Function to retrieve relevant documents\n",
    "def retrieve_relevant_documents(query, document_embeddings, documents, top_k=1):\n",
    "    query_embedding = generate_embeddings([query])\n",
    "    similarities = cosine_similarity(query_embedding, document_embeddings)\n",
    "    relevant_indices = np.argsort(similarities[0])[-top_k:][::-1]\n",
    "    return [documents[i] for i in relevant_indices]\n",
    "\n",
    "# Example query\n",
    "query = \"Who designed the Eiffel Tower?\"\n",
    "relevant_docs = retrieve_relevant_documents(query, document_embeddings, documents)\n",
    "\n",
    "# Simple generator function\n",
    "def generate_answer(query, relevant_docs):\n",
    "    context = \" \".join(relevant_docs)\n",
    "    # In a real application, you would use a generative model here\n",
    "    answer = f\"Based on the context: {context}, the answer to '{query}' is: Gustave Eiffel's company designed and built the tower.\"\n",
    "    return answer\n",
    "\n",
    "# Generate an answer\n",
    "answer = generate_answer(query, relevant_docs)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb7234",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
