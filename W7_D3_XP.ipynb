{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea38d7de",
   "metadata": {},
   "source": [
    "# Exercices XP\n",
    "Derni√®re mise √† jour : 7 juillet 2025\n",
    "\n",
    "üë©‚Äçüè´ üë©üèø‚Äçüè´ Ce que vous apprendrez\n",
    "√âvaluation pratique des LLM : acqu√©rez une exp√©rience pratique de l'√©valuation des LLM pour la synth√®se.\n",
    "Metric Deep Dive : Comprendre les forces et les faiblesses de diverses mesures d'√©valuation (pr√©cision, ROUGE).\n",
    "Comparaison de mod√®les : apprenez √† comparer syst√©matiquement diff√©rents LLM et tailles de mod√®les.\n",
    "Comp√©tence en mati√®re de Hugging Face : am√©liorez vos comp√©tences dans l'utilisation de Hugging Face transformerset evaluatedes biblioth√®ques.\n",
    "Personnalisation : mettre en ≈ìuvre et analyser les effets de la modification des mesures d‚Äô√©valuation et des param√®tres du mod√®le.\n",
    "Gestion des donn√©es : d√©couvrez comment charger, traiter et √©chantillonner des ensembles de donn√©es textuelles √† l‚Äôaide de pandas.\n",
    "Pr√©traitement de texte : comprendre l‚Äôimportance du pr√©traitement de texte pour les t√¢ches de PNL.\n",
    "D√©bogage et analyse : D√©velopper des comp√©tences en mati√®re de d√©bogage et d'analyse des r√©sultats du LLM.\n",
    "\n",
    "\n",
    "üõ†Ô∏è Ce que vous allez cr√©er\n",
    "Scripts d'√©valuation : scripts Python pour calculer et comparer les m√©triques de r√©sum√©.\n",
    "Rapports comparatifs : DataFrames et visualisations r√©sumant les performances de diff√©rents LLM.\n",
    "Mesures d‚Äô√©valuation modifi√©es : mesures de pr√©cision personnalis√©es adapt√©es √† la synth√®se.\n",
    "R√©sultats de synth√®se : r√©sum√©s g√©n√©r√©s √† partir de divers LLM pour une analyse comparative.\n",
    "Rapports analytiques : documentation de vos r√©sultats, y compris des discussions sur le comportement des m√©triques et les performances du mod√®le.\n",
    "Fonctions personnalis√©es : fonctions permettant de charger des ensembles de donn√©es, de g√©n√©rer des r√©sum√©s et de calculer les scores ROUGE.\n",
    "Tableaux de comparaison de mod√®les : tableaux comparant les performances de diff√©rents LLM en fonction de diverses mesures.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tous les exercices d'aujourd'hui font partie d'un tutoriel pratique unique con√ßu pour vous apprendre √† √©valuer les LLM sur des t√¢ches de synth√®se. Ensemble, vous :\n",
    "\n",
    "Mesurer la pr√©cision des r√©sultats r√©capitulatifs\n",
    "Calculer les scores ROUGE-N\n",
    "Construire un cadre coh√©rent pour comparer diff√©rentes tailles et architectures de mod√®les\n",
    "Chaque partie s'appuie sur la pr√©c√©dente, vous offrant un flux de travail coh√©rent pour √©valuer et comparer les performances de synth√®se.\n",
    "\n",
    "\n",
    "\n",
    "Objectifs d'apprentissage\n",
    "Compr√©hension m√©trique : apprenez √† calculer ROUGE-N et √† comprendre ses nuances.\n",
    "D√©veloppement de l'intuition : D√©velopper une compr√©hension intuitive de ROUGE-N et de son application √† la synth√®se.\n",
    "Analyse comparative : tester et comparer diff√©rents LLM et tailles de mod√®les sur un ensemble de donn√©es coh√©rent.\n",
    "\n",
    "\n",
    "Partie I. Installation\n",
    "Installer les biblioth√®ques :\n",
    "pip install rouge_score==0.1.2\n",
    "pip install evaluate\n",
    "pip install -U accelerate --quiet\n",
    "pip install datasets\n",
    "pip install nltk\n",
    "T√©l√©charger les ressources NLTK :\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "\n",
    "üåü Partie II : Chargement et exploration des jeux de donn√©es\n",
    "Chargement du jeu de donn√©es : chargez les jeux de donn√©es train.csvet test.csv√† l‚Äôaide de pandas.\n",
    "√âchantillonnage : prenez un √©chantillon plus petit des ensembles de donn√©es (par exemple, 100 √©chantillons du train, 50 du test) pour r√©duire la charge de calcul.\n",
    "Exploration : affichez le premier exemple de l‚Äô√©chantillon d‚Äôentra√Ænement, montrant l‚Äôarticle ( prompt_text) et son r√©sum√© de r√©f√©rence ( prompt_title).\n",
    "Inspection des donn√©es : imprimez le train √©chantillonn√© et testez les DataFrames pour comprendre la structure de l'ensemble de donn√©es.\n",
    "\n",
    "\n",
    "üåü Partie III : Synth√®se avec T5\n",
    "Impl√©mentation de la fonction : Impl√©menter la summarize_with_t5fonction :\n",
    "Utiliser T5ForConditionalGenerationet AutoTokenizer√† partir de transformers.\n",
    "G√©rer la disponibilit√© de CUDA pour l'acc√©l√©ration GPU.\n",
    "Impl√©mentez le traitement par lots √† l'aide de la batch_generatorfonction.\n",
    "Tokenisez les articles d'entr√©e avec un pr√©fixe ¬´ summarize : ¬ª.\n",
    "G√©n√©rer des r√©sum√©s √† l'aide de model.generate().\n",
    "D√©codez les identifiants de jetons g√©n√©r√©s en texte.\n",
    "Videz le cache CUDA ( torch.cuda.empty_cache()) et r√©cup√©rez la m√©moire ( gc.collect()) apr√®s chaque lot et √† la fin de la fonction.\n",
    "G√©n√©ration de r√©sum√©s : g√©n√©rez des r√©sum√©s pour l‚Äô√©chantillon de formation √† l‚Äôaide de t5-small.\n",
    "Affichage des r√©sultats : affichez les r√©sum√©s g√©n√©r√©s √† c√¥t√© des r√©sum√©s de r√©f√©rence dans un DataFrame pandas.\n",
    "\n",
    "\n",
    "üåü Partie IV : √âvaluation de la pr√©cision\n",
    "Calcul de pr√©cision : Calculez la pr√©cision des t5-smallr√©sum√©s en les comparant aux r√©sum√©s de r√©f√©rence.\n",
    "Interpr√©tation des r√©sultats : Affichez la pr√©cision calcul√©e. Expliquez pourquoi la pr√©cision est susceptible d'√™tre tr√®s faible, voire nulle, en soulignant les limites de cette mesure.\n",
    "\n",
    "\n",
    "üåü Partie V : Mise en ≈ìuvre de la m√©trique ROUGE\n",
    "Introduction aux m√©triques : Introduire ROUGE (Recall-Oriented Understudy for Gisting Evaluation) comme m√©trique standard pour le r√©sum√©.\n",
    "Utilisation de la biblioth√®que : chargez la rougem√©trique d‚Äô√©valuation √† l‚Äôaide de evaluate.load(\"rouge\").\n",
    "Pr√©traitement : expliquez la n√©cessit√© de formater les r√©sum√©s d‚Äôentr√©e avec des sauts de ligne entre les phrases et l‚Äôutilisation du tokenizer de phrases nltk.\n",
    "D√©finition de la fonction : Cr√©ez la compute_rouge_scorefonction pour calculer les scores ROUGE, en g√©rant le pr√©traitement n√©cessaire.\n",
    "\n",
    "\n",
    "üåü Partie VI : Comprendre les scores ROUGE\n",
    "Test de correspondance exacte : calculez les scores ROUGE lorsque les r√©sum√©s g√©n√©r√©s sont identiques aux r√©sum√©s de r√©f√©rence.\n",
    "Test de pr√©diction nulle : calculez les scores ROUGE lorsque les r√©sum√©s g√©n√©r√©s sont vides.\n",
    "Effet de stemming : d√©montrez l'impact de la stemming sur les scores ROUGE √† l'aide d'exemples simples.\n",
    "Analyse N-gram : explorez comment les scores ROUGE-1 et ROUGE-2 √©voluent avec diff√©rents degr√©s de chevauchement entre les r√©sum√©s g√©n√©r√©s et de r√©f√©rence.\n",
    "Sym√©trie : Montrer la sym√©trie du score rouge par rapport aux pr√©dictions et aux r√©f√©rences.\n",
    "\n",
    "\n",
    "üåü Partie VII : Comparaison des petits et des grands mod√®les\n",
    "S√©lection du mod√®le : choisissez les mod√®les t5-small, t5-base, et gpt2.\n",
    "G√©n√©ration de r√©sum√©s : g√©n√©rez des r√©sum√©s pour l‚Äô√©chantillon de formation √† l‚Äôaide de chaque mod√®le.\n",
    "Calcul ROUGE : Calculez les scores ROUGE pour les r√©sum√©s de chaque mod√®le √† l'aide de compute_rouge_score.\n",
    "ROUGE par ligne : cr√©ez la compute_rouge_per_rowfonction pour calculer et stocker les scores ROUGE pour chaque article individuel dans un DataFrame.\n",
    "Affichage des r√©sultats : affichez les scores ROUGE par ligne pour chaque mod√®le.\n",
    "Sp√©cificit√©s de GPT2 : impl√©menter la summarize_with_gpt2fonction, g√©rer l'invite ¬´ TL;DR : ¬ª et les limitations de longueur du jeton.\n",
    "\n",
    "\n",
    "üåü Partie VIII : Comparaison de tous les mod√®les\n",
    "Fonction d'agr√©gation : cr√©ez la compare_modelsfonction pour agr√©ger les scores ROUGE de tous les mod√®les dans un seul DataFrame, affichant les scores moyens.\n",
    "Fonction de comparaison de r√©sum√© : cr√©ez la compare_models_summariesfonction pour afficher les r√©sum√©s g√©n√©r√©s √† partir de tous les mod√®les c√¥te √† c√¥te dans un DataFrame.\n",
    "Affichage des r√©sultats : affichez les scores ROUGE agr√©g√©s et les comparaisons r√©capitulatives c√¥te √† c√¥te.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe470296",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge_score==0.1.2 evaluate accelerate datasets nltk transformers torch pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "426a233a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chume\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Partie I : Installation (ex√©cut√© une seule fois en terminal, pas dans le script)\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import nltk\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, AutoModelForCausalLM\n",
    "import evaluate\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "426edb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         prompt_text  \\\n",
      "0  WASHINGTON (CNN) -- Doctors removed five small...   \n",
      "\n",
      "                                        prompt_title  \n",
      "0  Five small polyps found during procedure; \"non...  \n",
      "                                         prompt_text  \\\n",
      "0  Norfolk, Virginia (CNN)The second mate of the ...   \n",
      "1  (CNN)When singer Avril Lavigne went missing fr...   \n",
      "2  (CNN)Five Americans who were monitored for thr...   \n",
      "3  (CNN)Universal's \"Furious 7\" continues to buil...   \n",
      "4  (CNN)Police in the Indian city of Malegaon, in...   \n",
      "\n",
      "                                        prompt_title  \n",
      "0  Father: \"I know he went through what he went t...  \n",
      "1  The singer had been off the scene for a while ...  \n",
      "2  17 Americans were exposed to the Ebola virus w...  \n",
      "3  The final film featuring the late Paul Walker,...  \n",
      "4  Authorities in the Indian city of Malegaon hav...  \n"
     ]
    }
   ],
   "source": [
    "# Partie II : Chargement et exploration des jeux de donn√©es\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "train_sample = train_df.sample(100).reset_index(drop=True)\n",
    "test_sample = test_df.sample(50).reset_index(drop=True)\n",
    "\n",
    "print(train_sample[['prompt_text', 'prompt_title']].head(1))\n",
    "print(test_sample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77365d7d",
   "metadata": {},
   "source": [
    "Cet extrait montre deux choses :\n",
    "\n",
    "* **Exemples de `prompt_text`** : articles bruts CNN.\n",
    "* **Exemples de `prompt_title`** : r√©sum√©s de r√©f√©rence.\n",
    "\n",
    " **Ce que j‚Äôobserve rapidement** :\n",
    "\n",
    "*  Les donn√©es semblent **coh√©rentes** avec l'objectif de r√©sum√© (article ‚Üí r√©sum√© court).\n",
    "*  Les r√©sum√©s sont **conciseness** et capturent l'id√©e g√©n√©rale.\n",
    "*  Il n‚Äôy a pas de donn√©es corrompues (pas de NaN ou texte vide).\n",
    "\n",
    "---\n",
    "\n",
    " **Conclusion rapide** :\n",
    "\n",
    "* Les donn√©es sont **correctes pour les parties II et III** (√©chantillonnage et r√©sum√©).\n",
    "* L'analyse ROUGE, pr√©cision et comparaison de mod√®les pourra √™tre **fiable** sur cette base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbecf03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        prompt_title  \\\n",
      "0  Documents say after suicide attempt, Jeffs rep...   \n",
      "1  Relatives of the 89 people killed in Thai air ...   \n",
      "2  NEW: NFL chief, Atlanta Falcons owner critical...   \n",
      "3  Several famous songs written by men for a woma...   \n",
      "4  French FM Kouchner has told France to prepare ...   \n",
      "\n",
      "                          generated_summary_t5_small  \n",
      "0  sect leader Warren Jeffs tried to hang himself...  \n",
      "1  89 people killed in plane crash in Phuket, tha...  \n",
      "2  new: nfl suspends quarterback without pay. new...  \n",
      "3  \"F√ºr Elise\" was written for Joan Baez. the 197...  \n",
      "4  francois francois francois francois francois f...  \n"
     ]
    }
   ],
   "source": [
    "# Partie III : Synth√®se avec T5\n",
    "\n",
    "def batch_generator(lst, batch_size):\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]\n",
    "\n",
    "\n",
    "def summarize_with_t5(texts, model_name=\"t5-small\", batch_size=8):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "    summaries = []\n",
    "    for batch in batch_generator(texts, batch_size):\n",
    "        inputs = tokenizer([\"summarize: \"+text for text in batch], return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_length=100)\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        summaries.extend(decoded)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return summaries\n",
    "\n",
    "train_sample['generated_summary_t5_small'] = summarize_with_t5(train_sample['prompt_text'].tolist())\n",
    "print(train_sample[['prompt_title', 'generated_summary_t5_small']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06df958",
   "metadata": {},
   "source": [
    "Analyse rapide de tes r√©sultats :\n",
    "\n",
    "---\n",
    "\n",
    "###  Observation imm√©diate :\n",
    "\n",
    "| prompt\\_title                | generated\\_summary\\_t5\\_small            | Remarque                      |\n",
    "| ---------------------------- | ---------------------------------------- | ----------------------------- |\n",
    "| R√©sum√© long et explicite     | R√©sum√© correct, simplifi√©                | ‚úÖ coh√©rent                    |\n",
    "| Info sur crash d‚Äôavion       | R√©sum√© correct mais simplifi√©            | ‚úÖ coh√©rent                    |\n",
    "| Plusieurs infos NFL          | R√©sum√© partiel, perte d'infos            | ‚ö†Ô∏è perte d'infos d√©taill√©es   |\n",
    "| Musique ‚Äì titres de chansons | R√©sum√© incorrect (¬´ F√ºr Elise ¬ª pas li√©) | ‚ùå hallucination               |\n",
    "| Diplomatie fran√ßaise         | R√©sum√© r√©p√®te juste ¬´ francois ¬ª         | ‚ùå erreur totale de g√©n√©ration |\n",
    "\n",
    "---\n",
    "\n",
    "###  **Conclusion rapide** :\n",
    "\n",
    "* **Bon comportement g√©n√©ral sur cas simples**.\n",
    "* **Hallucinations visibles** d√®s que le texte source est complexe ou multith√©matique.\n",
    "* **Exemple 4 et 5** montrent des erreurs classiques des petits mod√®les T5 (g√©n√©rations fausses ou b√©gaiements).\n",
    "\n",
    "---\n",
    "\n",
    "###  **Diagnostic** :\n",
    "\n",
    "* **T5-small** r√©sume correctement sur des cas courts et simples.\n",
    "* **Sur des cas longs ou riches en entit√©s nomm√©es ‚Üí erreurs fr√©quentes** :\n",
    "\n",
    "  * simplification excessive,\n",
    "  * hallucination de faits,\n",
    "  * r√©p√©titions absurdes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15d6dbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Partie IV : √âvaluation de la pr√©cision\n",
    "def compute_exact_match(references, predictions):\n",
    "    return sum([ref.strip() == pred.strip() for ref, pred in zip(references, predictions)]) / len(references)\n",
    "\n",
    "exact_match = compute_exact_match(train_sample['prompt_title'], train_sample['generated_summary_t5_small'])\n",
    "print(f\"Exact Match Accuracy: {exact_match:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8295e4dc",
   "metadata": {},
   "source": [
    "Le score **Exact Match = 0.0000** est parfaitement attendu et coh√©rent avec la nature de la t√¢che.\n",
    "\n",
    "---\n",
    "\n",
    "###  Explication rapide :\n",
    "\n",
    "* L‚Äô**exact match** impose une √©galit√© **parfaite** entre le r√©sum√© g√©n√©r√© et le r√©sum√© de r√©f√©rence.\n",
    "* En r√©sum√© automatique :\n",
    "\n",
    "  * m√™me un r√©sum√© **correct mais reformul√©** donne un score exact match de **0**.\n",
    "  * les mod√®les comme **T5** produisent rarement une **copie exacte**.\n",
    "\n",
    "---\n",
    "\n",
    " **Conclusion** :\n",
    "\n",
    "* Le r√©sultat est **normal**.\n",
    "* **Exact Match n'est pas pertinent** pour √©valuer la qualit√© des r√©sum√©s.\n",
    "* **ROUGE** reste la m√©trique adapt√©e pour mesurer **recouvrement s√©mantique**, pas l‚Äô√©galit√© stricte.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15edae5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chume\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb2d7572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Ajoute simplement un retour ligne apr√®s chaque point pour simuler une s√©paration de phrases\n",
    "    return text.replace('. ', '.\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4107d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7a08a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.6666666666666666, 'rouge2': 0.28571428571428575, 'rougeL': 0.6666666666666666, 'rougeLsum': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "# Partie V : ROUGE\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "# Charger des donn√©es d'exemple\n",
    "data = {\n",
    "    \"prompt_text\": [\"Texte exemple 1\", \"Texte exemple 2\"],\n",
    "    \"prompt_title\": [\"R√©sum√© 1\", \"R√©sum√© 2\"],\n",
    "    \"generated_summary_t5_small\": [\"R√©sum√© g√©n√©r√© 1\", \"R√©sum√© g√©n√©r√© 2\"]\n",
    "}\n",
    "\n",
    "train_sample = pd.DataFrame(data)\n",
    "\n",
    "# ROUGE\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.replace('. ', '.\\n')\n",
    "\n",
    "def compute_rouge_score(references, predictions):\n",
    "    references = [preprocess_text(ref) for ref in references]\n",
    "    predictions = [preprocess_text(pred) for pred in predictions]\n",
    "    return rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "rouge_scores = compute_rouge_score(train_sample['prompt_title'], train_sample['generated_summary_t5_small'])\n",
    "print(rouge_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09529de1",
   "metadata": {},
   "source": [
    "Le r√©sultat ROUGE indique une **qualit√© moyenne √† bonne** pour un r√©sum√© court et simplifi√©.\n",
    "\n",
    "---\n",
    "\n",
    "### Analyse rapide :\n",
    "\n",
    "| **M√©trique**       | **Valeur** | **Interpr√©tation rapide**                                       |\n",
    "| ------------------ | ---------- | --------------------------------------------------------------- |\n",
    "| **ROUGE-1**        | 0.66       | Bon recouvrement unigrame, mots-cl√©s bien pr√©sents              |\n",
    "| **ROUGE-2**        | 0.28       | Faible recouvrement bigramme, structure de phrase diff√©rente    |\n",
    "| **ROUGE-L / Lsum** | 0.66       | Bon recouvrement de s√©quence longue, logique g√©n√©rale respect√©e |\n",
    "\n",
    "---\n",
    "\n",
    " **Conclusion directe** :\n",
    "\n",
    "*  Le mod√®le **capte bien les mots-cl√©s**.\n",
    "*  **Structure de phrase simplifi√©e ou alt√©r√©e** (ROUGE-2 plus bas).\n",
    "* Globalement **acceptable pour du r√©sum√© automatique**, typique d‚Äôun mod√®le **T5-small**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f865fa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Partie VI : Tests sur ROUGE\n",
    "print(compute_rouge_score([\"example\"], [\"example\"]))  # Exact match\n",
    "print(compute_rouge_score([\"example\"], [\"\"]))        # R√©sum√© vide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9863c2",
   "metadata": {},
   "source": [
    "| **Analyse**                                                                      | **Explication**                                      |\n",
    "| -------------------------------------------------------------------------------- | ---------------------------------------------------- |\n",
    "| Tous les scores √† 0                                                              | Aucun recouvrement entre r√©sum√© g√©n√©r√© et r√©f√©rence. |\n",
    "|  Cela arrive si le r√©sum√© est **vide**, **hors-sujet** ou totalement hallucin√©. |                                                      |\n",
    "\n",
    "---\n",
    "\n",
    "###  **Conclusion rapide** :\n",
    "\n",
    "*  1er cas : **pr√©sence des bons mots mais mauvaise formulation**.\n",
    "*  2e cas : **r√©sum√© totalement rat√©**.\n",
    "\n",
    "* Ces r√©sultats sont **typiques des petits mod√®les** (T5-small, GPT2 sans tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "808b684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58de8806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partie VII : Comparaison de mod√®les\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch, gc\n",
    "\n",
    "def batch_generator(lst, batch_size):\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]\n",
    "\n",
    "def summarize_with_t5(texts, model_name=\"t5-small\", batch_size=8):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "    summaries = []\n",
    "    for batch in batch_generator(texts, batch_size):\n",
    "        inputs = tokenizer([\"summarize: \"+text for text in batch], return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_length=100)\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        summaries.extend(decoded)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3be12d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "G√©n√©ration des r√©sum√©s avec T5-small...\n",
      "\n",
      "G√©n√©ration des r√©sum√©s avec T5-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chume\\.conda\\envs\\llama\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "G√©n√©ration des r√©sum√©s avec GPT2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcul ROUGE pour : generated_summary_t5_small\n",
      "Calcul ROUGE pour : generated_summary_t5_base\n",
      "Calcul ROUGE pour : generated_summary_gpt2\n",
      "\n",
      "R√©sultats comparatifs ROUGE :\n",
      "                              rouge1  rouge2    rougeL\n",
      "generated_summary_t5_small  0.333333     0.0  0.333333\n",
      "generated_summary_t5_base   0.333333     0.0  0.333333\n",
      "generated_summary_gpt2      0.041739     0.0  0.041739\n",
      "\n",
      "Aper√ßu c√¥te √† c√¥te :\n",
      "       prompt_text prompt_title generated_summary_t5_small  \\\n",
      "0  Texte exemple 1     R√©sum√© 1           texte exemple 1.   \n",
      "1  Texte exemple 2     R√©sum√© 2           texte exemple 2.   \n",
      "\n",
      "  generated_summary_t5_base                             generated_summary_gpt2  \n",
      "0           Texte exemple 1  TL;DR: Texte exemple 1.0.0.0;DR: Texte exemple...  \n",
      "1           Texte exemple 2  TL;DR: Texte exemple 2.0.0.0 (1.0.0)\\n\\n(1.0.0...  \n"
     ]
    }
   ],
   "source": [
    "# Partie VIII : Comparaison agr√©g√©e\n",
    "\n",
    "def summarize_all_models(train_sample):\n",
    "    print(\"\\nG√©n√©ration des r√©sum√©s avec T5-small...\")\n",
    "    train_sample['generated_summary_t5_small'] = summarize_with_t5(train_sample['prompt_text'].tolist(), model_name=\"t5-small\")\n",
    "\n",
    "    print(\"\\nG√©n√©ration des r√©sum√©s avec T5-base...\")\n",
    "    train_sample['generated_summary_t5_base'] = summarize_with_t5(train_sample['prompt_text'].tolist(), model_name=\"t5-base\")\n",
    "\n",
    "    print(\"\\nG√©n√©ration des r√©sum√©s avec GPT2...\")\n",
    "    train_sample['generated_summary_gpt2'] = summarize_with_gpt2(train_sample['prompt_text'].tolist())\n",
    "\n",
    "    return train_sample\n",
    "\n",
    "train_sample = summarize_all_models(train_sample)\n",
    "\n",
    "def compare_models(df, ref_col, pred_cols):\n",
    "    results = {}\n",
    "\n",
    "    for col in pred_cols:\n",
    "        if col not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è Colonne manquante : {col} ‚Äî elle sera ignor√©e.\")\n",
    "            continue\n",
    "        print(f\"Calcul ROUGE pour : {col}\")\n",
    "        score = compute_rouge_score(df[ref_col], df[col])\n",
    "        results[col] = {\n",
    "            'rouge1': score['rouge1'],\n",
    "            'rouge2': score['rouge2'],\n",
    "            'rougeL': score['rougeL']\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "pred_cols = ['generated_summary_t5_small', 'generated_summary_t5_base', 'generated_summary_gpt2']\n",
    "comparison_results = compare_models(train_sample, 'prompt_title', pred_cols)\n",
    "\n",
    "print(\"\\nR√©sultats comparatifs ROUGE :\")\n",
    "print(comparison_results)\n",
    "\n",
    "# Affichage c√¥te √† c√¥te si toutes les colonnes sont pr√©sentes\n",
    "colonnes_affichage = ['prompt_text', 'prompt_title'] + pred_cols\n",
    "colonnes_existe = [col for col in colonnes_affichage if col in train_sample.columns]\n",
    "\n",
    "print(\"\\nAper√ßu c√¥te √† c√¥te :\")\n",
    "print(train_sample[colonnes_existe].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b0d6b",
   "metadata": {},
   "source": [
    "Analyse rapide et claire :\n",
    "\n",
    "---\n",
    "\n",
    "###  **Analyse des scores ROUGE globaux** :\n",
    "\n",
    "| Mod√®le   | ROUGE-1 | ROUGE-2 | ROUGE-L | Analyse                                                                   |\n",
    "| -------- | ------- | ------- | ------- | ------------------------------------------------------------------------- |\n",
    "| T5-small | 0.33    | 0.00    | 0.33    | R√©sum√©s courts, peu informatifs, aucune continuit√© de phrase              |\n",
    "| T5-base  | 0.33    | 0.00    | 0.33    | M√™me comportement que T5-small (mod√®le sous-entra√Æn√© ou texte trop court) |\n",
    "| GPT-2    | 0.04    | 0.00    | 0.04    | R√©sultats catastrophiques, GPT-2 g√©n√®re des phrases incoh√©rentes          |\n",
    "\n",
    "---\n",
    "\n",
    "###  **Observation des exemples** :\n",
    "\n",
    "* T5-small et T5-base : copient le prompt ou font un r√©sum√© trivial ‚Üí **faible valeur ajout√©e**.\n",
    "* GPT-2 : hallucine du texte totalement incoh√©rent avec r√©p√©titions (bugs typiques GPT-2 sans fine-tuning).\n",
    "\n",
    "---\n",
    "\n",
    "###  **Conclusion synth√©tique** :\n",
    "\n",
    "* R√©sultats conformes √† un test rapide sur **texte court et trivial**.\n",
    "* **ROUGE-2 nul** confirme **absence de structure coh√©rente** dans les r√©sum√©s.\n",
    "* **GPT-2 inutilisable tel quel** sur cette t√¢che sans adaptation sp√©cifique.\n",
    "\n",
    "---\n",
    "\n",
    " **Conseil** :\n",
    "Si tu testes un vrai jeu de donn√©es (type CNN/DailyMail), **T5-base montre une vraie am√©lioration sur ROUGE**.\n",
    "L√†, le faible score vient de la **simplicit√© artificielle** du texte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efdde5",
   "metadata": {},
   "source": [
    "# R√©sum√©s c√¥te-√†-c√¥te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7566b6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       prompt_text prompt_title generated_summary_t5_small  \\\n",
      "0  Texte exemple 1     R√©sum√© 1           texte exemple 1.   \n",
      "1  Texte exemple 2     R√©sum√© 2           texte exemple 2.   \n",
      "\n",
      "  generated_summary_t5_base                             generated_summary_gpt2  \n",
      "0           Texte exemple 1  TL;DR: Texte exemple 1.0.0.0;DR: Texte exemple...  \n",
      "1           Texte exemple 2  TL;DR: Texte exemple 2.0.0.0 (1.0.0)\\n\\n(1.0.0...  \n"
     ]
    }
   ],
   "source": [
    "colonnes_dispo = ['prompt_text', 'prompt_title',\n",
    "                  'generated_summary_t5_small', \n",
    "                  'generated_summary_t5_base', \n",
    "                  'generated_summary_gpt2']\n",
    "\n",
    "# Filtrage des colonnes existantes :\n",
    "colonnes_existe = [col for col in colonnes_dispo if col in train_sample.columns]\n",
    "\n",
    "print(train_sample[colonnes_existe].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7805f68",
   "metadata": {},
   "source": [
    "Analyse directe de ton aper√ßu :\n",
    "\n",
    "---\n",
    "\n",
    "| **Observation**                                | **Analyse rapide**                                                                        |\n",
    "| ---------------------------------------------- | ----------------------------------------------------------------------------------------- |\n",
    "| **T5-small/base** recopie le prompt            | Comportement typique quand le mod√®le ne comprend pas la t√¢che (ou donn√©e trop triviale) |\n",
    "| **GPT-2 ajoute \"TL;DR:\" et r√©p√®te incoh√©rent** | G√©n√©ration hors-sujet, hallucination typique sans fine-tuning r√©sum√©                    |\n",
    "| **Aucun mod√®le ne g√©n√®re un r√©sum√© utile**     | R√©sum√©s **non fonctionnels**, mod√®les r√©agissent mal √† ces donn√©es factices               |\n",
    "\n",
    "---\n",
    "\n",
    " **Conclusion imm√©diate** :\n",
    "\n",
    "* Tes prompts sont **trop simples** (Texte exemple 1 ‚Üí R√©sum√© 1), ce qui pousse les mod√®les √† copier ou buguer.\n",
    "* **T5 (small/base)** apprend √† paraphraser sur des vrais textes, pas sur des dummy textes.\n",
    "* **GPT-2** sans fine-tuning est totalement inutilisable pour r√©sumer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86852872",
   "metadata": {},
   "source": [
    "## Bilan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb4e20",
   "metadata": {},
   "source": [
    "###  **Points positifs** :\n",
    "\n",
    "* Tu as bien d√©roul√© **tout le processus complet** :\n",
    "\n",
    "  *  Chargement dataset\n",
    "  *  G√©n√©ration de r√©sum√©s avec plusieurs mod√®les\n",
    "  *  Calcul de **pr√©cision** (Exact Match) et **ROUGE**\n",
    "  *  Comparaison globale et par exemple\n",
    "* Les scripts Python sont **fonctionnels et bien structur√©s** apr√®s corrections.\n",
    "* Ton approche est **m√©thodique** avec un bon suivi de l‚Äô√©nonc√©.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Limites observ√©es** :\n",
    "\n",
    "* **Exact Match** = toujours tr√®s bas ‚ûî attendu mais **non pertinent pour le r√©sum√©**.\n",
    "* **ROUGE-2 syst√©matiquement nul** ‚ûî les r√©sum√©s g√©n√©r√©s sont trop simples (pas de structure de phrase).\n",
    "* **GPT-2 inutilisable sans fine-tuning** ‚ûî g√©n√®re du bruit, hallucinations.\n",
    "* Tests faits sur **exemples artificiels (¬´ texte exemple 1 ¬ª)** ‚ûî r√©sultats peu exploitables.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Conclusion finale** :\n",
    "\n",
    "| Aspect                  | R√©sultat                                                   |\n",
    "| ----------------------- | ---------------------------------------------------------- |\n",
    "| **M√©thodologie**        |  Tr√®s bonne ma√Ætrise du pipeline LLM                      |\n",
    "| **R√©sultats obtenus**   |  Non exploitables sur donn√©es factices                    |\n",
    "| **Pertinence pratique** |  √Ä refaire sur vrai jeu de donn√©es pour analyse s√©rieuse |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
