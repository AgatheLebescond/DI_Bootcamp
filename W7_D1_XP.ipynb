{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b705fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "XP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5db535",
   "metadata": {},
   "source": [
    "Exercise 1: Traditional vs. Modern NLP: A Comparative Analysis\n",
    "1. Create a table comparing and contrasting the traditional and modern NLP paradigms. Include the following aspects:\n",
    "\n",
    "Feature Engineering (manual vs. automatic)\n",
    "Word Representations (static vs. contextual)\n",
    "Model Architectures (shallow vs. deep)\n",
    "Training Methodology (task-specific vs. pre-training/fine-tuning)\n",
    "Key Examples of Models (e.g., Naïve Bayes, BERT)\n",
    "Advantages and Disadvantages of each paradigm.\n",
    "2. Discuss how the evolution from traditional to modern NLP has impacted the scalability and efficiency of NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01cdf59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Aspect",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "NLP Traditionnel",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "NLP Moderne",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "09c63086-d13e-40ff-ae05-0211ff2f6e36",
       "rows": [
        [
         "0",
         "Feature Engineering",
         "Manuel",
         "Automatique"
        ],
        [
         "1",
         "Représentations des Mots",
         "Statiques (ex: Word2Vec)",
         "Contextuelles (ex: BERT)"
        ],
        [
         "2",
         "Architectures des Modèles",
         "Peu profondes (ex: Naïve Bayes, SVM)",
         "Profondes (ex: Réseaux de neurones, Transformers)"
        ],
        [
         "3",
         "Méthodologie d'Entraînement",
         "Spécifique à la tâche",
         "Pré-entraînement et fine-tuning"
        ],
        [
         "4",
         "Exemples de Modèles",
         "Naïve Bayes, SVM, HMM",
         "BERT, GPT, RoBERTa"
        ],
        [
         "5",
         "Avantages",
         "Simplicité, interprétabilité, moins de données nécessaires",
         "Meilleure performance, capacité à capturer des dépendances complexes"
        ],
        [
         "6",
         "Inconvénients",
         "Performances limitées, incapacité à capturer le contexte",
         "Nécessite beaucoup de données et de ressources computationnelles"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aspect</th>\n",
       "      <th>NLP Traditionnel</th>\n",
       "      <th>NLP Moderne</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature Engineering</td>\n",
       "      <td>Manuel</td>\n",
       "      <td>Automatique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Représentations des Mots</td>\n",
       "      <td>Statiques (ex: Word2Vec)</td>\n",
       "      <td>Contextuelles (ex: BERT)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architectures des Modèles</td>\n",
       "      <td>Peu profondes (ex: Naïve Bayes, SVM)</td>\n",
       "      <td>Profondes (ex: Réseaux de neurones, Transformers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Méthodologie d'Entraînement</td>\n",
       "      <td>Spécifique à la tâche</td>\n",
       "      <td>Pré-entraînement et fine-tuning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Exemples de Modèles</td>\n",
       "      <td>Naïve Bayes, SVM, HMM</td>\n",
       "      <td>BERT, GPT, RoBERTa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Avantages</td>\n",
       "      <td>Simplicité, interprétabilité, moins de données...</td>\n",
       "      <td>Meilleure performance, capacité à capturer des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Inconvénients</td>\n",
       "      <td>Performances limitées, incapacité à capturer l...</td>\n",
       "      <td>Nécessite beaucoup de données et de ressources...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Aspect  \\\n",
       "0          Feature Engineering   \n",
       "1     Représentations des Mots   \n",
       "2    Architectures des Modèles   \n",
       "3  Méthodologie d'Entraînement   \n",
       "4          Exemples de Modèles   \n",
       "5                    Avantages   \n",
       "6                Inconvénients   \n",
       "\n",
       "                                    NLP Traditionnel  \\\n",
       "0                                             Manuel   \n",
       "1                           Statiques (ex: Word2Vec)   \n",
       "2               Peu profondes (ex: Naïve Bayes, SVM)   \n",
       "3                              Spécifique à la tâche   \n",
       "4                              Naïve Bayes, SVM, HMM   \n",
       "5  Simplicité, interprétabilité, moins de données...   \n",
       "6  Performances limitées, incapacité à capturer l...   \n",
       "\n",
       "                                         NLP Moderne  \n",
       "0                                        Automatique  \n",
       "1                           Contextuelles (ex: BERT)  \n",
       "2  Profondes (ex: Réseaux de neurones, Transformers)  \n",
       "3                    Pré-entraînement et fine-tuning  \n",
       "4                                 BERT, GPT, RoBERTa  \n",
       "5  Meilleure performance, capacité à capturer des...  \n",
       "6  Nécessite beaucoup de données et de ressources...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Créer les données pour le DataFrame\n",
    "data = {\n",
    "    \"Aspect\": [\n",
    "        \"Feature Engineering\",\n",
    "        \"Représentations des Mots\",\n",
    "        \"Architectures des Modèles\",\n",
    "        \"Méthodologie d'Entraînement\",\n",
    "        \"Exemples de Modèles\",\n",
    "        \"Avantages\",\n",
    "        \"Inconvénients\"\n",
    "    ],\n",
    "    \"NLP Traditionnel\": [\n",
    "        \"Manuel\",\n",
    "        \"Statiques (ex: Word2Vec)\",\n",
    "        \"Peu profondes (ex: Naïve Bayes, SVM)\",\n",
    "        \"Spécifique à la tâche\",\n",
    "        \"Naïve Bayes, SVM, HMM\",\n",
    "        \"Simplicité, interprétabilité, moins de données nécessaires\",\n",
    "        \"Performances limitées, incapacité à capturer le contexte\"\n",
    "    ],\n",
    "    \"NLP Moderne\": [\n",
    "        \"Automatique\",\n",
    "        \"Contextuelles (ex: BERT)\",\n",
    "        \"Profondes (ex: Réseaux de neurones, Transformers)\",\n",
    "        \"Pré-entraînement et fine-tuning\",\n",
    "        \"BERT, GPT, RoBERTa\",\n",
    "        \"Meilleure performance, capacité à capturer des dépendances complexes\",\n",
    "        \"Nécessite beaucoup de données et de ressources computationnelles\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Créer le DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e132e2",
   "metadata": {},
   "source": [
    "L'évolution du NLP traditionnel vers le NLP moderne a eu un impact significatif sur la scalabilité et l'efficacité des applications NLP :\n",
    "\n",
    "Scalabilité :\n",
    "\n",
    "NLP Traditionnel : Les modèles traditionnels étaient souvent limités par leur capacité à traiter de grandes quantités de données et à capturer des dépendances complexes. Ils nécessitaient une ingénierie manuelle des caractéristiques, ce qui pouvait être fastidieux et peu scalable.\n",
    "NLP Moderne : Les modèles modernes, grâce à l'apprentissage profond et aux architectures comme les Transformers, peuvent traiter de vastes ensembles de données et capturer des dépendances à long terme. Le pré-entraînement sur de grands corpus de texte permet de créer des modèles génériques qui peuvent ensuite être fine-tunés pour des tâches spécifiques, améliorant ainsi la scalabilité.\n",
    "Efficacité :\n",
    "\n",
    "NLP Traditionnel : Les modèles traditionnels étaient souvent moins efficaces en termes de performance sur des tâches complexes comme la compréhension du langage naturel ou la génération de texte. Ils étaient également moins capables de généraliser à de nouvelles tâches sans un réentraînement complet.\n",
    "NLP Moderne : Les modèles modernes, grâce à des techniques comme l'attention et les représentations contextuelles, offrent une meilleure performance sur une large gamme de tâches NLP. Ils sont également plus efficaces en termes de temps et de ressources nécessaires pour l'entraînement et l'inférence, grâce à des avancées comme le transfert learning et l'optimisation des architectures de modèles.\n",
    "En résumé, le passage du NLP traditionnel au NLP moderne a permis de surmonter de nombreuses limitations des approches traditionnelles, ouvrant la voie à des applications NLP plus puissantes et plus efficaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bfa1fb",
   "metadata": {},
   "source": [
    "EXERCICE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c69e46",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "Différences architecturales :\n",
    "\n",
    "Bidirectionnel : BERT utilise un mécanisme bidirectionnel pour lire le texte, ce qui signifie qu'il prend en compte le contexte des deux côtés (gauche et droite) d'un mot lors de l'apprentissage des représentations.\n",
    "Modélisation de langage masqué : BERT est pré-entraîné en utilisant une tâche de modélisation de langage masqué, où certains mots dans une phrase sont masqués et le modèle doit les prédire.\n",
    "Application réelle :\n",
    "\n",
    "Compréhension de texte et réponse aux questions : BERT excelle dans les tâches qui nécessitent une compréhension approfondie du contexte, comme les systèmes de réponse aux questions.\n",
    "Pourquoi BERT est adapté :\n",
    "\n",
    "La capacité de BERT à comprendre le contexte des deux côtés d'un mot permet une meilleure compréhension des nuances du langage, ce qui est crucial pour répondre précisément aux questions basées sur un texte donné.\n",
    "GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "Différences architecturales :\n",
    "\n",
    "Unidirectionnel : GPT utilise un mécanisme unidirectionnel, ce qui signifie qu'il lit le texte dans une seule direction (de gauche à droite).\n",
    "Modélisation de langage causale : GPT est entraîné pour prédire le prochain mot dans une séquence, ce qui le rend particulièrement bon pour générer du texte cohérent.\n",
    "Application réelle :\n",
    "\n",
    "Génération de texte : GPT est excellent pour les tâches de génération de texte, comme la rédaction automatique d'articles, la création de contenu, ou même la génération de code.\n",
    "Pourquoi GPT est adapté :\n",
    "\n",
    "La capacité de GPT à générer du texte de manière cohérente et contextuellement appropriée en fait un outil puissant pour toute application nécessitant la création de nouveau contenu textuel.\n",
    "T5 (Text-To-Text Transfer Transformer)\n",
    "\n",
    "Différences architecturales :\n",
    "\n",
    "Approche text-to-text : T5 traite toutes les tâches NLP comme un problème de conversion de texte en texte. Cela signifie qu'il prend du texte en entrée et génère du texte en sortie, quelle que soit la tâche.\n",
    "Flexibilité : T5 est conçu pour être flexible et peut être adapté à une large gamme de tâches NLP en reformulant chaque tâche comme un problème de génération de texte.\n",
    "Application réelle :\n",
    "\n",
    "Traduction automatique et résumé de texte : T5 est particulièrement efficace pour les tâches qui impliquent la transformation d'un texte en un autre, comme la traduction ou le résumé.\n",
    "Pourquoi T5 est adapté :\n",
    "\n",
    "L'approche text-to-text de T5 permet de traiter diverses tâches NLP de manière uniforme, ce qui simplifie le processus de fine-tuning et permet une grande flexibilité dans les applications. Cela le rend particulièrement adapté aux tâches de transformation de texte comme la traduction et le résumé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f7bd5",
   "metadata": {},
   "source": [
    "EXERCICE 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eea088",
   "metadata": {},
   "source": [
    "Cinq avantages clés des modèles pré-entraînés\n",
    "\n",
    "Amélioration de la généralisation :\n",
    "\n",
    "Les modèles pré-entraînés sont capables de capturer une large gamme de motifs linguistiques et de connaissances à partir de grands volumes de données. Cela leur permet de mieux généraliser à de nouvelles tâches ou à des données qu'ils n'ont pas vues pendant l'entraînement, améliorant ainsi leur performance sur des tâches variées.\n",
    "Réduction du besoin de données étiquetées :\n",
    "\n",
    "L'entraînement de modèles à partir de zéro nécessite souvent de grandes quantités de données étiquetées, qui peuvent être coûteuses et longues à obtenir. Les modèles pré-entraînés, ayant déjà appris des représentations utiles des données, nécessitent moins de données étiquetées pour le fine-tuning, ce qui réduit le coût et l'effort nécessaires.\n",
    "Fine-tuning plus rapide :\n",
    "\n",
    "Comme les modèles pré-entraînés ont déjà appris des caractéristiques générales des données, le processus de fine-tuning pour des tâches spécifiques est généralement plus rapide que l'entraînement d'un modèle à partir de zéro. Cela permet des itérations plus rapides et un développement plus efficace des applications.\n",
    "Apprentissage par transfert :\n",
    "\n",
    "Les modèles pré-entraînés permettent l'apprentissage par transfert, où les connaissances acquises à partir d'une tâche peuvent être transférées à une autre. Cela est particulièrement utile dans les domaines où les données sont rares ou difficiles à obtenir, car il permet de tirer parti des connaissances préexistantes.\n",
    "Robustesse :\n",
    "\n",
    "Les modèles pré-entraînés sont souvent plus robustes aux variations et aux bruits dans les données. Grâce à leur exposition à une grande diversité de données pendant le pré-entraînement, ils sont mieux équipés pour gérer les incertitudes et les variations dans les nouvelles données.\n",
    "Préoccupations éthiques associées au pré-entraînement des LLMs\n",
    "\n",
    "Biais :\n",
    "\n",
    "Les grands ensembles de données utilisés pour le pré-entraînement peuvent contenir des biais existants, reflétant des stéréotypes ou des préjugés présents dans la société. Les modèles entraînés sur ces données peuvent alors perpétuer ou amplifier ces biais, conduisant à des résultats injustes ou discriminatoires.\n",
    "Désinformation :\n",
    "\n",
    "Les modèles pré-entraînés peuvent être utilisés pour générer de la désinformation ou des contenus trompeurs de manière convaincante. Cela pose un risque significatif pour la diffusion de fausses informations et la manipulation de l'opinion publique.\n",
    "Mauvaise utilisation :\n",
    "\n",
    "Les modèles puissants peuvent être détournés à des fins malveillantes, comme la création de deepfakes, le harcèlement en ligne, ou l'automatisation de la production de spam et de contenus nuisibles.\n",
    "Stratégies de mitigation potentielles\n",
    "\n",
    "Audit et évaluation des biais :\n",
    "\n",
    "Mettre en place des processus rigoureux pour auditer et évaluer les biais dans les ensembles de données et les modèles. Utiliser des techniques pour détecter et corriger les biais, et s'assurer que les modèles sont testés sur des groupes diversifiés et représentatifs.\n",
    "Transparence et responsabilité :\n",
    "\n",
    "Encourager la transparence dans le développement et le déploiement des modèles, en documentant les sources de données, les méthodes d'entraînement, et les limitations des modèles. Établir des mécanismes de responsabilité pour les résultats des modèles et leur impact.\n",
    "Éducation et sensibilisation :\n",
    "\n",
    "Sensibiliser les développeurs, les utilisateurs et le public aux risques et aux limitations des modèles pré-entraînés. Promouvoir une utilisation éthique et responsable des technologies de l'IA.\n",
    "Régulation et collaboration :\n",
    "\n",
    "Travailler avec les régulateurs et les parties prenantes pour établir des normes et des régulations qui encadrent le développement et l'utilisation des modèles pré-entraînés. Encourager la collaboration entre les chercheurs, les industriels et les décideurs pour aborder les défis éthiques de manière collective.\n",
    "En mettant en œuvre ces stratégies, il est possible de maximiser les bénéfices des modèles pré-entraînés tout en minimisant les risques et les impacts négatifs potentiels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db494b",
   "metadata": {},
   "source": [
    "EXERCICE 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae07eb01",
   "metadata": {},
   "source": [
    "Mécanismes d'Attention dans les Transformers\n",
    "\n",
    "Self-Attention\n",
    "\n",
    "Le mécanisme de self-attention permet à un modèle de peser l'importance des différents mots dans une séquence par rapport à un mot donné. Voici comment cela fonctionne :\n",
    "\n",
    "Création des Vecteurs : Pour chaque mot dans la séquence d'entrée, trois vecteurs sont créés : une requête (Query), une clé (Key), et une valeur (Value). Ces vecteurs sont obtenus en multipliant les embeddings des mots par des matrices apprises.\n",
    "\n",
    "Calcul des Scores d'Attention : Pour chaque mot, un score d'attention est calculé entre sa requête et les clés de tous les autres mots de la séquence. Ce score détermine l'importance des autres mots par rapport au mot courant.\n",
    "\n",
    "Application de la Fonction Softmax : Les scores d'attention sont passés à travers une fonction softmax pour obtenir une distribution de probabilités.\n",
    "\n",
    "Pondération des Valeurs : Les valeurs des mots sont pondérées par les scores d'attention, et la somme de ces valeurs pondérées produit un nouveau vecteur pour chaque mot, qui capture le contexte de la séquence entière.\n",
    "\n",
    "Multi-Head Attention\n",
    "\n",
    "Le multi-head attention étend le mécanisme de self-attention en utilisant plusieurs ensembles de matrices Query, Key, et Value. Chaque ensemble est appelé une \"tête\" d'attention. Voici pourquoi c'est avantageux :\n",
    "\n",
    "Attention à Différents Sous-Espaces : Chaque tête peut apprendre à se concentrer sur différents types de relations ou d'informations dans les données.\n",
    "Capacité Accrue : En combinant les informations de plusieurs têtes, le modèle peut capturer une gamme plus large de dépendances et de relations.\n",
    "Exemple concret :\n",
    "\n",
    "Prenons la phrase : \"Le chat assis sur le tapis regarde l'oiseau.\"\n",
    "\n",
    "Une tête d'attention pourrait se concentrer sur les relations sujet-verbe, comme \"chat\" et \"regarde\".\n",
    "Une autre tête pourrait capturer les relations spatiales, comme \"assis\" et \"tapis\".\n",
    "Une troisième tête pourrait se concentrer sur les relations objet, comme \"chat\" et \"oiseau\".\n",
    "Objectifs de Pré-entraînement\n",
    "\n",
    "Masked Language Modeling (MLM) vs. Causal Language Modeling (CLM)\n",
    "\n",
    "MLM : Utilisé par des modèles comme BERT, où certains mots dans une phrase sont masqués et le modèle doit les prédire. Cela permet au modèle de comprendre le contexte bidirectionnel.\n",
    "\n",
    "CLM : Utilisé par des modèles comme GPT, où le modèle prédit le prochain mot dans une séquence. Cela est utile pour les tâches de génération de texte.\n",
    "\n",
    "Scénarios d'utilisation :\n",
    "\n",
    "MLM : Plus approprié pour des tâches nécessitant une compréhension approfondie du contexte, comme l'analyse de sentiments ou la réponse aux questions.\n",
    "CLM : Plus approprié pour des tâches de génération de texte, comme la rédaction automatique ou la création de dialogues.\n",
    "Next Sentence Prediction (NSP)\n",
    "\n",
    "Les premiers modèles BERT utilisaient le NSP pour apprendre les relations entre les phrases, mais les modèles modernes tendent à éviter cette tâche car elle s'est avérée moins efficace que prévu et peut introduire des artefacts indésirables dans les données.\n",
    "\n",
    "Sélection de Modèles Transformers\n",
    "\n",
    "Analyse de Sentiments\n",
    "\n",
    "Type de Modèle : Encoder-only (comme BERT).\n",
    "Justification : Les modèles Encoder-only sont excellents pour capturer le contexte bidirectionnel, ce qui est crucial pour comprendre le sentiment global d'une phrase.\n",
    "Chatbot\n",
    "\n",
    "Type de Modèle : Decoder-only (comme GPT).\n",
    "Justification : Les modèles Decoder-only sont optimisés pour la génération de texte, ce qui est idéal pour produire des réponses créatives et engageantes.\n",
    "Traduction Automatique\n",
    "\n",
    "Type de Modèle : Encoder-Decoder (comme le modèle original Transformer).\n",
    "Justification : Les modèles Encoder-Decoder peuvent capturer le contexte de la langue source avec l'encoder et générer une traduction précise avec le decoder.\n",
    "Codage Positionnel\n",
    "\n",
    "But du Codage Positionnel\n",
    "\n",
    "Le codage positionnel est utilisé pour donner aux modèles Transformers des informations sur la position relative ou absolue des mots dans une séquence. Comme les Transformers ne traitent pas les mots dans un ordre séquentiel (contrairement aux RNNs), le codage positionnel est crucial pour capturer l'ordre des mots.\n",
    "\n",
    "Exemple de Problème sans Codage Positionnel\n",
    "\n",
    "Sans codage positionnel, le modèle pourrait traiter \"Le chat a mangé la souris\" et \"La souris a mangé le chat\" de la même manière, car il ne saurait pas quel mot vient en premier. Cela conduirait à une mauvaise interprétation de la phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fe1bf",
   "metadata": {},
   "source": [
    "EXERCICE 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0df36a",
   "metadata": {},
   "source": [
    "Scénarios\n",
    "\n",
    "Scénario 1 : Analyse de sentiments en temps réel sur une application mobile avec des ressources limitées.\n",
    "\n",
    "Modèle recommandé : DistilBERT\n",
    "Justification : DistilBERT est une version plus petite et plus efficace de BERT, conçue pour être rapide et nécessiter moins de ressources computationnelles. Cela le rend idéal pour les applications mobiles où les ressources sont limitées.\n",
    "Scénario 2 : Recherche sur des documents juridiques nécessitant une haute précision.\n",
    "\n",
    "Modèle recommandé : RoBERTa\n",
    "Justification : RoBERTa améliore la performance de BERT en utilisant plus de données d'entraînement et des techniques d'optimisation avancées. Il est particulièrement adapté pour les tâches nécessitant une compréhension approfondie et précise du langage, comme l'analyse de documents juridiques.\n",
    "Scénario 3 : Support client mondial dans plusieurs langues.\n",
    "\n",
    "Modèle recommandé : XLM-RoBERTa\n",
    "Justification : XLM-RoBERTa est une version multilingue de RoBERTa, entraînée sur un large corpus de données dans plusieurs langues. Il est donc bien adapté pour les applications nécessitant une compréhension multilingue, comme le support client mondial.\n",
    "Scénario 4 : Pré-entraînement efficace et détection de remplacement de tokens.\n",
    "\n",
    "Modèle recommandé : ELECTRA\n",
    "Justification : ELECTRA utilise une méthode innovante de pré-entraînement appelée \"replaced token detection\", qui est plus efficace que les méthodes traditionnelles de modélisation de langage masqué. Cela le rend particulièrement adapté pour des tâches nécessitant une détection précise des tokens.\n",
    "Scénario 5 : Traitement efficace du langage naturel dans des environnements avec des ressources limitées.\n",
    "\n",
    "Modèle recommandé : ALBERT\n",
    "Justification : ALBERT utilise des techniques de partage de paramètres pour réduire la taille du modèle et améliorer l'efficacité de l'entraînement. Cela le rend adapté pour les environnements avec des ressources limitées.\n",
    "Tableau Comparatif des Variations de BERT\n",
    "\n",
    "Modèle\tDonnées d'Entraînement et Méthodes\tTaille du Modèle et Efficacité\tOptimisations et Innovations\tCas d'Utilisation Idéaux\n",
    "RoBERTa\tPlus de données et techniques d'optimisation avancées\tPlus grand et plus gourmand en ressources\tOptimisation des hyperparamètres et entraînement prolongé\tTâches nécessitant une haute précision et une compréhension approfondie du langage\n",
    "ALBERT\tPartage de paramètres et techniques de réduction de modèle\tPlus petit et plus efficace\tPartage de paramètres et factorisation des matrices\tEnvironnements avec des ressources limitées\n",
    "DistilBERT\tDistillation de connaissances à partir de BERT\tPlus petit et plus efficace\tDistillation de connaissances\tApplications mobiles et en temps réel\n",
    "ELECTRA\tReplaced Token Detection\tEfficace en termes de données d'entraînement\tMéthode de pré-entraînement innovante\tDétection de remplacement de tokens et tâches nécessitant un pré-entraînement efficace\n",
    "XLM-RoBERTa\tLarge corpus de données multilingues\tGrand et gourmand en ressources\tEntraînement multilingue\tApplications multilingues et support client mondial\n",
    "Ce tableau résume les principales caractéristiques et compromis de chaque variation de BERT, ainsi que les cas d'utilisation idéaux pour chacun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb6385a",
   "metadata": {},
   "source": [
    "EXERCICE 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b721a",
   "metadata": {},
   "source": [
    "Scénarios de Température Softmax\n",
    "\n",
    "Température Softmax réglée à 0.2\n",
    "\n",
    "Effet : Une température de 0.2 rend la distribution de probabilité plus \"pointue\". Cela signifie que les mots avec les probabilités les plus élevées sont encore plus favorisés.\n",
    "Résultat : Le modèle produira des sorties plus déterministes et répétitives. Les phrases générées seront très prévisibles et manqueront de diversité.\n",
    "Température Softmax réglée à 1.5\n",
    "\n",
    "Effet : Une température de 1.5 rend la distribution de probabilité plus \"plate\". Cela signifie que les mots avec des probabilités plus basses ont une chance plus élevée d'être sélectionnés.\n",
    "Résultat : Le modèle produira des sorties plus variées et créatives. Cependant, cela peut aussi conduire à des phrases moins cohérentes ou grammaticalement incorrectes.\n",
    "Température Softmax réglée à 1\n",
    "\n",
    "Effet : Une température de 1 maintient la distribution de probabilité inchangée par rapport à la sortie du modèle.\n",
    "Résultat : Le modèle génère des phrases avec un équilibre entre prévisibilité et diversité, reflétant fidèlement les probabilités apprises pendant l'entraînement.\n",
    "Conception d'Applications\n",
    "\n",
    "Système de Génération d'Histoires pour Enfants\n",
    "\n",
    "Utilisation de la Température Softmax : Pour générer des histoires personnalisées pour enfants, il est important de trouver un équilibre entre créativité et cohérence. Une température légèrement supérieure à 1, par exemple 1.2, pourrait être utilisée pour encourager la créativité tout en maintenant une certaine cohérence narrative.\n",
    "Justification : Une température plus élevée permet au modèle de générer des idées plus variées et intéressantes, rendant les histoires plus captivantes. Cependant, il est important de ne pas trop augmenter la température pour éviter des sorties incohérentes.\n",
    "Système de Génération de Résumés de Rapports Financiers\n",
    "\n",
    "Utilisation de la Température Softmax : Pour générer des résumés de rapports financiers, une température inférieure à 1, par exemple 0.7, pourrait être utilisée pour favoriser la précision et la fiabilité.\n",
    "Justification : Une température plus basse rend le modèle plus déterministe, réduisant le risque d'erreurs ou d'informations trompeuses dans les résumés. Cela est crucial pour les rapports financiers, où la précision est essentielle.\n",
    "Température Softmax et Biais\n",
    "\n",
    "Effet sur le Biais\n",
    "\n",
    "Température Basse : Une température basse peut amplifier les biais présents dans les données d'entraînement, car elle favorise les sorties les plus probables, qui peuvent refléter des stéréotypes ou des préjugés courants.\n",
    "Température Élevée : Une température élevée peut réduire l'impact des biais en permettant une plus grande diversité dans les sorties. Cependant, cela peut aussi conduire à des résultats moins cohérents ou pertinents.\n",
    "Exemple Pratique\n",
    "\n",
    "Scénario : Considérons un modèle de langage utilisé pour générer des descriptions de personnes basées sur des professions.\n",
    "Température Basse : Si le modèle a été entraîné sur des données où les médecins sont majoritairement décrits comme des hommes, une température basse pourrait amplifier ce biais en générant presque toujours des descriptions de médecins masculins.\n",
    "Température Élevée : Une température plus élevée pourrait permettre au modèle de générer des descriptions plus variées, incluant des femmes médecins, réduisant ainsi le biais de genre.\n",
    "En ajustant la température softmax, il est possible de contrôler non seulement la créativité et la cohérence des sorties du modèle, mais aussi de moduler l'impact des biais présents dans les données d'entraînement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
