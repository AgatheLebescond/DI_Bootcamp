{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b10cef",
   "metadata": {},
   "source": [
    "# D√©fi quotidien : comment affiner les LLM avec LoRA\n",
    "\n",
    "\n",
    "Les m√©thodes de r√©glage fin efficace des param√®tres (PEFT) , comme LoRA, r√©pondent aux d√©fis du r√©glage fin des grands mod√®les de langage (LLM) en ne mettant √† jour qu'un petit sous-ensemble des param√®tres du mod√®le. Cette approche r√©duit consid√©rablement les co√ªts de calcul et de stockage, rendant le r√©glage fin des LLM plus accessible. Les techniques PEFT permettent aux d√©veloppeurs d'adapter des mod√®les pr√©-entra√Æn√©s √† des t√¢ches sp√©cifiques sans avoir √† r√©entra√Æner l'ensemble du mod√®le, ce qui acc√©l√®re les cycles de d√©veloppement et r√©duit la consommation de ressources.\n",
    "Vous les mettrez en ≈ìuvre pour relever ce d√©fi.\n",
    "\n",
    "\n",
    "\n",
    "üë©‚Äçüè´ üë©üèø‚Äçüè´ Ce que vous apprendrez\n",
    "Comment appliquer l‚Äôadaptation de bas rang (LoRA) √† un mod√®le de langage pr√©-entra√Æn√©.\n",
    "Comment affiner un mod√®le adapt√© √† LoRA √† l'aide de la biblioth√®que PEFT Hugging Face.\n",
    "Comment enregistrer et charger un mod√®le LoRA affin√©.\n",
    "Comment effectuer une inf√©rence √† l‚Äôaide d‚Äôun mod√®le LoRA affin√©.\n",
    "\n",
    "\n",
    "üõ†Ô∏è Ce que vous allez cr√©er\n",
    "Un mod√®le de langage affin√© qui g√©n√®re du texte bas√© sur un ensemble de donn√©es sp√©cifique de citations, en utilisant LoRA.\n",
    "\n",
    "\n",
    "Ensemble de donn√©es\n",
    "L'ensemble de donn√©es ¬´ Abirate/english_quotes ¬ª, en particulier un √©chantillon de 10 % de la r√©partition de la formation.\n",
    "\n",
    "\n",
    "T√¢che\n",
    "Installer les biblioth√®ques n√©cessaires (PEFT, jeux de donn√©es).\n",
    "Chargez un mod√®le de langage pr√©-entra√Æn√© (bigscience/bloomz-560m) et son tokeniseur.\n",
    "Chargez l‚Äôensemble de donn√©es et pr√©traitez-le pour le mod√®le.\n",
    "Configurez LoRA en utilisant LoraConfig.\n",
    "Appliquez LoRA au mod√®le pr√©-entra√Æn√© √† l'aide de get_peft_model.\n",
    "Configurez les arguments de formation √† l'aide de TrainingArguments.\n",
    "Initialisez et entra√Ænez le mod√®le √† l‚Äôaide de Trainer.\n",
    "Enregistrez le mod√®le LoRA affin√©.\n",
    "Chargez le mod√®le LoRA enregistr√© pour l'inf√©rence √† l'aide de PeftModel.from_pretrained.\n",
    "G√©n√©rer du texte √† l‚Äôaide du mod√®le affin√© et du tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a05677",
   "metadata": {},
   "source": [
    "%pip install peft==0.4.0\n",
    "\n",
    "mkdir cache\n",
    "\n",
    "!pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \n",
    "tokenizer = \n",
    "foundation_model = \n",
    "\n",
    "data =  # Sample 10%\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "train_sample = data.select(range(5))\n",
    "display(train_sample)\n",
    "\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "#Fill in `r=1` and `target_modules`.\n",
    "lora_config = LoraConfig(\n",
    "    r=,\n",
    "    lora_alpha=, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
    "    target_modules=,\n",
    "    lora_dropout=,\n",
    "    bias=\"none\", # this specifies if the bias parameter should be trained.\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "#Add the adapter layers to the foundation model to be trained\n",
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "print(peft_model.print_trainable_parameters())\n",
    "\n",
    "\n",
    "### Fill out the `Trainer` class. \n",
    "\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "output_directory = os.path.join(\"../cache/working\", \"peft_lab_outputs\")\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    output_dir=output_directory,\n",
    "    auto_find_batch_size=,\n",
    "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=,\n",
    "    use_cpu=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=,\n",
    "    args=,\n",
    "    train_dataset=e,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "### Load the PEFT model using pre-defined LoRA configs and foundation model. We set `is_trainable=False` to avoid further training.\n",
    "\n",
    "import time\n",
    "\n",
    "time_now = \n",
    "peft_model_path = os.path.join(output_directory, f\"peft_model_{time_now}\")\n",
    "trainer.model.save_pretrained(peft_model_path)\n",
    "\n",
    "### Generate output tokens\n",
    "\n",
    "inputs = tokenizer(\"Two things are infinite: \", return_tensors=\"pt\")\n",
    "outputs = peft_model.generate(\n",
    "    ...\n",
    "    )\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4dc158",
   "metadata": {},
   "source": [
    "## √âtape 1 : Installer les biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aafeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft==0.4.0 datasets transformers accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163190cc",
   "metadata": {},
   "source": [
    "## √âtape 2 : Charger le mod√®le et le tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d946f9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chume\\.conda\\envs\\llama\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\chume\\.conda\\envs\\llama\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\chume\\.cache\\huggingface\\hub\\models--bigscience--bloomz-560m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"bigscience/bloomz-560m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57f5e5",
   "metadata": {},
   "source": [
    "Avertissements mineurs, pas bloquants. Corrige si tu veux un environnement propre et plus efficace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc59225b",
   "metadata": {},
   "source": [
    "## √âtape 3 : Charger et pr√©traiter les donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e4767f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chume\\.conda\\envs\\llama\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\chume\\.cache\\huggingface\\hub\\datasets--Abirate--english_quotes. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2508/2508 [00:00<00:00, 250812.20 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [00:00<00:00, 15686.77 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['quote', 'author', 'tags', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 5\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"Abirate/english_quotes\", split=\"train[:10%]\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "train_sample = data.select(range(5))\n",
    "print(train_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3184e4",
   "metadata": {},
   "source": [
    " **Analyse rapide :**\n",
    "\n",
    "*  **Avertissement symlink toujours pr√©sent** : m√™me cause que pr√©c√©demment ‚Üí Windows sans symlink actif ‚Üí **pas bloquant** mais plus de stockage utilis√©.\n",
    "\n",
    "*  **Chargement Dataset r√©ussi** :\n",
    "\n",
    "   * **251 exemples** extraits (10 % du dataset),\n",
    "   * **5 √©chantillons s√©lectionn√©s** pour l‚Äôentra√Ænement,\n",
    "    * Colonnes bien pr√©par√©es : `input_ids`, `attention_mask` ‚Üí **pr√©traitement OK**.\n",
    "\n",
    "---\n",
    "\n",
    " **Conclusion** :\n",
    "**Pas d'erreur critique**. Je peux continuer l'entra√Ænement. Pour optimiser l‚Äôespace disque, je dois activer **Developer Mode**, sinon ignorer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db97ad",
   "metadata": {},
   "source": [
    "## √âtape 4 : Configurer LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77276182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 98,304 || all params: 559,312,896 || trainable%: 0.01757585078102687\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=1,\n",
    "    lora_alpha=1,\n",
    "    target_modules=[\"query_key_value\"],  # typique pour Bloom\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "print(peft_model.print_trainable_parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17887b1",
   "metadata": {},
   "source": [
    " **Analyse rapide** :\n",
    "\n",
    "* **Param√®tres entra√Ænables** : 98 304\n",
    "* **Param√®tres totaux** : 559,3 millions\n",
    "* **Proportion entra√Æn√©e** : \\~**0,0176 %**\n",
    "\n",
    " **Interpr√©tation** :\n",
    "\n",
    "* **Normal avec LoRA** : seul un minuscule sous-ensemble des poids (couches LoRA) est entra√Æn√©.\n",
    "* **Avantages** :\n",
    "\n",
    "  * **Moins de ressources**, entra√Ænement rapide.\n",
    "  * **Faible risque de surapprentissage** sur petit dataset.\n",
    "* **Inconv√©nient** :\n",
    "\n",
    "  * Pas adapt√© si tu veux modifier en profondeur les capacit√©s du mod√®le ‚Üí LoRA = **affinage l√©ger et cibl√©**.\n",
    "\n",
    " **Conclusion** : le r√©sultat est attendu et confirme que **LoRA fonctionne comme pr√©vu**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fc10b",
   "metadata": {},
   "source": [
    "## √âtape 5 : Configurer TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e780e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import os\n",
    "\n",
    "output_directory = \"../cache/working/peft_lab_outputs\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    output_dir=output_directory,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=3e-2,\n",
    "    num_train_epochs=3,\n",
    "    use_cpu=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb784514",
   "metadata": {},
   "source": [
    "## √âtape 6 : Entra√Æner le mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d835675a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4.2541, 'train_samples_per_second': 3.526, 'train_steps_per_second': 0.705, 'train_loss': 3.142648696899414, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=3.142648696899414, metrics={'train_runtime': 4.2541, 'train_samples_per_second': 3.526, 'train_steps_per_second': 0.705, 'total_flos': 1388070051840.0, 'train_loss': 3.142648696899414, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb5a0d5",
   "metadata": {},
   "source": [
    " **Analyse rapide des r√©sultats d'entra√Ænement** :\n",
    "\n",
    "* **Dur√©e d'entra√Ænement** : 4,25 secondes ‚Üí **tr√®s rapide**, logique vu la petite taille de donn√©es et LoRA.\n",
    "* **Vitesse** : \\~3,5 √©chantillons/seconde, 0,7 √©tape/seconde ‚Üí **efficace**, normal avec CPU.\n",
    "* **Loss final** : 3,14 ‚Üí **coh√©rent** pour un mod√®le pr√©-entra√Æn√© sur un jeu de donn√©es minuscule (5 exemples sur 3 √©poques).\n",
    "* **√âpoques** : 3 ‚Üí **conforme** au param√©trage.\n",
    "\n",
    " **Conclusion** : entra√Ænement **ultra-rapide**, **loss stable** mais attention :\n",
    "\n",
    "* **dataset trop petit** ‚Üí perte peu significative.\n",
    "* Utilisable uniquement pour **d√©monstration rapide**, pas production s√©rieuse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce5de59",
   "metadata": {},
   "source": [
    "## √âtape 7 : Enregistrer le mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9226e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_now = int(time.time())\n",
    "peft_model_path = os.path.join(output_directory, f\"peft_model_{time_now}\")\n",
    "trainer.model.save_pretrained(peft_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d55ff",
   "metadata": {},
   "source": [
    "## √âtape 8 : Charger et inf√©rence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee5d9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two things are infinite:  the universe and the universe.‚Äù And that is the universe and the universe.‚Äù And that is the universe and the universe.‚Äù‚Äù And that']\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(foundation_model, peft_model_path, is_trainable=False)\n",
    "\n",
    "inputs = tokenizer(\"Two things are infinite: \", return_tensors=\"pt\")\n",
    "outputs = peft_model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=30)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a473224",
   "metadata": {},
   "source": [
    " **Analyse rapide du r√©sultat g√©n√©r√©** :\n",
    "\n",
    "* **Probl√®me clair** : **r√©p√©titions absurdes** ‚Üí \"the universe and the universe...\" boucle incoh√©rente.\n",
    "* **Cause probable** :\n",
    "\n",
    "  *  **Dataset minuscule** ‚Üí 5 exemples = **surapprentissage massif**, absence de g√©n√©ralisation.\n",
    "  *  **Pas de r√©gulation du mod√®le** : pas de `temperature`, `top_k`, `top_p` ‚Üí **sorties d√©terministes** qui accentuent la r√©p√©tition.\n",
    "\n",
    "---\n",
    "\n",
    " **Conclusion directe** :\n",
    "\n",
    "* Avec **si peu de donn√©es**, le mod√®le **r√©p√®te m√©caniquement** ce qu'il a vu ‚Üí r√©sultat attendu.\n",
    "*  **Solution rapide** : augmenter le nombre d'exemples, ajouter :\n",
    "\n",
    "```python\n",
    "outputs = peft_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_new_tokens=30,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")\n",
    "```\n",
    "\n",
    " Meilleure **diversit√©** dans la g√©n√©ration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae9e3ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two things are infinite:  the universe and the universe.‚Äù If God is only like a book, what better book is sure to be in the universe than a book that is']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Two things are infinite: \", return_tensors=\"pt\")\n",
    "\n",
    "outputs = peft_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    max_new_tokens=30,          # Limite la longueur de la r√©ponse\n",
    "    temperature=0.9,            # Introduit de la diversit√© (plus √©lev√© = plus cr√©atif)\n",
    "    top_k=50,                   # Ne garde que les 50 tokens les plus probables\n",
    "    top_p=0.95,                 # Nucleus sampling : consid√®re les tokens qui totalisent 95% de la probabilit√©\n",
    "    do_sample=True              # Active l'√©chantillonnage al√©atoire\n",
    ")\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc83cb8d",
   "metadata": {},
   "source": [
    " **Analyse rapide du nouveau r√©sultat** :\n",
    "\n",
    "*  **Am√©lioration claire** : moins de r√©p√©titions m√©caniques, une **phrase construite** avec une id√©e coh√©rente.\n",
    "*  **Limite persistante** :\n",
    "\n",
    "   * Le mod√®le reste **obs√©d√© par \"the universe\"**, car le **dataset reste trop pauvre (5 exemples)**.\n",
    "   * M√©lange de **structure de citation** sans vrai fond de sens ‚Üí typique d'un mod√®le **surentra√Æn√© sur tr√®s peu de donn√©es**.\n",
    "\n",
    "---\n",
    "\n",
    " **Conclusion directe** :\n",
    "\n",
    "*  G√©n√©ration plus **vari√©e** gr√¢ce aux param√®tres (`temperature`, `top_k`, `top_p`).\n",
    "*  **Probl√®me fondamental** = **dataset insuffisant**.\n",
    "*  **Solution optimale** : utiliser un **dataset plus large** (ex : tout le split `train[:50%]`), **nettoyer les donn√©es** et √©ventuellement augmenter les epochs **uniquement si le dataset est enrichi**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add7fe6",
   "metadata": {},
   "source": [
    "## Bilan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e61f7dc",
   "metadata": {},
   "source": [
    " **Objectif atteint** : entra√Ænement rapide d'un LLM (BloomZ-560m) avec **LoRA** sur un petit dataset sp√©cifique (citations), sans r√©entra√Æner tout le mod√®le.\n",
    "\n",
    " **M√©thode efficace** :\n",
    "\n",
    "* **Peu de ressources** (seules quelques couches sont ajust√©es).\n",
    "* **Temps d'entra√Ænement r√©duit**.\n",
    "* **Simplicit√© via PEFT (Hugging Face)**.\n",
    "\n",
    " **Points cl√©s** :\n",
    "\n",
    "* Utilisation d'un mod√®le pr√©-entra√Æn√©.\n",
    "* Adaptation rapide avec LoRA (r=1).\n",
    "* Validation pratique avec g√©n√©ration de texte.\n",
    "\n",
    " **Conclusion** : LoRA rend l'adaptation des LLM **rapide, l√©g√®re et efficace**, parfait pour des cas d‚Äôusage sp√©cifiques.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
